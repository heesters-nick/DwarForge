{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb11b2-d6dc-46ef-96e0-faefa5555071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62157f-3212-4392-a99f-59418beda481",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f869533-b555-406e-9d16-c19a088f2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = '/arc/home/heestersnick/dwarforge/tables'\n",
    "models = '/arc/home/heestersnick/dwarforge/models'\n",
    "data_dir = '/arc/projects/unions/ssl/data/raw/tiles/dwarforge'\n",
    "training_data_file = 'umap_data_master.parquet'\n",
    "if training_data_file.endswith('.csv'):\n",
    "    training_data = pd.read_csv(os.path.join(tables, training_data_file))\n",
    "elif training_data_file.endswith('.parquet'):\n",
    "    training_data = pd.read_parquet(os.path.join(tables, training_data_file))\n",
    "    training_data['lsb_cfis_lsb-r'] = training_data['lsb_cfis_lsb-r'].fillna(0)\n",
    "    training_data.rename(columns={'lsb_cfis_lsb-r': 'lsb'}, inplace=True)\n",
    "    training_data.drop(columns=['lsb_whigs-g'], inplace=True)\n",
    "    training_data.drop(columns=['lsb_ps-i'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5376d4-3d9e-4401-a26b-3e5933070369",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8cf02-9542-40e5-b718-9f14c6bc9ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, labels, title='Confusion Matrix', cmap='viridis'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    # cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    gs = plt.GridSpec(len(labels), len(labels) + 1, width_ratios=[4] * len(labels) + [0.1])\n",
    "    plt.figure(figsize=(len(labels) + 3, len(labels) + 3))\n",
    "    ax = sns.heatmap(\n",
    "        cm,\n",
    "        annot=False,\n",
    "        cmap=cmap,\n",
    "        cbar_kws={'shrink': 0.805},\n",
    "        square=True,\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "    )\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            if i == j:\n",
    "                ax.text(\n",
    "                    j + 0.5,\n",
    "                    i + 0.5,\n",
    "                    f'{cm[i, j]:d}\\n{cm_percentage[i, j]:.1f}%',\n",
    "                    ha='center',\n",
    "                    va='center',\n",
    "                    fontsize=10,\n",
    "                    color='black',\n",
    "                )\n",
    "            else:\n",
    "                ax.text(\n",
    "                    j + 0.5,\n",
    "                    i + 0.5,\n",
    "                    f'{cm[i, j]:d}\\n{cm_percentage[i, j]:.1f}%',\n",
    "                    ha='center',\n",
    "                    va='center',\n",
    "                    fontsize=10,\n",
    "                    color='white',\n",
    "                )\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    # plt.savefig('/home/nick/Documents/Candidacy_Exam/Presentation/Figures/'+'cm_w_percentage.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def classify_and_save_parquet(\n",
    "    parent_folder, random_forest_model, features, df_path_out, band='cfis_lsb-r'\n",
    "):\n",
    "    pattern = os.path.join(parent_folder, '*_*', band, 'CFIS_LSB.*.r_rebin_det_params.parquet')\n",
    "    positive_class_rows = []\n",
    "    for file in tqdm(glob(pattern)):\n",
    "        try:\n",
    "            filename = os.path.basename(file)\n",
    "            tile_numbers = filename.split('.')[1:3]\n",
    "            tile_id = f'({tile_numbers[0]}, {tile_numbers[1]})'\n",
    "\n",
    "            # Read the parquet file\n",
    "            df = pd.read_parquet(file)\n",
    "\n",
    "            # Select only the columns used for inference\n",
    "            X = df[features]\n",
    "\n",
    "            # Apply the random forest model\n",
    "            df['class'] = random_forest_model.predict(X)\n",
    "\n",
    "            # Aggregate rows where class = 1\n",
    "            positive_rows = df[df['class'] == 1].copy()\n",
    "            positive_rows['tile'] = tile_id  # Add file path for reference\n",
    "            positive_class_rows.append(positive_rows)\n",
    "\n",
    "            # Save the updated dataframe back to parquet\n",
    "            df.to_parquet(file, index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing file {file}: {str(e)}')\n",
    "            continue\n",
    "\n",
    "    # Combine all positive class rows into a single DataFrame\n",
    "    if positive_class_rows:\n",
    "        all_positive_rows = pd.concat(positive_class_rows, ignore_index=True)\n",
    "        all_positive_rows.to_parquet(df_path_out, index=False)\n",
    "    else:\n",
    "        all_positive_rows = pd.DataFrame()\n",
    "\n",
    "    return all_positive_rows\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Additional metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "    # ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Average Precision (AP) summarizes a precision-recall curve\n",
    "    average_precision = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall (Sensitivity): {recall:.4f}')\n",
    "    print(f'Specificity: {specificity:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Balanced Accuracy: {balanced_accuracy:.4f}')\n",
    "    print(f'ROC AUC: {roc_auc:.4f}')\n",
    "    print(f'Average Precision: {average_precision:.4f}')\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, labels=['no dwarf', 'dwarf'])\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(f'Precision-Recall curve: AP={average_precision:0.2f}')\n",
    "    plt.show()\n",
    "\n",
    "    # Calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sample_and_shuffle_data(df, label_column='lsb', neg_pos_ratio=3, random_state=42):\n",
    "    # Separate positive and negative examples\n",
    "    positive_df = df[df[label_column] == 1]\n",
    "    negative_df = df[df[label_column] == 0]\n",
    "\n",
    "    # Calculate the number of negative examples to keep\n",
    "    n_positive = len(positive_df)\n",
    "    n_negative_to_keep = n_positive * neg_pos_ratio\n",
    "\n",
    "    # Sample negative examples\n",
    "    if len(negative_df) > n_negative_to_keep:\n",
    "        negative_sample = negative_df.sample(n=int(n_negative_to_keep), random_state=random_state)\n",
    "    else:\n",
    "        negative_sample = negative_df  # Keep all if we don't have enough\n",
    "\n",
    "    # Combine positive examples with sampled negative examples\n",
    "    combined_df = pd.concat([positive_df, negative_sample])\n",
    "\n",
    "    # Shuffle the combined DataFrame\n",
    "    shuffled_df = combined_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    return shuffled_df\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Additional metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "    # ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Average Precision (AP) summarizes a precision-recall curve\n",
    "    average_precision = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall (Sensitivity): {recall:.4f}')\n",
    "    print(f'Specificity: {specificity:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Balanced Accuracy: {balanced_accuracy:.4f}')\n",
    "    print(f'ROC AUC: {roc_auc:.4f}')\n",
    "    print(f'Average Precision: {average_precision:.4f}')\n",
    "\n",
    "    plot_confusion_matrix(y_test, y_pred, labels=['no dwarf', 'dwarf'])\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(f'Precision-Recall curve: AP={average_precision:0.2f}')\n",
    "    plt.show()\n",
    "\n",
    "    # Calibration curve\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel('Mean predicted probability')\n",
    "    plt.ylabel('Fraction of positives')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972d62d-4c24-4450-850b-9f8959769ee6",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb9ebcf-003d-4657-a422-99f6ba6d6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['total_flux', 'mu_max', 'mu_median', 'mu_mean', 'n_pix', 're_arcsec', 'r_fwhm_arcsec', 'r_10_arcsec', 'r_25_arcsec', 'r_75_arcsec', 'r_90_arcsec', 'r_100_arcsec', 'A_arcsec', 'B_arcsec', 'axis_ratio', 'mag', 'mu', 'lsb']\n",
    "features = [\n",
    "    # \"total_flux_cfis_lsb-r\",\n",
    "    # \"mu_max_cfis_lsb-r\",\n",
    "    # \"mu_median_cfis_lsb-r\",\n",
    "    # \"mu_mean_cfis_lsb-r\",\n",
    "    're_arcsec_cfis_lsb-r',\n",
    "    # \"r_fwhm_arcsec_cfis_lsb-r\",\n",
    "    'r_10_arcsec_cfis_lsb-r',\n",
    "    'r_25_arcsec_cfis_lsb-r',\n",
    "    'r_75_arcsec_cfis_lsb-r',\n",
    "    'r_90_arcsec_cfis_lsb-r',\n",
    "    # \"r_100_arcsec_cfis_lsb-r\",\n",
    "    'axis_ratio_cfis_lsb-r',\n",
    "    'mag_cfis_lsb-r',\n",
    "    'mu_cfis_lsb-r',\n",
    "    # \"total_flux_whigs-g\",\n",
    "    # \"mu_max_whigs-g\",\n",
    "    # \"mu_median_whigs-g\",\n",
    "    # \"mu_mean_whigs-g\",\n",
    "    're_arcsec_whigs-g',\n",
    "    # \"r_fwhm_arcsec_whigs-g\",\n",
    "    'r_10_arcsec_whigs-g',\n",
    "    'r_25_arcsec_whigs-g',\n",
    "    'r_75_arcsec_whigs-g',\n",
    "    'r_90_arcsec_whigs-g',\n",
    "    # \"r_100_arcsec_whigs-g\",\n",
    "    # \"axis_ratio_whigs-g\",\n",
    "    'mag_whigs-g',\n",
    "    'mu_whigs-g',\n",
    "    # \"total_flux_ps-i\",\n",
    "    # \"mu_max_ps-i\",\n",
    "    # \"mu_median_ps-i\",\n",
    "    # \"mu_mean_ps-i\",\n",
    "    're_arcsec_ps-i',\n",
    "    # \"r_fwhm_arcsec_ps-i\",\n",
    "    'r_10_arcsec_ps-i',\n",
    "    'r_25_arcsec_ps-i',\n",
    "    'r_75_arcsec_ps-i',\n",
    "    'r_90_arcsec_ps-i',\n",
    "    # \"r_100_arcsec_ps-i\",\n",
    "    # \"axis_ratio_ps-i\",\n",
    "    'mag_ps-i',\n",
    "    'mu_ps-i',\n",
    "    'lsb',\n",
    "]\n",
    "dwarf_column = 'lsb'\n",
    "training_df = training_data[features]\n",
    "training_df = sample_and_shuffle_data(training_df, neg_pos_ratio=4.0)\n",
    "X, y = training_df.drop(columns=[dwarf_column]), training_df[dwarf_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=np.random.randint(99)\n",
    ")\n",
    "print(\n",
    "    f'Using {len(training_df)} examples. {np.count_nonzero(y == 1)} positive and {np.count_nonzero(y == 0)} negative ones.'\n",
    ")\n",
    "print(f'{len(X_train)} are used for training, {len(X_test)} for testing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22f9db-ee82-44e9-8d06-68d0180afc4f",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56687488-2fba-410a-956a-366f33c50480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DMatrix objects\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='auc')\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'learning_rate': [0.01, 0.1],\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'min_child_weight': [1, 5],\n",
    "#     'gamma': [0, 0.1],\n",
    "#     'subsample': [0.8, 1.0],\n",
    "#     'colsample_bytree': [0.8, 1.0]\n",
    "# }\n",
    "\n",
    "\n",
    "# # Set up RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=xgb_model,\n",
    "#     param_distributions=param_grid,\n",
    "#     n_iter=20,  # number of parameter settings that are sampled\n",
    "#     scoring=make_scorer(accuracy_score),\n",
    "#     cv=3,  # reduced from 5 to 3\n",
    "#     verbose=2,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "scale_pos_weight = 2.0\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'min_child_weight': randint(1, 10),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'scale_pos_weight': [scale_pos_weight, 1],  # Try both weighted and unweighted\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,  # number of parameter settings that are sampled\n",
    "    scoring=make_scorer(roc_auc_score),\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e3ae3-5b6b-4bfa-92cf-9cfbb09dcd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "print('Best cross-validation score:', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a2510-5e57-48be-aab3-96527a63ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa78b4-6de0-4281-8c5c-f0df3ab90420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079bc5f-cd2e-4ea3-b317-ad1305d46be1",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb16cbe-f22e-4833-a87d-f37af2e10ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf4413-471e-4f0d-8a83-81084515ba84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
