{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56280b-7bd4-442a-9187-934a2cb9d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Literal, Optional, Union\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "import torch\n",
    "from scipy.ndimage import binary_dilation, label\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "from zoobot.pytorch.training.representations import ZoobotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad435ef-6950-4592-8d20-b2106f3d1426",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11536c-54a3-4616-9f14-9d14b37bf760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5(file_path):\n",
    "    \"\"\"\n",
    "    Reads cutout data from HDF5 file\n",
    "    :param file_path: path to HDF5 file\n",
    "    :return: cutout data\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Create empty dictionaries to store data for each group\n",
    "        cutout_data = {}\n",
    "\n",
    "        # Loop through datasets\n",
    "        for dataset_name in f:\n",
    "            if dataset_name == 'images':\n",
    "                data = np.nan_to_num(np.array(f[dataset_name]), nan=0.0)\n",
    "            else:\n",
    "                data = np.array(f[dataset_name])\n",
    "            cutout_data[dataset_name] = data\n",
    "    return cutout_data\n",
    "\n",
    "\n",
    "def extract_and_save_objects(\n",
    "    catalog_path, output_dir, base_path='/projects/unions/ssl/data/raw/tiles/dwarforge'\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract objects from HDF5 files based on a catalog and save them to new HDF5 files in chunks of 5000 objects.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    catalog_path : str\n",
    "        Path to the catalog file containing 'tile', 'unique_id', 'Z', and 'LOGM_CIGALE' columns.\n",
    "    output_dir : str\n",
    "        Directory where output HDF5 files will be saved.\n",
    "    base_path : str\n",
    "        Base path to the directory containing the tile directories.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (objects_processed, objects_not_found, files_created)\n",
    "    \"\"\"\n",
    "    # 1. Load and prepare the catalog\n",
    "    print('Loading catalog...')\n",
    "    catalog = pd.read_csv(catalog_path, low_memory=False)\n",
    "\n",
    "    # Check if required columns exist\n",
    "    required_cols = ['tile', 'unique_id', 'Z', 'LOGM_CIGALE']\n",
    "    for col in required_cols:\n",
    "        if col not in catalog.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found in catalog\")\n",
    "\n",
    "    # 2. Group objects by tile\n",
    "    print('Grouping objects by tile...')\n",
    "    grouped_catalog = catalog.groupby('tile')\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize data accumulation\n",
    "    accumulated_data = {}\n",
    "    accumulated_count = 0\n",
    "    datasets_to_extract = None\n",
    "    bands_data = None\n",
    "    tile_data = None\n",
    "\n",
    "    # Counters for statistics\n",
    "    total_objects = len(catalog)\n",
    "    objects_processed = 0\n",
    "    objects_not_found = 0\n",
    "    files_created = 0\n",
    "\n",
    "    print(f'Processing {len(grouped_catalog)} unique tiles with {total_objects} total objects...')\n",
    "\n",
    "    # 3. Process each tile\n",
    "    for tile, tile_objects in tqdm(grouped_catalog):\n",
    "        # Get the tile format\n",
    "        tile_formatted = str(tile)\n",
    "        # Construct file path\n",
    "        tile_path = (\n",
    "            f'{base_path}/{tile_formatted}/gri/{tile_formatted}_matched_cutouts_full_res_final.h5'\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            with h5py.File(tile_path, 'r') as src_file:\n",
    "                # 4. Create object index mapping\n",
    "                unique_ids = src_file['unique_id'][:]\n",
    "                id_to_index = {uid: idx for idx, uid in enumerate(unique_ids)}\n",
    "\n",
    "                # Store bands data (only need to do this once)\n",
    "                if bands_data is None:\n",
    "                    bands_data = src_file['band_names'][:]\n",
    "                    tile_data = src_file['tile'][:]\n",
    "\n",
    "                # Get list of datasets to copy (all except 'bands')\n",
    "                if datasets_to_extract is None:\n",
    "                    datasets_to_extract = [\n",
    "                        key for key in src_file.keys() if key not in ['band_names', 'tile']\n",
    "                    ]\n",
    "\n",
    "                    # Initialize data accumulation\n",
    "                    for dataset_name in datasets_to_extract:\n",
    "                        accumulated_data[dataset_name] = []\n",
    "\n",
    "                    # Add our new datasets\n",
    "                    accumulated_data['Z'] = []\n",
    "                    accumulated_data['LOGM_CIGALE'] = []\n",
    "\n",
    "                # 5. Select objects for extraction\n",
    "                tile_objects_array = tile_objects['unique_id'].values\n",
    "                valid_indices = []\n",
    "                valid_catalog_indices = []\n",
    "\n",
    "                for cat_idx, uid in enumerate(tile_objects_array):\n",
    "                    if uid in id_to_index:\n",
    "                        valid_indices.append(id_to_index[uid])\n",
    "                        valid_catalog_indices.append(cat_idx)\n",
    "                    else:\n",
    "                        objects_not_found += 1\n",
    "\n",
    "                n_valid = len(valid_indices)\n",
    "                if n_valid == 0:\n",
    "                    print(f'No valid objects found in tile {tile_formatted}')\n",
    "                    continue\n",
    "\n",
    "                # 6. Extract data for each dataset\n",
    "                for dataset_name in datasets_to_extract:\n",
    "                    extracted_data = src_file[dataset_name][:][valid_indices]\n",
    "                    accumulated_data[dataset_name].append(extracted_data)\n",
    "\n",
    "                # Add Z and LOGM_CIGALE from catalog\n",
    "                z_values = tile_objects.iloc[valid_catalog_indices]['Z'].values\n",
    "                logm_values = tile_objects.iloc[valid_catalog_indices]['LOGM_CIGALE'].values\n",
    "\n",
    "                accumulated_data['Z'].append(z_values)\n",
    "                accumulated_data['LOGM_CIGALE'].append(logm_values)\n",
    "\n",
    "                # Update object counts\n",
    "                accumulated_count += n_valid\n",
    "                objects_processed += n_valid\n",
    "\n",
    "                # Check if we have enough objects to save a chunk\n",
    "                while accumulated_count >= 5000:\n",
    "                    # Save a chunk of exactly 5000 objects\n",
    "                    accumulated_count = save_chunk(\n",
    "                        accumulated_data,\n",
    "                        bands_data,\n",
    "                        tile_data,\n",
    "                        output_dir,\n",
    "                        files_created,\n",
    "                        accumulated_count,\n",
    "                    )\n",
    "                    files_created += 1\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f'Warning: File not found for tile {tile_formatted} at {tile_path}')\n",
    "            objects_not_found += len(tile_objects)\n",
    "\n",
    "    # Save any remaining objects\n",
    "    if accumulated_count > 0:\n",
    "        save_chunk(\n",
    "            accumulated_data,\n",
    "            bands_data,\n",
    "            tile_data,\n",
    "            output_dir,\n",
    "            files_created,\n",
    "            accumulated_count,\n",
    "            is_final=True,\n",
    "        )\n",
    "        files_created += 1\n",
    "\n",
    "    print('Processing complete:')\n",
    "    print(f'  Total objects processed: {objects_processed}/{total_objects}')\n",
    "    print(f'  Objects not found: {objects_not_found}')\n",
    "    print(f'  Output files created: {files_created}')\n",
    "\n",
    "    return objects_processed, objects_not_found, files_created\n",
    "\n",
    "\n",
    "def save_chunk(\n",
    "    accumulated_data,\n",
    "    bands_data,\n",
    "    tile_data,\n",
    "    output_dir,\n",
    "    chunk_index,\n",
    "    accumulated_count,\n",
    "    is_final=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Save a chunk of exactly 5000 objects (or fewer if it's the last chunk) to an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    accumulated_data : dict\n",
    "        Dictionary containing lists of arrays for each dataset\n",
    "    bands_data : ndarray\n",
    "        The 'bands' dataset to save with each chunk\n",
    "    tile_data : ndarray\n",
    "        The 'tile' dataset to save with each chunk\n",
    "    output_dir : str\n",
    "        Directory to save the output file\n",
    "    chunk_index : int\n",
    "        Index for the chunk file name\n",
    "    accumulated_count : int\n",
    "        Total number of objects accumulated\n",
    "    is_final : bool, optional\n",
    "        Whether this is the final chunk (may be less than 5000 objects)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Number of objects remaining after saving the chunk\n",
    "    \"\"\"\n",
    "    output_path = os.path.join(output_dir, f'chunk_{chunk_index:04d}.h5')\n",
    "\n",
    "    # Determine how many objects to save\n",
    "    objects_to_save = 5000 if not is_final else accumulated_count\n",
    "    objects_to_save = min(objects_to_save, accumulated_count)\n",
    "\n",
    "    print(f'Saving chunk {chunk_index} with {objects_to_save} objects to {output_path}')\n",
    "\n",
    "    # Create a dictionary to store concatenated data\n",
    "    concatenated_data = {}\n",
    "\n",
    "    # Concatenate arrays for each dataset\n",
    "    for dataset_name, arrays in accumulated_data.items():\n",
    "        if arrays:  # Check if there are any arrays to concatenate\n",
    "            concatenated_data[dataset_name] = np.concatenate(arrays)\n",
    "\n",
    "    # Open output file and save data\n",
    "    with h5py.File(output_path, 'w') as out_file:\n",
    "        # Save each dataset (first objects_to_save objects)\n",
    "        for dataset_name, data in concatenated_data.items():\n",
    "            chunk_data = data[:objects_to_save]\n",
    "            out_file.create_dataset(dataset_name, data=chunk_data)\n",
    "\n",
    "        # Save the bands dataset\n",
    "        out_file.create_dataset('band_names', data=bands_data)\n",
    "        out_file.create_dataset('tile', data=tile_data)\n",
    "\n",
    "    # Update the accumulated_data to keep remaining objects\n",
    "    for dataset_name, data in concatenated_data.items():\n",
    "        if len(data) > objects_to_save:\n",
    "            accumulated_data[dataset_name] = [data[objects_to_save:]]\n",
    "        else:\n",
    "            accumulated_data[dataset_name] = []\n",
    "\n",
    "    # Return the number of objects remaining\n",
    "    return accumulated_count - objects_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674fd67-5785-4314-bbb3-7dd8c3a7ae61",
   "metadata": {},
   "source": [
    "# RGB Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a0700-38ea-4b0e-ba85-3492631b63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cutout(\n",
    "    cutout: np.ndarray, mode: str = 'vis', replace_anomaly: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Create an RGB image from the cutout data and save or plot it.\n",
    "\n",
    "    Args:\n",
    "        cutout (numpy.ndarray): cutout data with shape (channels, height, width)\n",
    "        mode (str, optional): mode of operation. Defaults to 'training'. Valid options are 'training' or 'vis'. Fills missing channels for visualization.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: preprocessed image cutout\n",
    "\n",
    "    \"\"\"\n",
    "    # map out bands to RGB\n",
    "    cutout_red = cutout[2]  # i-band\n",
    "    cutout_green = cutout[1]  # r-band\n",
    "    cutout_blue = cutout[0]  # g-band\n",
    "\n",
    "    # adjust zero-point for the g-band\n",
    "    if np.count_nonzero(cutout_blue) > 0:\n",
    "        cutout_blue = adjust_flux_with_zp(cutout_blue, 27.0, 30.0)\n",
    "\n",
    "    if replace_anomaly:\n",
    "        # replace anomalies\n",
    "        cutout_red = detect_anomaly(cutout_red)\n",
    "        cutout_green = detect_anomaly(cutout_green)\n",
    "        cutout_blue = detect_anomaly(cutout_blue)\n",
    "\n",
    "    # synthesize missing channel from the existing ones\n",
    "    # longest valid wavelength is mapped to red, middle to green, shortest to blue\n",
    "    if mode == 'vis':\n",
    "        if np.count_nonzero(cutout_red > 1e-10) == 0:\n",
    "            cutout_red = cutout_green\n",
    "            cutout_green = (cutout_green + cutout_blue) / 2\n",
    "        elif np.count_nonzero(cutout_green > 1e-10) == 0:\n",
    "            cutout_green = (cutout_red + cutout_blue) / 2\n",
    "        elif np.count_nonzero(cutout_blue > 1e-10) == 0:\n",
    "            cutout_blue = cutout_red\n",
    "            cutout_red = (cutout_red + cutout_green) / 2\n",
    "\n",
    "    # stack the channels in the order red, green, blue\n",
    "    cutout_prep = np.stack([cutout_red, cutout_green, cutout_blue], axis=-1)\n",
    "\n",
    "    return cutout_prep\n",
    "\n",
    "\n",
    "def generate_rgb(\n",
    "    cutout: np.ndarray,\n",
    "    scaling_type: Literal['asinh', 'linear'] = 'asinh',\n",
    "    stretch: float = 125,\n",
    "    Q: float = 7.0,\n",
    "    gamma: float = 0.25,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Create an RGB image from three bands of data preserving relative intensities.\n",
    "\n",
    "    Processes multi-band astronomical data into a properly scaled RGB image\n",
    "    suitable for visualization, handling high dynamic range and empty channels.\n",
    "\n",
    "    Args:\n",
    "        cutout: 3D array of shape (height, width, 3) with band data\n",
    "        scaling_type: Type of scaling to apply (\"asinh\" or \"linear\")\n",
    "        stretch: Scaling factor controlling overall brightness\n",
    "        Q: Softening parameter for asinh scaling (higher = more linear)\n",
    "        gamma: Gamma correction factor (lower = enhances faint features)\n",
    "\n",
    "    Returns:\n",
    "        Normalized RGB image with values in range [0, 1]\n",
    "\n",
    "    Notes:\n",
    "        For astronomical data with high dynamic range, \"asinh\" scaling is\n",
    "        typically preferred as it preserves both bright and faint details.\n",
    "    \"\"\"\n",
    "    frac = 0.1\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        red = cutout[:, :, 0]\n",
    "        green = cutout[:, :, 1]\n",
    "        blue = cutout[:, :, 2]\n",
    "\n",
    "        # Check for zero channels\n",
    "        red_is_zero = np.all(red == 0)\n",
    "        green_is_zero = np.all(green == 0)\n",
    "        blue_is_zero = np.all(blue == 0)\n",
    "\n",
    "        # Compute average intensity before scaling choice (avoiding zero channels)\n",
    "        nonzero_channels = []\n",
    "        if not red_is_zero:\n",
    "            nonzero_channels.append(red)\n",
    "        if not green_is_zero:\n",
    "            nonzero_channels.append(green)\n",
    "        if not blue_is_zero:\n",
    "            nonzero_channels.append(blue)\n",
    "\n",
    "        if nonzero_channels:\n",
    "            i_mean = sum(nonzero_channels) / len(nonzero_channels)\n",
    "        else:\n",
    "            i_mean = np.zeros_like(red)  # All channels are zero\n",
    "\n",
    "        if scaling_type == 'asinh':\n",
    "            # Apply asinh scaling\n",
    "            if not red_is_zero:\n",
    "                red = red * np.arcsinh(Q * i_mean / stretch) / (Q * i_mean)\n",
    "            if not green_is_zero:\n",
    "                green = green * np.arcsinh(Q * i_mean / stretch) / (Q * i_mean)\n",
    "            if not blue_is_zero:\n",
    "                blue = blue * np.arcsinh(Q * i_mean / stretch) / (Q * i_mean)\n",
    "        elif scaling_type == 'asinh_frac':\n",
    "            # Apply asinh scaling\n",
    "            if not red_is_zero:\n",
    "                red = (\n",
    "                    red * np.arcsinh(Q * i_mean / stretch) * frac / (np.arcsinh(frac * Q) * i_mean)\n",
    "                )\n",
    "            if not green_is_zero:\n",
    "                green = (\n",
    "                    green\n",
    "                    * np.arcsinh(Q * i_mean / stretch)\n",
    "                    * frac\n",
    "                    / (np.arcsinh(frac * Q) * i_mean)\n",
    "                )\n",
    "            if not blue_is_zero:\n",
    "                blue = (\n",
    "                    blue * np.arcsinh(Q * i_mean / stretch) * frac / (np.arcsinh(frac * Q) * i_mean)\n",
    "                )\n",
    "        elif scaling_type == 'linear':\n",
    "            # Apply linear scaling\n",
    "            if not red_is_zero:\n",
    "                red = red * stretch\n",
    "            if not green_is_zero:\n",
    "                green = green * stretch\n",
    "            if not blue_is_zero:\n",
    "                blue = blue * stretch\n",
    "        else:\n",
    "            raise ValueError(f'Unknown scaling type: {scaling_type}')\n",
    "\n",
    "        # Apply gamma correction while preserving sign\n",
    "        if gamma is not None:\n",
    "            if not red_is_zero:\n",
    "                red_mask = abs(red) <= 1e-9\n",
    "                red = np.sign(red) * (abs(red) ** gamma)\n",
    "                red[red_mask] = 0\n",
    "\n",
    "            if not green_is_zero:\n",
    "                green_mask = abs(green) <= 1e-9\n",
    "                green = np.sign(green) * (abs(green) ** gamma)\n",
    "                green[green_mask] = 0\n",
    "\n",
    "            if not blue_is_zero:\n",
    "                blue_mask = abs(blue) <= 1e-9\n",
    "                blue = np.sign(blue) * (abs(blue) ** gamma)\n",
    "                blue[blue_mask] = 0\n",
    "        # Stack the channels after scaling and gamma correction\n",
    "        result = np.stack([red, green, blue], axis=-1).astype(np.float32)\n",
    "\n",
    "    # back to original axis ordering\n",
    "    result = np.moveaxis(result, -1, 0)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def adjust_flux_with_zp(\n",
    "    flux: np.ndarray, current_zp: Union[float, int], standard_zp: Union[float, int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adjust flux values to a standard zero-point.\n",
    "\n",
    "    Args:\n",
    "        flux (numpy.ndarray): Flux values to adjust.\n",
    "        current_zp (float/int): Current zero-point of the flux values.\n",
    "        standard_zp (float/int): Standard zero-point to adjust to.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Adjusted flux values.\n",
    "    \"\"\"\n",
    "    adjusted_flux = flux * 10 ** (-0.4 * (current_zp - standard_zp))\n",
    "    return adjusted_flux\n",
    "\n",
    "\n",
    "def detect_anomaly(\n",
    "    image: np.ndarray,\n",
    "    zero_threshold: float = 0.05,\n",
    "    min_size: int = 50,\n",
    "    replace_anomaly: bool = True,\n",
    "    dilate_mask: bool = True,\n",
    "    dilation_iters: int = 1,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Detect and replace anomalies in an image using wavelet decomposition.\n",
    "\n",
    "    This function analyzes an astronomical image to identify anomalous regions\n",
    "    by performing wavelet decomposition and identifying regions with minimal\n",
    "    fluctuations below a threshold. It can optionally replace detected anomalous\n",
    "    pixels with zeros.\n",
    "\n",
    "    Args:\n",
    "        image: Input astronomical image to process\n",
    "        zero_threshold: Fluctuation threshold below which an anomaly is detected\n",
    "        min_size: Minimum connected pixel count to be considered an anomaly\n",
    "        replace_anomaly: Whether to set anomalous pixels to zero\n",
    "        dilate_mask: Whether to expand the detected anomaly mask\n",
    "        dilation_iters: Number of dilation iterations if dilate_mask is True\n",
    "\n",
    "    Returns:\n",
    "        Processed image with anomalies optionally replaced\n",
    "\n",
    "    Notes:\n",
    "        This function uses Haar wavelet decomposition to identify regions with\n",
    "        suspiciously low variation, which often indicate detector artifacts or\n",
    "        other non-astronomical features in the image.\n",
    "    \"\"\"\n",
    "    # replace nan values with zeros\n",
    "    image[np.isnan(image)] = 0.0\n",
    "\n",
    "    # Perform a 2D Discrete Wavelet Transform using Haar wavelets\n",
    "    coeffs = pywt.dwt2(image, 'haar')\n",
    "    cA, (cH, cV, cD) = coeffs  # Decomposition into approximation and details\n",
    "\n",
    "    # Create binary masks where wavelet coefficients are below the threshold\n",
    "    mask_horizontal = np.abs(cH) <= zero_threshold\n",
    "    mask_vertical = np.abs(cV) <= zero_threshold\n",
    "    mask_diagonal = np.abs(cD) <= zero_threshold\n",
    "\n",
    "    masks = [mask_diagonal, mask_horizontal, mask_vertical]\n",
    "\n",
    "    # Create a global mask to accumulate all anomalies\n",
    "    global_mask = np.zeros_like(image, dtype=bool)\n",
    "    # Create masks for each component\n",
    "    component_masks = np.zeros((3, cA.shape[0], cA.shape[1]), dtype=bool)\n",
    "    anomalies = np.zeros(3, dtype=bool)\n",
    "    for i, mask in enumerate(masks):\n",
    "        # Apply connected-component labeling to find connected regions in the mask\n",
    "        labeled_array, num_features = label(mask)  # type: ignore\n",
    "\n",
    "        # Calculate the sizes of all components\n",
    "        component_sizes = np.bincount(labeled_array.ravel())\n",
    "\n",
    "        # Check if any component is larger than the minimum size\n",
    "        anomaly_detected = np.any(component_sizes[1:] >= min_size)\n",
    "        anomalies[i] = anomaly_detected\n",
    "\n",
    "        if not anomaly_detected:\n",
    "            continue\n",
    "\n",
    "        # Prepare to accumulate a total mask\n",
    "        total_feature_mask = np.zeros_like(image, dtype=bool)\n",
    "\n",
    "        # Loop through all labels to find significant components\n",
    "        for component_label in range(1, num_features + 1):  # Start from 1 to skip background\n",
    "            if component_sizes[component_label] >= min_size:\n",
    "                # Create a binary mask for this component\n",
    "                component_mask = labeled_array == component_label\n",
    "                # add component mask to component masks\n",
    "                component_masks[i] |= component_mask\n",
    "                # Upscale the mask to match the original image dimensions\n",
    "                upscaled_mask = np.kron(component_mask, np.ones((2, 2), dtype=bool))\n",
    "                # Accumulate the upscaled feature mask\n",
    "                total_feature_mask |= upscaled_mask\n",
    "\n",
    "        # Accumulate global mask\n",
    "        global_mask |= total_feature_mask\n",
    "        # Dilate the masks to catch some odd pixels on the outskirts of the anomaly\n",
    "        if dilate_mask:\n",
    "            global_mask = binary_dilation(global_mask, iterations=dilation_iters)\n",
    "            for j, comp_mask in enumerate(component_masks):\n",
    "                component_masks[j] = binary_dilation(comp_mask, iterations=dilation_iters)\n",
    "    # Replace the anomaly with zeros\n",
    "    if replace_anomaly:\n",
    "        image[global_mask] = 0.0\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_h5_images(\n",
    "    h5_file_path,\n",
    "    output_dir=None,\n",
    "    replace_anomaly=True,\n",
    "    scaling_type='asinh',\n",
    "    mode='vis',\n",
    "    stretch=125,\n",
    "    Q=7.0,\n",
    "    gamma=0.25,\n",
    "    use_compression=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Read an H5 file containing image cutouts, preprocess them, and save to a new H5 file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    h5_file_path : str\n",
    "        Path to the input H5 file containing image cutouts.\n",
    "    output_dir : str, optional\n",
    "        Directory where the processed H5 file will be saved. If None, uses same directory as input.\n",
    "    replace_anomaly : bool, optional\n",
    "        Whether to replace anomalies in the images. Default is True.\n",
    "    scaling_type : str, optional\n",
    "        Type of scaling to apply (\"asinh\" or \"linear\"). Default is \"asinh\".\n",
    "    stretch : float, optional\n",
    "        Scaling factor controlling overall brightness. Default is 125.\n",
    "    Q : float, optional\n",
    "        Softening parameter for asinh scaling. Default is 7.0.\n",
    "    gamma : float, optional\n",
    "        Gamma correction factor. Default is 0.25.\n",
    "    use_compression : bool, optional\n",
    "        Whether to compress the output datasets. Default is False for faster read\n",
    "        times during deep learning training.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to the output H5 file.\n",
    "    \"\"\"\n",
    "    # Set output directory and filename\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(h5_file_path)\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Generate output filename\n",
    "    base_name = os.path.basename(h5_file_path)\n",
    "    output_filename = os.path.splitext(base_name)[0] + '_prep.h5'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "    print(f'Processing {h5_file_path} -> {output_path}')\n",
    "\n",
    "    # Open input file for reading\n",
    "    with h5py.File(h5_file_path, 'r') as src_file:\n",
    "        # Get image data\n",
    "        images = src_file['images'][:]\n",
    "        num_objects = images.shape[0]\n",
    "\n",
    "        print(f'Found {num_objects} objects with image shape {images.shape[1:]}')\n",
    "\n",
    "        # Create output file\n",
    "        with h5py.File(output_path, 'w') as out_file:\n",
    "            # Process images\n",
    "            processed_images = np.zeros(\n",
    "                (num_objects, 3, images.shape[2], images.shape[3]), dtype=np.float32\n",
    "            )\n",
    "\n",
    "            print('Processing images...')\n",
    "            for i in tqdm(range(num_objects)):\n",
    "                # Get the current cutout (with shape [channels, height, width])\n",
    "                cutout = images[i]\n",
    "\n",
    "                # Apply preprocessing to convert to RGB format\n",
    "                preprocessed = preprocess_cutout(cutout, mode=mode, replace_anomaly=replace_anomaly)\n",
    "\n",
    "                # Apply RGB scaling and normalization\n",
    "                rgb_image = generate_rgb(\n",
    "                    preprocessed,\n",
    "                    scaling_type=scaling_type,\n",
    "                    stretch=stretch,\n",
    "                    Q=Q,\n",
    "                    gamma=gamma,\n",
    "                )\n",
    "\n",
    "                # Store processed image\n",
    "                processed_images[i] = rgb_image\n",
    "\n",
    "            # Save processed images to output file\n",
    "            if use_compression:\n",
    "                out_file.create_dataset(\n",
    "                    'images',\n",
    "                    data=processed_images,\n",
    "                    compression='gzip',\n",
    "                    compression_opts=6,\n",
    "                )\n",
    "            else:\n",
    "                # No compression for faster read times during training\n",
    "                out_file.create_dataset('images', data=processed_images)\n",
    "\n",
    "            # Copy all other datasets from source file\n",
    "            print('Copying metadata and other datasets...')\n",
    "            for key in src_file.keys():\n",
    "                if key != 'images' and key not in out_file:\n",
    "                    src_file.copy(key, out_file)\n",
    "\n",
    "            # Add processing parameters as attributes\n",
    "            out_file.attrs['preprocessing_replace_anomaly'] = replace_anomaly\n",
    "            out_file.attrs['rgb_scaling_type'] = scaling_type\n",
    "            out_file.attrs['rgb_stretch'] = stretch\n",
    "            out_file.attrs['rgb_Q'] = Q\n",
    "            out_file.attrs['rgb_gamma'] = gamma\n",
    "            out_file.attrs['compression_used'] = use_compression\n",
    "\n",
    "    print(f'Processing complete. Output saved to {output_path}')\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21222d1-3b43-4c37-944c-d31e2695c4bc",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e25a80-c6cb-4adf-b117-6c58450db419",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/arc/projects/unions/ssl/data/raw/tiles/dwarforge'\n",
    "table_dir = '/arc/home/heestersnick/dwarforge/tables'\n",
    "output_dir = '/arc/home/heestersnick/dwarforge/desi'\n",
    "desi_unions_path = os.path.join(table_dir, 'all_desi_unions_matched.csv')\n",
    "desi_unions_matched = pd.read_csv(desi_unions_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05587895-41d5-4cf5-8ab0-39bdcd783e8c",
   "metadata": {},
   "source": [
    "# Collect matched object data & save to new file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b396655-167d-4fc0-aa62-2c1096cef877",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_and_save_objects(catalog_path=desi_unions_path, output_dir=output_dir, base_path=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2289bf-2c32-457c-b042-f2c91735e529",
   "metadata": {},
   "source": [
    "# Make data RGB-ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338bd23-0e95-4961-945a-f77472f6f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file_path = os.path.join(output_dir, 'chunk_0004.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d668a0-8a98-4aaf-99a5-4c180fd5c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = preprocess_h5_images(\n",
    "    h5_file_path,\n",
    "    output_dir=output_dir,\n",
    "    replace_anomaly=True,\n",
    "    scaling_type='asinh',\n",
    "    stretch=125,\n",
    "    Q=7.0,\n",
    "    gamma=0.25,\n",
    "    use_compression=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d953c0-bb7d-4fb8-a00a-d9e022091147",
   "metadata": {},
   "source": [
    "# Check RGB-ready file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01434299-70a4-4d71-837f-35f6dd1c17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orig = read_h5('/arc/home/heestersnick/dwarforge/desi/chunk_0004.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c72c54-c27a-4c9e-94d1-be78b3bdbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4256fa-baa0-44ff-ad51-b9cda1814f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3083\n",
    "\n",
    "z = test['Z'][idx]\n",
    "ra, dec = test['ra'][idx], test['dec'][idx]\n",
    "zoobot_pred = test['zoobot_pred'][idx]\n",
    "\n",
    "cutout = test['images'][idx]\n",
    "cutout = np.moveaxis(cutout, 0, -1)\n",
    "cutout = np.clip(cutout, 0, 1)\n",
    "\n",
    "plt.figure(figsize=(6, 6), frameon=False, clear=True)\n",
    "plt.imshow(cutout, origin='lower')\n",
    "plt.show()\n",
    "\n",
    "print(f'Object is at ({ra:.4f}, {dec:.4f}) at a redshift of {z:.4f}')\n",
    "print(f'Zoobot pred: {zoobot_pred:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835c0bc-0539-4301-ae08-aa0b71dd25aa",
   "metadata": {},
   "source": [
    "# Get Zoobot latent vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5ee95-50d8-4672-ba2a-1d4a958928ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zoobot_model(name='hf_hub:mwalmsley/zoobot-encoder-convnext_nano'):\n",
    "    model = ZoobotEncoder.load_from_name(name)\n",
    "    model.freeze()\n",
    "    model.eval()\n",
    "    model = model.to(DEVICE)\n",
    "    print(\n",
    "        f'Model is on device: {next(model.parameters()).device}.\\nModel is in eval mode: {all(not p.requires_grad for p in model.parameters())}.\\nModel architecture is: {model.encoder.default_cfg[\"architecture\"]}'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_inference(model, images_np, batch_size=256):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images_np: numpy array of shape (N, 3, H, W)\n",
    "\n",
    "    Returns:\n",
    "        latent representations: numpy array of shape (N,640)\n",
    "    \"\"\"\n",
    "    # 1. Verify input normalization\n",
    "    assert images_np.dtype == np.float32, 'Input must be float32'\n",
    "\n",
    "    # 2. Convert to tensor\n",
    "    images_tensor = torch.tensor(images_np, dtype=torch.float32)\n",
    "\n",
    "    # 3. Create DataLoader\n",
    "    dataset = TensorDataset(images_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # 4. Run inference\n",
    "    latent_representations = []\n",
    "    model.eval()  # Redundant with freeze() but safe\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            inputs = batch[0].to(model.device)\n",
    "            latent = model(inputs)\n",
    "            latent_representations.append(latent.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(latent_representations)\n",
    "\n",
    "\n",
    "def process_objects_with_model(\n",
    "    catalog_path: str,\n",
    "    output_path: str,\n",
    "    model: Any,  # Ideally replace Any with torch.nn.Module or specific model type\n",
    "    base_path: str = '/projects/unions/ssl/data/raw/tiles/dwarforge',\n",
    "    accumulation_size: int = 5000,\n",
    "    preprocessing_params: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Process objects from a catalog with a machine learning model and save results.\n",
    "\n",
    "    Uses efficient HDF5 slicing for image loading and ensures metadata alignment\n",
    "    by sorting both images and corresponding metadata based on HDF5 index order.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    catalog_path : str\n",
    "        Path to the catalog CSV file. Requires columns:\n",
    "        'tile', 'unique_id', 'ra', 'dec', 'Z', 'LOGM_CIGALE'.\n",
    "    output_path : str\n",
    "        Path to the output H5 file where results will be saved.\n",
    "    model : object\n",
    "        The machine learning model for latent vector extraction.\n",
    "    base_path : str, optional\n",
    "        Base path to the directory containing tile subdirectories.\n",
    "    accumulation_size : int, optional\n",
    "        Number of preprocessed images to accumulate before running inference. Default is 5000.\n",
    "    preprocessing_params : dict, optional\n",
    "        Parameters for image preprocessing. Defaults will be used if None.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Statistics about the processing (objects processed, not found, etc.)\n",
    "    \"\"\"\n",
    "    # 1. Load and prepare the catalog\n",
    "    print('Loading catalog...')\n",
    "    catalog = pd.read_csv(catalog_path, low_memory=False)\n",
    "\n",
    "    # Check if required columns exist\n",
    "    required_cols = ['tile', 'unique_id', 'ra', 'dec', 'z', 'zerr']\n",
    "    for col in required_cols:\n",
    "        if col not in catalog.columns:\n",
    "            raise ValueError(f\"Required column '{col}' not found in catalog\")\n",
    "\n",
    "    # Ensure unique_id is integer type if needed for matching H5 IDs\n",
    "    # catalog['unique_id'] = catalog['unique_id'].astype(int) # Uncomment if necessary\n",
    "\n",
    "    # Create output directory if needed\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set default preprocessing parameters if not provided\n",
    "    if preprocessing_params is None:\n",
    "        preprocessing_params = {\n",
    "            'replace_anomaly': True,\n",
    "            'scaling_type': 'asinh',\n",
    "            'mode': 'vis',\n",
    "            'stretch': 125,\n",
    "            'Q': 7.0,\n",
    "            'gamma': 0.25,\n",
    "        }\n",
    "\n",
    "    # 2. Group objects by tile\n",
    "    print('Grouping objects by tile...')\n",
    "    grouped_catalog = catalog.groupby('tile')\n",
    "\n",
    "    # Initialize data collection\n",
    "    latent_vectors: List[np.ndarray] = []\n",
    "    metadata = {\n",
    "        'unique_id': [],\n",
    "        'ra': [],\n",
    "        'dec': [],\n",
    "        'tile': [],\n",
    "        'z': [],\n",
    "        'zerr': [],\n",
    "    }  # Lists to hold batches of metadata arrays\n",
    "\n",
    "    # Counters for statistics\n",
    "    total_objects = len(catalog)\n",
    "    objects_processed = 0\n",
    "    objects_not_found = 0\n",
    "\n",
    "    # Accumulation buffers\n",
    "    accumulated_images: List[np.ndarray] = []\n",
    "    accumulated_metadata = {\n",
    "        key: [] for key in metadata.keys()\n",
    "    }  # Lists to hold batches before inference\n",
    "\n",
    "    print(f'Processing {len(grouped_catalog)} unique tiles with {total_objects} total objects...')\n",
    "\n",
    "    # 3. Process each tile\n",
    "    for tile, tile_objects in tqdm(grouped_catalog, desc='Processing tiles'):\n",
    "        tile_formatted = str(tile)\n",
    "        # Construct file path (adjust if structure differs)\n",
    "        tile_path = (\n",
    "            f'{base_path}/{tile_formatted}/gri/{tile_formatted}_matched_cutouts_full_res_final.h5'\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            with h5py.File(tile_path, 'r') as src_file:\n",
    "                # --- Efficient Object Selection & Loading ---\n",
    "\n",
    "                # 1. Get target IDs required for this tile from the catalog\n",
    "                target_ids = tile_objects['unique_id'].values\n",
    "                num_targets_in_catalog = len(target_ids)\n",
    "\n",
    "                # 2. Load unique IDs available in the H5 file\n",
    "                if 'unique_id' not in src_file:\n",
    "                    print(\n",
    "                        f\"Warning: 'unique_id' dataset not found in tile {tile_formatted}. Skipping.\"\n",
    "                    )\n",
    "                    objects_not_found += num_targets_in_catalog\n",
    "                    continue\n",
    "                h5_unique_ids = src_file['unique_id'][:]\n",
    "                # Ensure h5_unique_ids is 1D array\n",
    "                if h5_unique_ids.ndim == 0:\n",
    "                    h5_unique_ids = np.array([h5_unique_ids])\n",
    "                if h5_unique_ids.ndim > 1:\n",
    "                    h5_unique_ids = h5_unique_ids.flatten()  # Adjust if needed\n",
    "\n",
    "                # 3. Find matches: Map H5 IDs to their indices\n",
    "                h5_id_to_index = {uid: idx for idx, uid in enumerate(h5_unique_ids)}\n",
    "\n",
    "                # 4. Identify indices in H5 and corresponding catalog rows\n",
    "                valid_indices: List[int] = []  # Indices within H5 file\n",
    "                catalog_indices_to_keep: List[int] = []  # Indices within tile_objects\n",
    "\n",
    "                for cat_idx, target_id in enumerate(target_ids):\n",
    "                    h5_idx = h5_id_to_index.get(target_id)  # More efficient than 'in' + lookup\n",
    "                    if h5_idx is not None:\n",
    "                        valid_indices.append(h5_idx)\n",
    "                        catalog_indices_to_keep.append(cat_idx)\n",
    "\n",
    "                n_valid = len(valid_indices)\n",
    "                num_not_found_in_tile = num_targets_in_catalog - n_valid\n",
    "                objects_not_found += num_not_found_in_tile\n",
    "\n",
    "                if n_valid == 0:\n",
    "                    if num_targets_in_catalog > 0:\n",
    "                        print(\n",
    "                            f'Warning: No matching objects found in file {tile_path} for {num_targets_in_catalog} catalog entries.'\n",
    "                        )\n",
    "                    continue  # Skip to the next tile\n",
    "\n",
    "                # 5. Get initial metadata rows in the original discovery order\n",
    "                valid_catalog_objects = tile_objects.iloc[catalog_indices_to_keep]\n",
    "\n",
    "                # --- Sorting for Efficient HDF5 Read & Metadata Alignment ---\n",
    "                # 6. Get the order needed to sort the H5 indices\n",
    "                valid_indices_arr = np.array(valid_indices)\n",
    "                sort_order = np.argsort(valid_indices_arr)\n",
    "\n",
    "                # 7. Apply sort order to get sorted H5 indices\n",
    "                sorted_h5_indices = valid_indices_arr[sort_order]\n",
    "\n",
    "                # 8. Apply the SAME sort order to the metadata DataFrame\n",
    "                valid_catalog_objects_sorted = valid_catalog_objects.iloc[sort_order]\n",
    "                # --- End Sorting ---\n",
    "\n",
    "                # 9. Extract only the required images using sorted H5 indices\n",
    "                if 'images' not in src_file:\n",
    "                    print(\n",
    "                        f\"Warning: 'images' dataset not found in tile {tile_formatted}. Skipping {n_valid} found objects.\"\n",
    "                    )\n",
    "                    objects_not_found += n_valid  # Count these as not found now\n",
    "                    continue\n",
    "                images = src_file['images'][sorted_h5_indices]\n",
    "                # 'images' now corresponds row-by-row to 'valid_catalog_objects_sorted'\n",
    "\n",
    "                # 10. Preprocess all extracted images for this tile\n",
    "                # Assuming H, W are last dims in H5, C=3 is output of generate_rgb\n",
    "                preprocessed_images = np.zeros(\n",
    "                    (n_valid, 3, images.shape[-2], images.shape[-1]), dtype=np.float32\n",
    "                )\n",
    "\n",
    "                for i in range(n_valid):\n",
    "                    cutout = images[i]  # Already loaded efficiently\n",
    "                    preprocessed = preprocess_cutout(\n",
    "                        cutout,\n",
    "                        mode=preprocessing_params['mode'],\n",
    "                        replace_anomaly=preprocessing_params['replace_anomaly'],\n",
    "                    )\n",
    "                    rgb_image = generate_rgb(\n",
    "                        preprocessed,\n",
    "                        scaling_type=preprocessing_params['scaling_type'],\n",
    "                        stretch=preprocessing_params['stretch'],\n",
    "                        Q=preprocessing_params['Q'],\n",
    "                        gamma=preprocessing_params['gamma'],\n",
    "                    )\n",
    "                    preprocessed_images[i] = rgb_image\n",
    "\n",
    "                # Set problematic values to 0\n",
    "                preprocessed_images_clean = np.nan_to_num(\n",
    "                    preprocessed_images, nan=0.0, posinf=0.0, neginf=0.0\n",
    "                )\n",
    "\n",
    "                # 11. Add preprocessed images to accumulation buffer\n",
    "                accumulated_images.append(preprocessed_images_clean)\n",
    "\n",
    "                # 12. Add SORTED metadata to accumulation buffer\n",
    "                accumulated_metadata['unique_id'].append(\n",
    "                    valid_catalog_objects_sorted['unique_id'].values\n",
    "                )\n",
    "                accumulated_metadata['ra'].append(valid_catalog_objects_sorted['ra'].values)\n",
    "                accumulated_metadata['dec'].append(valid_catalog_objects_sorted['dec'].values)\n",
    "                accumulated_metadata['tile'].append(\n",
    "                    np.full(n_valid, tile)\n",
    "                )  # Use the actual tile variable\n",
    "                accumulated_metadata['z'].append(valid_catalog_objects_sorted['z'].values)\n",
    "                accumulated_metadata['zerr'].append(valid_catalog_objects_sorted['zerr'].values)\n",
    "\n",
    "                # 13. Calculate total accumulated images\n",
    "                total_accumulated = sum(arr.shape[0] for arr in accumulated_images)\n",
    "\n",
    "                # 14. Run inference when accumulation threshold is met (Original logic)\n",
    "                # Consider optimizing this part later if it becomes a bottleneck\n",
    "                while total_accumulated >= accumulation_size:\n",
    "                    print(f'Running inference on {accumulation_size} accumulated images...')\n",
    "\n",
    "                    # Concatenate accumulated images (Potential inefficiency here)\n",
    "                    all_images = np.concatenate(accumulated_images)\n",
    "                    inference_images = all_images[:accumulation_size]\n",
    "\n",
    "                    try:\n",
    "                        batch_vectors = run_inference(\n",
    "                            model=model, images_np=np.nan_to_num(inference_images, nan=0.0)\n",
    "                        )\n",
    "                        if np.isnan(batch_vectors).any():\n",
    "                            print('!!! WARNING: NaN values detected in latent vectors for a batch.')\n",
    "                        latent_vectors.append(batch_vectors)  # Add results batch\n",
    "\n",
    "                        # Process metadata for these images (Potential inefficiency here)\n",
    "                        remaining_metadata = {key: [] for key in metadata.keys()}\n",
    "                        processed_count_in_batch = (\n",
    "                            0  # Track how much metadata corresponds to inference_images\n",
    "                        )\n",
    "\n",
    "                        for key in metadata.keys():\n",
    "                            # Concatenate all buffered metadata for this key\n",
    "                            all_metadata_values_list = accumulated_metadata[key]\n",
    "                            if not all_metadata_values_list:\n",
    "                                continue  # Skip if empty\n",
    "\n",
    "                            all_values_key = np.concatenate(all_metadata_values_list)\n",
    "\n",
    "                            # Store metadata for processed images\n",
    "                            metadata[key].append(all_values_key[:accumulation_size])\n",
    "                            processed_count_in_batch = len(\n",
    "                                all_values_key[:accumulation_size]\n",
    "                            )  # Update count\n",
    "\n",
    "                            # Keep remaining metadata if any\n",
    "                            if len(all_values_key) > accumulation_size:\n",
    "                                remaining_values = all_values_key[accumulation_size:]\n",
    "                                remaining_metadata[key] = [remaining_values]  # Store as list\n",
    "                            else:\n",
    "                                remaining_metadata[key] = []  # No remainder\n",
    "\n",
    "                        # Keep remaining images\n",
    "                        remaining_images = all_images[accumulation_size:]\n",
    "\n",
    "                        # Update accumulation buffers\n",
    "                        accumulated_images = [remaining_images] if len(remaining_images) > 0 else []\n",
    "                        accumulated_metadata = remaining_metadata\n",
    "\n",
    "                        objects_processed += processed_count_in_batch  # Use actual count\n",
    "\n",
    "                        # Update total accumulated count *after* buffer update\n",
    "                        total_accumulated = sum(arr.shape[0] for arr in accumulated_images)\n",
    "\n",
    "                        # Free GPU memory if available\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f'Error running inference batch: {type(e).__name__} - {e}. Skipping batch.'\n",
    "                        )\n",
    "                        # Decide how to handle buffers - clear them or try to skip bad data?\n",
    "                        # Simplest: Clear buffers and hope next tile works.\n",
    "                        accumulated_images = []\n",
    "                        accumulated_metadata = {key: [] for key in metadata.keys()}\n",
    "                        total_accumulated = 0\n",
    "                        break  # Exit the while loop for this tile after error\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            num_expected = len(tile_objects)\n",
    "            print(\n",
    "                f'Warning: File not found for tile {tile_formatted} at {tile_path}. Skipping {num_expected} objects.'\n",
    "            )\n",
    "            objects_not_found += num_expected\n",
    "        except KeyError as e:\n",
    "            print(\n",
    "                f'Error: Dataset missing in H5 file for tile {tile_formatted} (Path: {tile_path}). Missing key: {e}'\n",
    "            )\n",
    "            objects_not_found += len(tile_objects)\n",
    "        except Exception as e:\n",
    "            print(f'Error processing tile {tile_formatted}: {type(e).__name__} - {e}')\n",
    "            objects_not_found += len(tile_objects)  # Option: count errors as not found\n",
    "\n",
    "    # Process any remaining images after the loop (Original logic)\n",
    "    final_accumulated_count = sum(arr.shape[0] for arr in accumulated_images)\n",
    "    if final_accumulated_count > 0:\n",
    "        print(f'Running inference on {final_accumulated_count} remaining images...')\n",
    "        try:\n",
    "            all_images = np.concatenate(accumulated_images)\n",
    "            batch_vectors = run_inference(model=model, images_np=all_images)\n",
    "            latent_vectors.append(batch_vectors)\n",
    "\n",
    "            # Process remaining metadata\n",
    "            for key in metadata.keys():\n",
    "                if accumulated_metadata[key]:  # Check if list is not empty\n",
    "                    all_values = np.concatenate(accumulated_metadata[key])\n",
    "                    metadata[key].append(all_values)\n",
    "                # Else: no remaining metadata for this key, do nothing\n",
    "\n",
    "            objects_processed += final_accumulated_count\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing final batch of images: {type(e).__name__} - {e}')\n",
    "            # Note: These objects won't be counted in objects_processed if error occurs here\n",
    "\n",
    "    # Concatenate all collected data (Original logic)\n",
    "    print('Concatenating final results...')\n",
    "    all_latent_vectors = (\n",
    "        np.concatenate(latent_vectors) if latent_vectors else np.empty((0, 640), dtype=np.float32)\n",
    "    )  # Specify shape if empty\n",
    "\n",
    "    all_metadata = {}\n",
    "    for key in metadata:\n",
    "        if metadata[key]:  # Check list has batches\n",
    "            all_metadata[key] = np.concatenate(metadata[key])\n",
    "        else:  # Create empty array of appropriate type if nothing was processed\n",
    "            if key == 'unique_id':\n",
    "                dtype = np.int64\n",
    "            elif key in ['ra', 'dec', 'z', 'zerr']:\n",
    "                dtype = np.float64\n",
    "            elif key == 'tile':\n",
    "                dtype = object  # Or infer from catalog dtype\n",
    "            else:\n",
    "                dtype = object\n",
    "            all_metadata[key] = np.array([], dtype=dtype)\n",
    "\n",
    "    # Check consistency (optional)\n",
    "    num_vecs = len(all_latent_vectors)\n",
    "    num_ids = len(all_metadata.get('unique_id', []))\n",
    "    if num_vecs != num_ids:\n",
    "        print(\n",
    "            f'!!!WARNING!!! Mismatch between number of latent vectors ({num_vecs}) and unique IDs ({num_ids}). Check accumulation logic.'\n",
    "        )\n",
    "        # Fallback: Try to use the minimum length if saving? Or raise error?\n",
    "        min_len = min(num_vecs, num_ids)\n",
    "        all_latent_vectors = all_latent_vectors[:min_len]\n",
    "        for key in all_metadata:\n",
    "            all_metadata[key] = all_metadata[key][:min_len]\n",
    "        objects_processed = min_len  # Adjust count\n",
    "\n",
    "    # Create the output H5 file (Original logic)\n",
    "    print(f'Saving results for {len(all_latent_vectors)} objects to {output_path}...')\n",
    "    with h5py.File(output_path, 'w') as out_file:\n",
    "        out_file.create_dataset('latent_vectors', data=all_latent_vectors)\n",
    "\n",
    "        for key, data in all_metadata.items():\n",
    "            # Handle string/object data saving if needed (e.g., tile)\n",
    "            if data.dtype == object or data.dtype.kind in 'SU':  # String or object\n",
    "                data = data.astype(h5py.string_dtype(encoding='utf-8'))\n",
    "            out_file.create_dataset(key, data=data)\n",
    "\n",
    "        # Add attributes\n",
    "        out_file.attrs['processing_date'] = str(datetime.datetime.now())\n",
    "        out_file.attrs['catalog_path'] = catalog_path\n",
    "        out_file.attrs['image_base_path'] = base_path\n",
    "        out_file.attrs['total_objects_in_catalog'] = total_objects\n",
    "        out_file.attrs['objects_successfully_processed'] = (\n",
    "            objects_processed  # Use final reliable count\n",
    "        )\n",
    "        out_file.attrs['objects_not_found_or_skipped'] = objects_not_found\n",
    "\n",
    "        for key, value in preprocessing_params.items():\n",
    "            # Attempt to save param; convert complex types if necessary\n",
    "            try:\n",
    "                out_file.attrs[f'preprocessing_{key}'] = value\n",
    "            except TypeError:\n",
    "                print(\n",
    "                    f\"Warning: Could not save preprocessing param '{key}'={value} as HDF5 attribute.\"\n",
    "                )\n",
    "                out_file.attrs[f'preprocessing_{key}'] = str(value)\n",
    "\n",
    "    # Print statistics\n",
    "    print('\\nProcessing complete:')\n",
    "    print(f'  Total objects in catalog: {total_objects}')\n",
    "    print(f'  Objects successfully processed (latent vectors generated): {objects_processed}')\n",
    "    print(f'  Objects not found in H5 files or skipped due to errors: {objects_not_found}')\n",
    "    # Note: total_objects might not equal processed + not_found if errors occurred during inference/saving\n",
    "\n",
    "    # Return statistics\n",
    "    return {\n",
    "        'total_objects': total_objects,\n",
    "        'objects_processed': objects_processed,\n",
    "        'objects_not_found_or_skipped': objects_not_found,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0b652-39b9-4b41-bf90-1a5c65bd21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DTYPE = torch.float32\n",
    "EPSILON = 1e-8\n",
    "\n",
    "model = load_zoobot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446f29b-0391-4ccb-9dd6-2c97d1cc06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_objects_with_model(\n",
    "    catalog_path=desi_unions_path,\n",
    "    output_path=os.path.join(output_dir, 'desi_unions_latent.h5'),\n",
    "    model=model,\n",
    "    base_path=data_dir,\n",
    "    accumulation_size=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8cb257-9534-4daf-96f2-4e253a559c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a1f0a-65a3-4b1e-a1d2-0bb576453217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38910d-6f79-4d19-96ab-39763b204644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806eb62d-412f-4f6b-9f15-5fcc3c99fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk = read_h5(os.path.join(output_dir, 'chunk_0000.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c141dc-eac8-4a65-b169-b1dccdaa8bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = test_chunk['images']\n",
    "test_latent = run_inference(model, test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797f13e-4422-4061-b59f-ce90aa9e303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ee65a-67c5-460d-9520-2aaae2e33910",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 501\n",
    "cutout = test_chunk['images'][idx]\n",
    "p = test_chunk['zoobot_pred'][idx]\n",
    "ra = test_chunk['ra'][idx]\n",
    "dec = test_chunk['dec'][idx]\n",
    "z = test_chunk['Z'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672cb267-8fe3-42ce-af80-f99f9ffc20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cutout(cutout):\n",
    "    plt.figure(figsize=(6, 6), frameon=False, clear=True)\n",
    "    cutout_prep = preprocess_cutout(cutout, mode='train', replace_anomaly=True)\n",
    "    cutout_rgb = generate_rgb(cutout_prep)\n",
    "    cutout_img = np.moveaxis(cutout_rgb, 0, -1)\n",
    "    plt.imshow(np.clip(cutout_img, 0, 1), origin='lower')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    # return cutout_rgb\n",
    "    return np.clip(cutout_rgb, 0, 1)\n",
    "\n",
    "\n",
    "print(f'coords: {ra:.4f} {dec:.4f}')\n",
    "print(f'p: {p:.3f}')\n",
    "print(f'z: {z:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea1b5f-13b0-4627-ab7f-f05fdab1519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_rgb = plot_cutout(cutout)\n",
    "cutout_model = torch.tensor(cutout_rgb)\n",
    "cutout_model = cutout_model.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6521f1-9dee-4fd4-b812-e56357b44120",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_no_synth = model(cutout_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cb72e-4a46-4b83-b96a-c106185e587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(test_output_synth, test_output_no_synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48ef5e-5945-4603-8e85-506b95005f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "x = np.arange(test_output.shape[1])\n",
    "plt.plot(x, test_output_synth.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2726480-b85d-46c3-a193-a008bf206b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "x = np.arange(test_output.shape[1])\n",
    "plt.plot(x, test_output_no_synth.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f6034-911d-4b2c-97e1-2f58f2c37306",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a12ae1-375b-4927-8fa9-4b69eff05177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
