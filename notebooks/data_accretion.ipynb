{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5664747-449e-4bc1-9f21-7309909d8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import simple_norm\n",
    "from astropy.wcs import WCS\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dwarforge.tile_cutter import read_h5\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# color blind friendly palette for plotting\n",
    "colors = {\n",
    "    'blue': '#377eb8',\n",
    "    'orange': '#ff7f00',\n",
    "    'green': '#4daf4a',\n",
    "    'pink': '#f781bf',\n",
    "    'brown': '#a65628',\n",
    "    'purple': '#984ea3',\n",
    "    'gray': '#999999',\n",
    "    'red': '#e41a1c',\n",
    "    'yellow': '#dede00',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_path = Path('/home/nick/astro/DwarForge/data/234_295/gri/234_295_matched_detections.parquet')\n",
    "h5_path = Path(\n",
    "    '/home/nick/astro/DwarForge/data/234_295/gri/234_295_matched_cutouts_full_res_final.h5'\n",
    ")\n",
    "fits_path = Path('/home/nick/astro/DwarForge/data/234_295/whigs-g/calexp-CFIS_234_295.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b558b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = pd.read_parquet(det_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4053896",
   "metadata": {},
   "outputs": [],
   "source": [
    "det = detections.head()\n",
    "det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b324bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [1, 3, 4]\n",
    "det.loc[rows, 'ID_cfis_lsb-r'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebe1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections[\n",
    "    [\n",
    "        'ID_whigs-g',\n",
    "        'X_whigs-g',\n",
    "        'Y_whigs-g',\n",
    "        'A_whigs-g',\n",
    "        'B_whigs-g',\n",
    "        'theta_whigs-g',\n",
    "        'total_flux_whigs-g',\n",
    "        'mu_max_whigs-g',\n",
    "        'mu_median_whigs-g',\n",
    "        'mu_mean_whigs-g',\n",
    "        'R_fwhm_whigs-g',\n",
    "        'R_e_whigs-g',\n",
    "        'R10_whigs-g',\n",
    "        'R25_whigs-g',\n",
    "        'R75_whigs-g',\n",
    "        'R90_whigs-g',\n",
    "        'R100_whigs-g',\n",
    "        'n_pix_whigs-g',\n",
    "        'ra_whigs-g',\n",
    "        'dec_whigs-g',\n",
    "        're_arcsec_whigs-g',\n",
    "        'r_fwhm_arcsec_whigs-g',\n",
    "        'r_10_arcsec_whigs-g',\n",
    "        'r_25_arcsec_whigs-g',\n",
    "        'r_75_arcsec_whigs-g',\n",
    "        'r_90_arcsec_whigs-g',\n",
    "        'r_100_arcsec_whigs-g',\n",
    "        'A_arcsec_whigs-g',\n",
    "        'B_arcsec_whigs-g',\n",
    "        'axis_ratio_whigs-g',\n",
    "        'mag_whigs-g',\n",
    "        'mu_whigs-g',\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutouts = read_h5(h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe34ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cutouts['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutouts['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal code to plot a cutout\n",
    "idx = 10\n",
    "image = cutouts['images'][idx]\n",
    "norm = simple_norm(image, 'sqrt', percent=99.9)\n",
    "plt.figure(figsize=(6, 6), frameon=False)\n",
    "# remove ticks\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(image, norm=norm, origin='lower', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a1e348-b340-45c1-b859-6a583a0aea77",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2e2c3-3bcb-4b95-81b5-b278f2db876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/arc/projects/unions/ssl/data/raw/tiles/dwarforge'\n",
    "table_dir = '/arc/home/heestersnick/dwarforge/tables'\n",
    "dwarf_cat_file_old = 'all_known_dwarfs_processed.csv'\n",
    "dwarf_cat_file_new = 'all_known_dwarfs_v3_processed.csv'\n",
    "dwarf_cat_new = pd.read_csv(os.path.join(table_dir, dwarf_cat_file_new))\n",
    "dwarf_cat_old = pd.read_csv(os.path.join(table_dir, dwarf_cat_file_old))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c614a-65f7-4957-b8ef-d65419f5253a",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df34b1-5b63-425c-86b8-3d7d5279c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_dict = {\n",
    "    'cfis-u': {\n",
    "        'name': 'CFIS',\n",
    "        'band': 'u',\n",
    "        'vos': 'vos:cfis/tiles_DR5/',\n",
    "        'suffix': '.u',\n",
    "        'delimiter': '.',\n",
    "        'fits_ext': 0,\n",
    "        'zfill': 3,\n",
    "        'zp': 30.0,\n",
    "    },\n",
    "    'whigs-g': {\n",
    "        'name': 'calexp-CFIS',\n",
    "        'band': 'g',\n",
    "        'vos': 'vos:cfis/whigs/stack_images_CFIS_scheme/',\n",
    "        'suffix': '',\n",
    "        'delimiter': '_',\n",
    "        'fits_ext': 1,\n",
    "        'zfill': 0,\n",
    "        'zp': 27.0,\n",
    "    },\n",
    "    'cfis_lsb-r': {\n",
    "        'name': 'CFIS_LSB',\n",
    "        'band': 'r',\n",
    "        'vos': 'vos:cfis/tiles_LSB_DR5/',\n",
    "        'suffix': '.r',\n",
    "        'delimiter': '.',\n",
    "        'fits_ext': 0,\n",
    "        'zfill': 3,\n",
    "        'zp': 30.0,\n",
    "    },\n",
    "    'ps-i': {\n",
    "        'name': 'PS-DR3',\n",
    "        'band': 'i',\n",
    "        'vos': 'vos:cfis/panstarrs/DR3/tiles/',\n",
    "        'suffix': '.i',\n",
    "        'delimiter': '.',\n",
    "        'fits_ext': 0,\n",
    "        'zfill': 3,\n",
    "        'zp': 30.0,\n",
    "    },\n",
    "    'wishes-z': {\n",
    "        'name': 'WISHES',\n",
    "        'band': 'z',\n",
    "        'vos': 'vos:cfis/wishes_1/coadd/',\n",
    "        'suffix': '.z',\n",
    "        'delimiter': '.',\n",
    "        'fits_ext': 1,\n",
    "        'zfill': 0,\n",
    "        'zp': 27.0,\n",
    "    },\n",
    "    'ps-z': {\n",
    "        'name': 'PSS.DR4',\n",
    "        'band': 'ps-z',\n",
    "        'vos': 'vos:cfis/panstarrs/DR4/resamp/',\n",
    "        'suffix': '.z',\n",
    "        'delimiter': '.',\n",
    "        'fits_ext': 0,\n",
    "        'zfill': 3,\n",
    "        'zp': 30.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def zfill_tile(tile):\n",
    "    return f'{str(tile[0]).zfill(3)}_{str(tile[1]).zfill(3)}'\n",
    "\n",
    "\n",
    "def transform_list(input_list):\n",
    "    result = []\n",
    "    for item in input_list:\n",
    "        # Extract the numbers from the string using regex\n",
    "        x, y = re.findall(r'\\d+', item)\n",
    "\n",
    "        # Format each number to a 3-digit format, adding leading zeros if necessary\n",
    "        formatted_x = x.zfill(3)\n",
    "        formatted_y = y.zfill(3)\n",
    "\n",
    "        # Combine them with an underscore\n",
    "        result.append(f'{formatted_x}_{formatted_y}')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def labels_to_df(parent_folder, tile_list, dwarf_df):\n",
    "    k = 0\n",
    "    unmatched_dwarf_counter = 0\n",
    "    unmatched_tile_counter = 0\n",
    "    additional_dwarf_counter = 0\n",
    "    for tile in tqdm(tile_list):\n",
    "        # Convert tile tuple to folder name format\n",
    "        folder_name = zfill_tile(tile)\n",
    "\n",
    "        # Construct the full path to the parquet file\n",
    "        tile_nums_zfill = folder_name.split('_')\n",
    "        file_path = os.path.join(\n",
    "            parent_folder,\n",
    "            folder_name,\n",
    "            'cfis_lsb-r',\n",
    "            f'CFIS_LSB.{tile_nums_zfill[0]}.{tile_nums_zfill[1]}.r_rebin_det_params.parquet',\n",
    "        )\n",
    "        fits_name = f'CFIS_LSB.{tile_nums_zfill[0]}.{tile_nums_zfill[1]}.r_rebin_seg.fits'\n",
    "        fits_path = os.path.join(parent_folder, folder_name, 'cfis_lsb-r', fits_name)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                # Attempt to read the parquet file\n",
    "                det_df = pd.read_parquet(file_path)\n",
    "                det_df_updated = det_df.copy()\n",
    "                dwarfs_in_tile = dwarf_df[dwarf_df['tile'] == str(tile)].reset_index(drop=True)\n",
    "                _, header = open_fits(fits_path, fits_ext=0)\n",
    "                additional_dwarfs = check_objects_in_neighboring_tiles(str(tile), dwarf_df, header)\n",
    "\n",
    "                if not additional_dwarfs.empty:\n",
    "                    dwarfs_in_tile = pd.concat([dwarfs_in_tile, additional_dwarfs]).reset_index(\n",
    "                        drop=True\n",
    "                    )\n",
    "\n",
    "                det_idx_lsb, lsb_matches, lsb_unmatches, _ = match_cats(\n",
    "                    det_df_updated, dwarfs_in_tile, tile, header, max_sep=15.0\n",
    "                )\n",
    "\n",
    "                # add lsb labels to detections dataframe\n",
    "                det_df_updated['lsb'] = np.nan\n",
    "                det_df_updated['ID_known'] = np.nan\n",
    "\n",
    "                if len(det_idx_lsb) > 0:\n",
    "                    print(f'Found {len(det_idx_lsb)} lsb detections for tile {tile}.')\n",
    "                    det_df_updated.loc[det_idx_lsb, 'lsb'] = 1\n",
    "                    # Initialize the column to accept strings\n",
    "                    det_df_updated['ID_known'] = det_df_updated['ID_known'].astype(object)\n",
    "                    det_df_updated.loc[det_idx_lsb, 'ID_known'] = lsb_matches['ID'].values\n",
    "                    # print(\n",
    "                    #     f'Added {np.count_nonzero(~np.isnan(det_df_updated[\"lsb\"]))} LSB labels to the detection dataframe for tile {tile}.'\n",
    "                    # )\n",
    "                    k += 1\n",
    "                    additional_dwarf_counter += len(additional_dwarfs)\n",
    "\n",
    "                if len(lsb_unmatches) > 0:\n",
    "                    print(f'Found {len(lsb_unmatches)} unmatched dwarf for tile: {tile}.')\n",
    "                    unmatched_tile_counter += 1\n",
    "                    unmatched_dwarf_counter += len(lsb_unmatches)\n",
    "\n",
    "                # Save updated dataframe\n",
    "                det_df_updated.to_parquet(file_path, index=False)\n",
    "            except Exception as e:\n",
    "                print(f'Something went wrong for tile {tile}: {e}')\n",
    "    print(f'Was able to match {k}/{len(tile_list)} tiles.')\n",
    "    print(\n",
    "        f'There were {unmatched_dwarf_counter} unmatched dwarfs in {unmatched_tile_counter} tiles.'\n",
    "    )\n",
    "    print(f'{additional_dwarf_counter} dwarfs are in multiple tiles.')\n",
    "\n",
    "\n",
    "def open_fits(file_path, fits_ext):\n",
    "    \"\"\"\n",
    "    Open fits file and return data and header.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): name of the fits file\n",
    "        fits_ext (int): extension of the fits file\n",
    "\n",
    "    Returns:\n",
    "        data (numpy.ndarray): image data\n",
    "        header (fits header): header of the fits file\n",
    "    \"\"\"\n",
    "    # logger.debug(f'Opening fits file {os.path.basename(file_path)}..')\n",
    "    with fits.open(file_path, memmap=True) as hdul:\n",
    "        data = hdul[fits_ext].data.astype(np.float32)  # type: ignore\n",
    "        header = hdul[fits_ext].header  # type: ignore\n",
    "    # logger.debug(f'Fits file {os.path.basename(file_path)} opened.')\n",
    "    return data, header\n",
    "\n",
    "\n",
    "def check_objects_in_neighboring_tiles(tile, dwarfs_df, header):\n",
    "    wcs = WCS(header)\n",
    "    # Get neighboring tile numbers\n",
    "    neighboring_tiles = get_neighboring_tile_numbers(tile)\n",
    "\n",
    "    # Filter dwarfs in neighboring tiles\n",
    "    neighboring_dwarfs = dwarfs_df[dwarfs_df['tile'].isin(neighboring_tiles)]\n",
    "\n",
    "    # Check which of these dwarfs are actually within the current tile's boundaries\n",
    "    dwarfs_in_current_tile = neighboring_dwarfs[\n",
    "        neighboring_dwarfs.apply(\n",
    "            lambda row: wcs.footprint_contains(\n",
    "                SkyCoord(row['ra'], row['dec'], unit='deg', frame='icrs')\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return dwarfs_in_current_tile\n",
    "\n",
    "\n",
    "def get_neighboring_tile_numbers(tile):\n",
    "    tile = ast.literal_eval(tile)\n",
    "    x, y = map(int, tile)\n",
    "    neighbors = [\n",
    "        (x - 1, y - 1),\n",
    "        (x - 1, y),\n",
    "        (x - 1, y + 1),\n",
    "        (x, y - 1),\n",
    "        (x, y + 1),\n",
    "        (x + 1, y - 1),\n",
    "        (x + 1, y),\n",
    "        (x + 1, y + 1),\n",
    "    ]\n",
    "    return [f'({nx:03d}, {ny:03d})' for nx, ny in neighbors if 0 <= nx < 1000 and 0 <= ny < 1000]\n",
    "\n",
    "\n",
    "def dwarfs_to_df(parent_folder):\n",
    "    # Pattern to match all relevant parquet files\n",
    "    pattern = os.path.join(\n",
    "        parent_folder, '*_*', 'cfis_lsb-r', 'CFIS_LSB.*.r_rebin_det_params.parquet'\n",
    "    )\n",
    "\n",
    "    # List to store filtered dataframes\n",
    "    filtered_dfs = []\n",
    "\n",
    "    # Iterate through all matching files\n",
    "    for file in tqdm(glob(pattern)):\n",
    "        try:\n",
    "            # Attempt to read the parquet file\n",
    "            df = pd.read_parquet(file)\n",
    "            # Check if 'label' column exists\n",
    "            if 'lsb' in df.columns:\n",
    "                # Filter rows where label is 1\n",
    "                df_filtered = df[df['lsb'] == 1]\n",
    "\n",
    "                if not df_filtered.empty:\n",
    "                    filtered_dfs.append(df_filtered)\n",
    "            # If 'label' column doesn't exist, we skip this file\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing file {file}: {str(e)}')\n",
    "            continue\n",
    "\n",
    "        # The file is automatically closed after reading\n",
    "\n",
    "    # Combine all dataframes\n",
    "    if filtered_dfs:\n",
    "        final_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty dataframe if no data found\n",
    "\n",
    "\n",
    "def gather_training_data(\n",
    "    parent_folder, in_dict, band='cfis_lsb-r', n_neighbors=1, dwarf_tiles=None\n",
    "):\n",
    "    zfill = in_dict[band]['zfill']\n",
    "    file_prefix = in_dict[band]['name']\n",
    "    delimiter = in_dict[band]['delimiter']\n",
    "    suffix = in_dict[band]['suffix']\n",
    "\n",
    "    if dwarf_tiles is None:\n",
    "        pattern = os.path.join(\n",
    "            parent_folder,\n",
    "            '*_*',\n",
    "            band,\n",
    "            f'{file_prefix}{delimiter}*{suffix}_rebin_star_mask*_det_params.parquet',\n",
    "        )\n",
    "        file_list = glob(pattern)\n",
    "    else:\n",
    "        file_list = []\n",
    "        for tile in dwarf_tiles:\n",
    "            tile_nums = tile.split('_')\n",
    "            num1, num2 = tile_nums[0], tile_nums[1]\n",
    "            pattern = os.path.join(\n",
    "                parent_folder,\n",
    "                tile,\n",
    "                band,\n",
    "                f'{file_prefix}{delimiter}{num1}{delimiter}{num2}{suffix}*_det_params.parquet',\n",
    "            )\n",
    "            result = glob(pattern)\n",
    "            if len(result) != 0:\n",
    "                file_list.append(result[0])\n",
    "\n",
    "    print(f'Found {len(file_list)} detection catalogs from tiles with dwarfs.')\n",
    "\n",
    "    all_examples = []\n",
    "\n",
    "    for file in tqdm(file_list):\n",
    "        try:\n",
    "            filename = os.path.basename(file)\n",
    "            tile_numbers = filename.split(f'{delimiter}')[1:3]\n",
    "            tile_id = f'{tile_numbers[0]}.{tile_numbers[1]}'\n",
    "\n",
    "            df = pd.read_parquet(file)\n",
    "\n",
    "            if 'lsb' in df.columns:\n",
    "                positive_examples = df[df['lsb'] == 1].copy()\n",
    "                potential_negatives = df[df['lsb'].isna()].copy()\n",
    "\n",
    "                if not positive_examples.empty and not potential_negatives.empty:\n",
    "                    nn = NearestNeighbors(n_neighbors=len(potential_negatives), metric='euclidean')\n",
    "                    nn.fit(potential_negatives[['ra', 'dec']])\n",
    "\n",
    "                    used_negatives = (\n",
    "                        set()\n",
    "                    )  # Set to keep track of used negative examples in this field\n",
    "                    all_file_examples = []\n",
    "\n",
    "                    for idx, lsb_obj in positive_examples.iterrows():\n",
    "                        lsb_df = pd.DataFrame({'ra': [lsb_obj['ra']], 'dec': [lsb_obj['dec']]})\n",
    "\n",
    "                        distances, indices = nn.kneighbors(lsb_df)\n",
    "\n",
    "                        # Find n_neighbors unique negative examples within this field\n",
    "                        unique_negatives = []\n",
    "                        for index in indices[0]:\n",
    "                            if index not in used_negatives:\n",
    "                                unique_negatives.append(index)\n",
    "                                used_negatives.add(index)\n",
    "                                if len(unique_negatives) == n_neighbors:\n",
    "                                    break\n",
    "\n",
    "                        # If we couldn't find enough unique negatives, continue to the next positive example\n",
    "                        if len(unique_negatives) < n_neighbors:\n",
    "                            continue\n",
    "\n",
    "                        nearest_neighbors = potential_negatives.iloc[unique_negatives].copy()\n",
    "\n",
    "                        lsb_obj['example_id'] = f'{tile_id}.{lsb_obj[\"ID\"]}'\n",
    "\n",
    "                        nearest_neighbors['example_id'] = nearest_neighbors['ID'].apply(\n",
    "                            lambda x: f'{tile_id}.{x}'\n",
    "                        )\n",
    "                        nearest_neighbors['lsb'] = 0  # Set to 0 for negative examples\n",
    "                        nearest_neighbors['associated_lsb_ra'] = lsb_obj['ra']\n",
    "                        nearest_neighbors['associated_lsb_dec'] = lsb_obj['dec']\n",
    "\n",
    "                        all_file_examples.append(\n",
    "                            pd.concat([lsb_obj.to_frame().T, nearest_neighbors])\n",
    "                        )\n",
    "\n",
    "                    if all_file_examples:\n",
    "                        all_examples.append(pd.concat(all_file_examples))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing file {file}: {str(e)}')\n",
    "            continue\n",
    "\n",
    "    if all_examples:\n",
    "        final_df = pd.concat(all_examples, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def create_cartesian_kdtree(ra, dec):\n",
    "    \"\"\"\n",
    "    Create a KD-Tree using Cartesian coordinates converted from RA and Dec.\n",
    "\n",
    "    :param ra: Right Ascension in degrees\n",
    "    :param dec: Declination in degrees\n",
    "    :return: cKDTree object and the corresponding SkyCoord object\n",
    "    \"\"\"\n",
    "    coords = SkyCoord(ra, dec, unit='deg', frame='icrs')\n",
    "    xyz = coords.cartesian.xyz.value.T\n",
    "    tree = cKDTree(xyz)\n",
    "    return tree, coords\n",
    "\n",
    "\n",
    "def match_cats(df_det, df_label, tile, pixel_scale, max_sep=15.0, re_multiplier=4.0):\n",
    "    tree, _ = create_cartesian_kdtree(df_det['ra'].values, df_det['dec'].values)\n",
    "    matches = []\n",
    "    potential_matches_df = pd.DataFrame()\n",
    "    for idx, known in df_label.iterrows():\n",
    "        known_coords = SkyCoord(known['ra'], known['dec'], unit='deg')\n",
    "        known_coords_xyz = known_coords.cartesian.xyz.value\n",
    "\n",
    "        # Calculate base search radius in degrees\n",
    "        base_search_radius = max_sep / 3600  # Convert arcseconds to degrees\n",
    "\n",
    "        # Adaptive search radius (if 're' is available)\n",
    "        if (\n",
    "            're' in known\n",
    "            and known['re'] is not None\n",
    "            and not np.isnan(known['re'])\n",
    "            and known['re'] > 0\n",
    "        ):\n",
    "            adaptive_radius = known['re'] * re_multiplier / 3600  # Convert to degrees\n",
    "            search_radius = max(base_search_radius, adaptive_radius)\n",
    "        else:\n",
    "            search_radius = base_search_radius\n",
    "\n",
    "        search_radius_chord = 2 * np.sin(np.deg2rad(search_radius) / 2)\n",
    "\n",
    "        potential_match_indices = tree.query_ball_point(known_coords_xyz, search_radius_chord)\n",
    "        potential_matches = df_det.iloc[potential_match_indices]\n",
    "\n",
    "        print(f'potential matches for {known[\"ID\"]}: {len(potential_matches)}')\n",
    "\n",
    "        potential_matches_df = pd.concat([potential_matches_df, potential_matches])\n",
    "        if len(potential_matches) > 0:\n",
    "            potential_matches_coords = SkyCoord(\n",
    "                potential_matches['ra'], potential_matches['dec'], unit='deg'\n",
    "            )\n",
    "            distances = known_coords.separation(potential_matches_coords).arcsec\n",
    "            max_n_pix = potential_matches['n_pix'].max()\n",
    "            max_mu = potential_matches['mu'].max()\n",
    "            scores = []\n",
    "            for i, det in potential_matches.iterrows():\n",
    "                size_score = np.log1p(det['n_pix']) / np.log1p(max_n_pix)\n",
    "                lsb_score = det['mu'] / max_mu\n",
    "                distance = distances[potential_matches.index.get_loc(i)]\n",
    "                distance_score = 1 - (\n",
    "                    distance / (3600 * search_radius)\n",
    "                )  # Normalized distance score\n",
    "                score = lsb_score * 0.2 + size_score * 0.4 + distance_score * 0.4\n",
    "                print(\n",
    "                    f'object: {det[\"ID\"]}; lsb score: {lsb_score:.4f}, size score: {size_score:.4f}, distance score: {distance_score:.4f}'\n",
    "                )\n",
    "                print(\n",
    "                    f'object: {det[\"ID\"]}; total score: {score:.4f}; distance: {distance:.2f} arcsec'\n",
    "                )\n",
    "                scores.append((i, score, distance))\n",
    "            best_match = max(scores, key=lambda x: x[1])\n",
    "            matches.append((idx, best_match[0], best_match[2]))\n",
    "\n",
    "    if matches:\n",
    "        label_match_idx, det_match_idx, match_distances = zip(*matches)\n",
    "    else:\n",
    "        label_match_idx, det_match_idx, match_distances = [], [], []\n",
    "    label_matches = df_label.loc[list(label_match_idx)].reset_index(drop=True)\n",
    "    label_unmatches = df_label.drop(list(label_match_idx)).reset_index(drop=True)\n",
    "    det_matches = df_det.loc[list(det_match_idx)].reset_index(drop=True)\n",
    "    det_matches['match_distance'] = match_distances\n",
    "    return list(det_match_idx), label_matches, label_unmatches, det_matches\n",
    "\n",
    "\n",
    "def get_tile_list(dwarf_cat):\n",
    "    tiles = dwarf_cat['tile'].values\n",
    "    non_nan_tiles = [x for x in tiles if x is not np.nan]\n",
    "    str_to_tuple = [ast.literal_eval(item) for item in non_nan_tiles]\n",
    "    unique_tiles = list(set(str_to_tuple))\n",
    "    return unique_tiles\n",
    "\n",
    "\n",
    "def check_bands(bands_str, to_check):\n",
    "    if isinstance(bands_str, str):\n",
    "        if bands_str.startswith('['):\n",
    "            # Handle string representation of a list\n",
    "            try:\n",
    "                bands_list = ast.literal_eval(bands_str)\n",
    "                return all(band in bands_list for band in to_check)\n",
    "            except:\n",
    "                return False\n",
    "        else:\n",
    "            # Handle simple string format\n",
    "            return all(band in bands_str for band in to_check)\n",
    "    return False  # Return False for NaN values\n",
    "\n",
    "\n",
    "def check_availability(dwarf_cat, check_for_bands):\n",
    "    df_select = dwarf_cat.loc[\n",
    "        (~dwarf_cat['tile'].isna())\n",
    "        & (dwarf_cat['bands'].apply(lambda x: check_bands(x, check_for_bands)))\n",
    "    ].reset_index(drop=True)\n",
    "    return df_select\n",
    "\n",
    "\n",
    "def check_param_filter_new(data: pd.DataFrame, a: float, b: float, band: str) -> pd.DataFrame:\n",
    "    params_field = data.copy()\n",
    "    df_dwarf = data.loc[data['lsb'] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Define band-specific conditions\n",
    "    band_conditions = {\n",
    "        'cfis_lsb-r': {\n",
    "            'basic': {\n",
    "                'mu': (22.0, None),\n",
    "                're_arcsec': (1.6, 55.0),\n",
    "                'axis_ratio': (0.17, None),\n",
    "                'r_10_arcsec': (0.39, 19.0),\n",
    "                'r_90_arcsec': (2.3, 150.0),\n",
    "                'r_fwhm_arcsec': (0.4, 10.6),\n",
    "                'mu_median': (0.3, 29.0),\n",
    "                'mu_mean': (0.4, 70.0),\n",
    "                'mu_max': (1.1, 5700.0),\n",
    "                'total_flux': (55, None),\n",
    "                'mag': (13.8, 25.7),\n",
    "            },\n",
    "            'complex': [\n",
    "                lambda df: df['mu'] > (a * df['mag'] + b),\n",
    "                # lambda df: df['mu_max'] < (1.2e10 * np.exp(0.84 * df['mag'])),\n",
    "                # lambda df: df['r_90_arcsec'] < (12.0000 * df['r_10_arcsec'] + 20.0000),\n",
    "                # lambda df: df['r_90_arcsec'] < (3.0 * df['re_arcsec'] + 9.0000),\n",
    "                # lambda df: df['mu_median'] < (4.0 * df['re_arcsec'] + 4.0000),\n",
    "                # lambda df: df['mu_median'] < (4.0 * df['r_10_arcsec'] + 12.0000),\n",
    "                # lambda df: df['mu_median'] < (0.3 * df['r_90_arcsec'] + 15.0000),\n",
    "                # lambda df: df['mu_max'] < (120.0 * df['r_90_arcsec'] + 650.0000)\n",
    "            ],\n",
    "        },\n",
    "        'whigs-g': {\n",
    "            'basic': {\n",
    "                'mu': (22.0, None),\n",
    "                're_arcsec': (1.6, 55.0),\n",
    "                'axis_ratio': (0.17, None),\n",
    "                # 'r_10_arcsec': (0.39, 8.), 'r_90_arcsec': (2.5, 40.), 'r_fwhm_arcsec': (0.4, 7.5),\n",
    "                # 'mu_median': (0.1, 12.), 'mu_mean': (0.25, 17.3), 'mu_max': (1.7, 2600.),\n",
    "                # 'total_flux': (39, None), 'mag': (15.3, 23.0)\n",
    "            },\n",
    "            'complex': [\n",
    "                lambda df: df['mu'] > (a * df['mag'] + b),\n",
    "                # lambda df: df['mu_max'] > (500.0 * -20.0 * df['mag']),\n",
    "                # lambda df: df['r_90_arcsec'] > (4.3025 * df['r_10_arcsec'] - 1.5000),\n",
    "                # lambda df: df['r_90_arcsec'] < (1.9000 * df['re_arcsec'] + 3.5000),\n",
    "                # lambda df: df['mu_median'] < (0.1500 * df['re_arcsec'] + 3.3),\n",
    "                # lambda df: df['mu_median'] < (3.0 * df['r_10_arcsec'] + 1.0000),\n",
    "                # lambda df: df['mu_median'] < (0.0800 * df['r_90_arcsec'] + 3.5),\n",
    "                # lambda df: df['mu_max'] < (14.0 * df['r_90_arcsec'] + 20)\n",
    "            ],\n",
    "        },\n",
    "        'ps-i': {\n",
    "            'basic': {\n",
    "                'mu': (22.0, None),\n",
    "                're_arcsec': (1.6, 55.0),\n",
    "                'axis_ratio': (0.17, None),\n",
    "                'r_10_arcsec': (0.39, 19.0),\n",
    "                'r_90_arcsec': (2.3, 150.0),\n",
    "                'r_fwhm_arcsec': (0.4, 10.6),\n",
    "                'mu_median': (0.3, 29.0),\n",
    "                'mu_mean': (0.4, 70.0),\n",
    "                'mu_max': (1.7, 5700.0),\n",
    "                'total_flux': (55, None),\n",
    "                'mag': (13.8, 25.7),\n",
    "            },\n",
    "            'complex': [\n",
    "                lambda df: df['mu'] > (a * df['mag'] + b),\n",
    "                # lambda df: df['mu_max'] < (1.2e10 * np.exp(0.84 * df['mag'])),\n",
    "                # lambda df: df['r_90_arcsec'] < (12.0000 * df['r_10_arcsec'] + 20.0000),\n",
    "                # lambda df: df['r_90_arcsec'] < (3.0 * df['re_arcsec'] + 9.0000),\n",
    "                # lambda df: df['mu_median'] < (4.0 * df['re_arcsec'] + 4.0000),\n",
    "                # lambda df: df['mu_median'] < (4.0 * df['r_10_arcsec'] + 12.0000),\n",
    "                # lambda df: df['mu_median'] < (0.3 * df['r_90_arcsec'] + 15.0000),\n",
    "                # lambda df: df['mu_max'] < (120.0 * df['r_90_arcsec'] + 650.0000)\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if band not in band_conditions:\n",
    "        print(f'Conditions not implemented for band {band}.')\n",
    "        return None\n",
    "\n",
    "    conditions = band_conditions[band]\n",
    "\n",
    "    # Apply basic conditions\n",
    "    for column, (min_val, max_val) in conditions['basic'].items():\n",
    "        if min_val is not None:\n",
    "            params_field = params_field[params_field[column] > min_val]\n",
    "        if max_val is not None:\n",
    "            params_field = params_field[params_field[column] < max_val]\n",
    "\n",
    "    # Apply complex conditions\n",
    "    for condition in conditions['complex']:\n",
    "        params_field = params_field[condition]\n",
    "\n",
    "    # Remove streaks\n",
    "    params_field = params_field[\n",
    "        (params_field['axis_ratio'] >= 0.17) | (params_field['n_pix'] <= 1000)\n",
    "    ]\n",
    "\n",
    "    # Reset index\n",
    "    params_field = params_field.reset_index(drop=True)\n",
    "\n",
    "    print(\n",
    "        f'Filtered out {len(data.loc[data[\"lsb\"] == 1]) - len(params_field.loc[params_field[\"lsb\"] == 1])} real dwarfs from a total of {len(df_dwarf)}.'\n",
    "    )\n",
    "    print(\n",
    "        f'Filtered out {len(data.loc[data[\"lsb\"] == 0]) - len(params_field.loc[params_field[\"lsb\"] == 0])} other objects from a total of {len(data.loc[data[\"lsb\"] == 0])}'\n",
    "    )\n",
    "\n",
    "    return params_field\n",
    "\n",
    "\n",
    "def read_h5(cutout_path):\n",
    "    \"\"\"\n",
    "    Reads cutout data from HDF5 file\n",
    "    :param cutout_dir: cutout directory\n",
    "    :return: cutout data\n",
    "    \"\"\"\n",
    "    with h5py.File(cutout_path, 'r') as f:\n",
    "        # Create empty dictionaries to store data for each group\n",
    "        cutout_data = {}\n",
    "\n",
    "        # Loop through datasets\n",
    "        for dataset_name in f:\n",
    "            data = np.array(f[dataset_name])\n",
    "            cutout_data[dataset_name] = data\n",
    "    return cutout_data\n",
    "\n",
    "\n",
    "def cutout2d_segmented(data, tile_str, segmentation_mask, object_id, x, y, size, cutout_in):\n",
    "    \"\"\"\n",
    "    Create 2d cutout from an image, applying a segmentation mask for a specific object.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): image data\n",
    "        tile_str (str): tile numbers\n",
    "        segmentation_mask (numpy.ndarray): segmentation mask with object IDs\n",
    "        object_id (int): ID of the object to isolate in the cutout\n",
    "        x (int): x-coordinate of cutout center\n",
    "        y (int): y-coordinate of cutout center\n",
    "        size (int): square cutout size\n",
    "        cutout_in (numpy.ndarray): empty input cutout\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if specified cutout has no overlap with the image data\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: 2d cutout (size x size pixels) with only the specified object\n",
    "    \"\"\"\n",
    "    y_large, x_large = data.shape\n",
    "    size_half = size // 2\n",
    "    y_start = max(0, y - size_half)\n",
    "    y_end = min(y_large, y + (size - size_half))\n",
    "    x_start = max(0, x - size_half)\n",
    "    x_end = min(x_large, x + (size - size_half))\n",
    "\n",
    "    if y_start >= y_end or x_start >= x_end:\n",
    "        logger.error(f'Tile: {tile_str}: an object is fully outside of the image.')\n",
    "\n",
    "    cutout_slice = (\n",
    "        slice(y_start - y + size_half, y_end - y + size_half),\n",
    "        slice(x_start - x + size_half, x_end - x + size_half),\n",
    "    )\n",
    "    data_slice = slice(y_start, y_end), slice(x_start, x_end)\n",
    "\n",
    "    # Apply data and segmentation mask\n",
    "    cutout_in[cutout_slice] = data[data_slice] * (segmentation_mask[data_slice] == object_id)\n",
    "\n",
    "    return cutout_in\n",
    "\n",
    "\n",
    "def create_segmented_cutouts(data, tile_str, segmentation_mask, object_ids, xs, ys, cutout_size):\n",
    "    \"\"\"\n",
    "    Create cutouts for multiple objects efficiently.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): image data\n",
    "        segmentation_mask (numpy.ndarray): segmentation mask with object IDs\n",
    "        object_ids (numpy.ndarray): array of object IDs\n",
    "        xs (numpy.ndarray): array of x-coordinates\n",
    "        ys (numpy.ndarray): array of y-coordinates\n",
    "        cutout_size (int): size of each cutout\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: array of cutouts\n",
    "    \"\"\"\n",
    "    cutouts = np.zeros((len(object_ids), cutout_size, cutout_size), dtype=data.dtype)\n",
    "    cutout_empty = np.zeros((cutout_size, cutout_size), dtype=data.dtype)\n",
    "\n",
    "    for i, (obj_id, x, y) in enumerate(zip(object_ids, xs, ys)):\n",
    "        cutouts[i] = cutout2d_segmented(\n",
    "            data, tile_str, segmentation_mask, obj_id, x, y, cutout_size, cutout_empty.copy()\n",
    "        )\n",
    "\n",
    "    return cutouts\n",
    "\n",
    "\n",
    "def make_cutouts(data, tile_str, df, cutout_size=64):\n",
    "    xs = np.floor(mto_det.X.values + 0.5).astype(np.int32)\n",
    "    ys = np.floor(mto_det.Y.values + 0.5).astype(np.int32)\n",
    "    object_ids = mto_det['ID'].values  # Assuming the index is the object ID\n",
    "\n",
    "    cutout_start = time.time()\n",
    "    cutouts = create_segmented_cutouts(data, tile_str, mto_seg, object_ids, xs, ys, cutout_size)\n",
    "    print(f'Done in {time.time() - cutout_start:.2f} seconds.')\n",
    "\n",
    "    return cutouts\n",
    "\n",
    "\n",
    "def plot_cutouts(cutouts, num_cutouts=9, figsize=(8, 8), percent=98.5):\n",
    "    num_cutouts = min(num_cutouts, len(cutouts))\n",
    "    rows = int(np.ceil(np.sqrt(num_cutouts)))\n",
    "    cols = int(np.ceil(num_cutouts / rows))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    fig.suptitle('Segmented Object Cutouts', fontsize=16)\n",
    "\n",
    "    for i in range(num_cutouts):\n",
    "        ax = axes[i // cols, i % cols] if rows > 1 else axes[i]\n",
    "        norm = simple_norm(cutouts[i], 'sqrt', percent=percent)\n",
    "        im = ax.imshow(cutouts[i], cmap='gray_r', origin='lower', norm=norm)\n",
    "        ax.set_title(f'Object {i + 1}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    for i in range(num_cutouts, rows * cols):\n",
    "        ax = axes[i // cols, i % cols] if rows > 1 else axes[i]\n",
    "        ax.remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def fuse_cutouts(parent_dir, tiles, in_dict, band_names):\n",
    "    for tile in tiles:\n",
    "        tile_nums = tile_dir.split('_')\n",
    "        for band in band_names:\n",
    "            zfill = in_dict[band]['zfill']\n",
    "            file_prefix = in_dict[band]['name']\n",
    "            band_name = in_dict[band]['band']\n",
    "            delimiter = in_dict[band]['delimiter']\n",
    "            suffix = in_dict[band]['suffix']\n",
    "            num1, num2 = tile_nums[0].zfill(zfill), tile_nums[1].zfill(zfill)\n",
    "            cutout_file = f'{file_prefix}{num1}{delimiter}{num2}{suffix}_cutouts.h5'\n",
    "            cutout_path = os.path.join(parent_dir, tile, band, cutout_file)\n",
    "            cutouts_band = read_h5(cutout_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb68d4a-2295-45d9-a5f9-0b9c92cca208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_coordinates(reference_coords, target_coords, max_sep=15.0):\n",
    "    \"\"\"\n",
    "    Match reference coordinates to target coordinates.\n",
    "\n",
    "    Args:\n",
    "        reference_coords (SkyCoord): Reference coordinates\n",
    "        target_coords (SkyCoord): Target coordinates to match against\n",
    "        max_sep (float): Maximum separation in arcseconds\n",
    "\n",
    "    Returns:\n",
    "        matched_ref_indices (array): Indices of matched reference coordinates\n",
    "        matched_target_indices (array): Indices of matched target coordinates\n",
    "    \"\"\"\n",
    "    idx, d2d, _ = reference_coords.match_to_catalog_3d(target_coords)\n",
    "    mask = d2d < max_sep * u.arcsec\n",
    "    return np.where(mask)[0], idx[mask]\n",
    "\n",
    "\n",
    "def fuse_cutouts(parent_dir, tiles, in_dict, band_names=['whigs-g', 'cfis_lsb-r', 'ps-i']):\n",
    "    for tile in tiles[:1]:\n",
    "        tile_nums = tile.split('_')\n",
    "        all_band_data = {}\n",
    "        r_band_data = {}\n",
    "\n",
    "        # Read data for all bands\n",
    "        for band in band_names:\n",
    "            zfill = in_dict[band]['zfill']\n",
    "            file_prefix = in_dict[band]['name']\n",
    "            delimiter = in_dict[band]['delimiter']\n",
    "            suffix = in_dict[band]['suffix']\n",
    "            num1, num2 = tile_nums[0].zfill(zfill), tile_nums[1].zfill(zfill)\n",
    "            cutout_file = f'{file_prefix}{delimiter}{num1}{delimiter}{num2}{suffix}_cutouts.h5'\n",
    "            cutout_path = os.path.join(parent_dir, tile, band, cutout_file)\n",
    "\n",
    "            cutout_dict = read_h5(cutout_path)\n",
    "            if band == 'cfis_lsb-r':\n",
    "                r_band_data = cutout_dict.copy()\n",
    "            all_band_data[band] = {\n",
    "                'cutouts': cutout_dict['images'],\n",
    "                'ra': cutout_dict['ra'],\n",
    "                'dec': cutout_dict['dec'],\n",
    "            }\n",
    "\n",
    "        # Use r-band as reference\n",
    "        r_band = 'cfis_lsb-r'\n",
    "        r_band_coords = SkyCoord(\n",
    "            all_band_data[r_band]['ra'], all_band_data[r_band]['dec'], unit=u.deg\n",
    "        )\n",
    "\n",
    "        # Match cutouts for each non-r band to r-band\n",
    "        matched_indices = {r_band: np.arange(len(all_band_data[r_band]['ra']))}\n",
    "        for band in band_names:\n",
    "            if band != r_band:\n",
    "                target_coords = SkyCoord(\n",
    "                    all_band_data[band]['ra'], all_band_data[band]['dec'], unit=u.deg\n",
    "                )\n",
    "                matched_r_indices, matched_target_indices = match_coordinates(\n",
    "                    r_band_coords, target_coords\n",
    "                )\n",
    "                matched_indices[band] = (matched_r_indices, matched_target_indices)\n",
    "\n",
    "        # Find common matches across all bands\n",
    "        common_r_indices = matched_indices[r_band]\n",
    "        for band in band_names:\n",
    "            if band != r_band:\n",
    "                common_r_indices = np.intersect1d(common_r_indices, matched_indices[band][0])\n",
    "\n",
    "        # Update matched indices for all bands based on common matches\n",
    "        final_indices = {r_band: common_r_indices}\n",
    "        for band in band_names:\n",
    "            if band != r_band:\n",
    "                mask = np.isin(matched_indices[band][0], common_r_indices)\n",
    "                final_indices[band] = matched_indices[band][1][mask]\n",
    "\n",
    "        num_matched = len(common_r_indices)\n",
    "        print(f'Final number of matched cutouts: {num_matched}')\n",
    "\n",
    "        # Create the final array with shape (num_cutouts, num_bands, cutout_size, cutout_size)\n",
    "        cutout_size = all_band_data[r_band]['cutouts'].shape[1:]\n",
    "        final_cutouts = np.zeros((num_matched, len(band_names), *cutout_size), dtype=np.float32)\n",
    "\n",
    "        # Populate the final_cutouts array\n",
    "        for i, band in enumerate(band_names):\n",
    "            final_cutouts[:, i] = all_band_data[band]['cutouts'][final_indices[band]]\n",
    "\n",
    "        # Create new .h5 file\n",
    "        output_file = f'{tile}_matched_cutouts.h5'\n",
    "        out_dir = os.path.join(parent_dir, tile, 'gri')\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        output_path = os.path.join(out_dir, output_file)\n",
    "\n",
    "        dt = h5py.special_dtype(vlen=str)\n",
    "        with h5py.File(output_path, 'w', libver='latest') as f:\n",
    "            # Store the matched cutouts\n",
    "            f.create_dataset('images', data=final_cutouts.astype(np.float32))\n",
    "            # Store r-band ra and dec\n",
    "            f.create_dataset('ra', data=all_band_data[r_band]['ra'][final_indices[r_band]])\n",
    "            f.create_dataset('dec', data=all_band_data[r_band]['dec'][final_indices[r_band]])\n",
    "            f.create_dataset('tile', data=r_band_data['tile'], dtype=np.int32)\n",
    "            f.create_dataset(\n",
    "                'known_id', data=r_band_data['known_id'][final_indices[r_band]], dtype=dt\n",
    "            )\n",
    "            f.create_dataset('mto_id', data=r_band_data['mto_id'][final_indices[r_band]])\n",
    "            f.create_dataset('label', data=r_band_data['label'][final_indices[r_band]])\n",
    "            f.create_dataset('zspec', data=r_band_data['zspec'][final_indices[r_band]])\n",
    "            # Store band information\n",
    "            f.create_dataset('band_names', data=np.array(band_names, dtype='S'))\n",
    "\n",
    "        print(f'Created matched cutouts file: {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f0a9c-e5ef-460a-abad-35b50079055e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "fused_data = fuse_cutouts(\n",
    "    data_dir, ['406_254'], band_dict, band_names=['whigs-g', 'cfis_lsb-r', 'ps-i']\n",
    ")\n",
    "print(f'Took {time.time() - start:.2f} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578be92-f3d5-4737-9312-f085278fe620",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutouts_r = fused_data['406_254']['cutouts'][:, 0, :]\n",
    "cutouts_g = fused_data['406_254']['cutouts'][:, 1, :]\n",
    "cutouts_i = fused_data['406_254']['cutouts'][:, 2, :]\n",
    "plot_cutouts(cutouts_r, num_cutouts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674110e-58dc-4c57-ba45-323645670fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a05166-1af2-4c56-8cba-ca135c10bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cutouts(cutouts_g, num_cutouts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d90910-e662-47d6-92cb-9901e340b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cutouts(cutouts_i, num_cutouts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e375e7e-8eb5-4f0b-9c1f-9ffaa0142336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /arc/projects/unions/ssl/data/raw/tiles/dwarforge/239_291/cfis_lsb-r/CFIS_LSB.239.291.r_cutouts_single.h5\n",
    "cutout_file_r = read_h5(\n",
    "    os.path.join(data_dir, '239_291', 'cfis_lsb-r', 'CFIS_LSB.239.291.r_cutouts_single.h5')\n",
    ")\n",
    "# cutout_file_g = read_h5(os.path.join(data_dir, '262_284','whigs-g', 'calexp-CFIS_262_284_cutouts.h5'))\n",
    "# cutout_file_i = read_h5(os.path.join(data_dir, '262_284','ps-i', 'PSS.DR4.262.284.i_cutouts.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402ec8a-2fe4-4b9f-8c78-d093335cc2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cutouts(cutout_file_r['images'], num_cutouts=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be73de-409f-4102-83e8-6dad9e741ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cutouts(cutout_file_r['segmaps'], num_cutouts=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f321a6-bb15-4364-b3fc-b49447a767a8",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f09e392-a45a-4f12-b526-49449a4adf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_list = get_tile_list(dwarf_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4a4df-646f-4219-9165-03c1ddd0c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd24b2b0-3885-40f6-b203-fcc529a6806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_param_filter(data, a, b, band, a1=None, b1=None):\n",
    "    params_field = data.copy()\n",
    "    df_dwarf = data.loc[data['lsb'] == 1].reset_index(drop=True)\n",
    "\n",
    "    if band == 'cfis_lsb-r':\n",
    "        conditions = {\n",
    "            'mu': (22.0, None),  # (min, max), None means no limit\n",
    "            're_arcsec': (1.6, 55.0),\n",
    "            'axis_ratio': (0.17, None),\n",
    "            'r_10_arcsec': (0.353, 18.2),\n",
    "            'r_25_arcsec': (0.4, 32.1),\n",
    "            'r_75_arcsec': (2.16, 102.1),\n",
    "            'r_90_arcsec': (2.5, 145.1),\n",
    "            'r_100_arcsec': (2.8, 254.1),\n",
    "            'r_fwhm_arcsec': (0.4, 13.8),\n",
    "            'mu_median': (0.34, 28.7),\n",
    "            'mu_mean': (0.4, 65.1),\n",
    "            'mu_max': (2.0, 6255.0),\n",
    "            'total_flux': (55, None),\n",
    "            'mag': (12.17, 25.7),\n",
    "        }\n",
    "\n",
    "        # mag vs mu filter\n",
    "        if a1 is not None and b1 is not None:\n",
    "            params_field = params_field[\n",
    "                (params_field['mu'] > (a * params_field['mag'] + b))\n",
    "                & (params_field['mu'] > (a1 * params_field['mag'] + b1))\n",
    "            ]\n",
    "        else:\n",
    "            params_field = params_field[params_field['mu'] > (a * params_field['mag'] + b)]\n",
    "        params_field = params_field[\n",
    "            (params_field['mu'] / params_field['r_75_arcsec'])\n",
    "            < np.maximum(1.5100 * params_field['mag'] - 24.6000, 3.0)\n",
    "        ]\n",
    "        params_field = params_field[\n",
    "            np.log(params_field['r_90_arcsec'] / params_field['r_75_arcsec'])\n",
    "            < (0.2950 * np.log(params_field['r_100_arcsec']) - 0.1300)\n",
    "        ]\n",
    "\n",
    "    elif band == 'whigs-g':\n",
    "        conditions = {\n",
    "            'mu': (22.51, None),  # (min, max), None means no limit\n",
    "            're_arcsec': (1.6, None),\n",
    "            'axis_ratio': (0.17, None),\n",
    "            'r_10_arcsec': (0.4, 8.1),\n",
    "            'r_25_arcsec': (0.72, 15.0),\n",
    "            'r_75_arcsec': (2.17, 42.0),\n",
    "            'r_90_arcsec': (2.47, 54.0),\n",
    "            'r_100_arcsec': (2.8, 79.0),\n",
    "            'r_fwhm_arcsec': (0.417, 13.5),\n",
    "            'mu_median': (0.0072, 0.8),\n",
    "            'mu_mean': (0.017, 2.01),\n",
    "            'mu_max': (0.066, 226.0),\n",
    "            'total_flux': (1.77, None),\n",
    "            'mag': (16.0, 26.5),\n",
    "        }\n",
    "\n",
    "        # mag vs mu filter\n",
    "        if a1 is not None and b1 is not None:\n",
    "            params_field = params_field[\n",
    "                (params_field['mu'] > (a * params_field['mag'] + b))\n",
    "                & (params_field['mu'] > (a1 * params_field['mag'] + b1))\n",
    "            ]\n",
    "        else:\n",
    "            params_field = params_field[params_field['mu'] > (a * params_field['mag'] + b)]\n",
    "\n",
    "        params_field = params_field[\n",
    "            np.log(params_field['r_100_arcsec'] / params_field['r_90_arcsec'])\n",
    "            < (-1.2615 * np.log(params_field['mag']) + 4.3000)\n",
    "        ]\n",
    "        params_field = params_field[\n",
    "            np.log(params_field['mag'] / params_field['r_75_arcsec'])\n",
    "            < np.maximum(6.4500 * np.log(params_field['mag']) + -17.9000, 0.75)\n",
    "        ]\n",
    "\n",
    "    elif band == 'ps-i':\n",
    "        conditions = {\n",
    "            'mu': (22.0, 29.5),  # (min, max), None means no limit\n",
    "            're_arcsec': (1.6, 40.0),\n",
    "            'axis_ratio': (0.17, None),\n",
    "            'r_10_arcsec': (0.4, 14.95),\n",
    "            'r_25_arcsec': (0.58, 25.5),\n",
    "            'r_75_arcsec': (2.1, 58.0),\n",
    "            'r_90_arcsec': (2.5, 77.0),\n",
    "            'r_100_arcsec': (2.7, 117.0),\n",
    "            'r_fwhm_arcsec': (0.4, 20.8),\n",
    "            'mu_median': (0.445, 40.7),\n",
    "            'mu_mean': (0.565, 100.8),\n",
    "            'mu_max': (2.47, 4465.0),\n",
    "            'total_flux': (66.0, None),\n",
    "            'mag': (13.1, 26.0),\n",
    "        }\n",
    "\n",
    "        params_field = params_field[\n",
    "            (params_field['r_90_arcsec'] / params_field['r_75_arcsec'])\n",
    "            < (0.0100 * params_field['r_100_arcsec'] + 1.4000)\n",
    "        ]\n",
    "        params_field = params_field[\n",
    "            (params_field['r_100_arcsec'] / params_field['r_90_arcsec'])\n",
    "            < (-0.0850 * params_field['mag'] + 3.3000)\n",
    "        ]\n",
    "        params_field = params_field[\n",
    "            (params_field['mag'] / params_field['r_75_arcsec'])\n",
    "            < np.maximum(1.4 * params_field['mag'] - 23, 2.5)\n",
    "        ]\n",
    "        params_field = params_field[\n",
    "            (params_field['r_90_arcsec'] / params_field['r_75_arcsec'])\n",
    "            < (-0.0700 * params_field['mag'] + 3.0000)\n",
    "        ]\n",
    "        params_field = params_field[\n",
    "            (params_field['r_75_arcsec'] / params_field['r_25_arcsec'])\n",
    "            < (0.2 * params_field['mag'] + 2.3)\n",
    "        ]\n",
    "        params_field = params_field[\n",
    "            np.log(params_field['r_100_arcsec'] / params_field['r_75_arcsec'])\n",
    "            < (-2.7 * np.log(params_field['mag']) + 9.05)\n",
    "        ]\n",
    "        params_field = params_field[\n",
    "            np.log(params_field['r_75_arcsec'] / params_field['r_25_arcsec'])\n",
    "            < (-0.9631 * np.log(params_field['mag']) + 4.5500)\n",
    "        ]\n",
    "    else:\n",
    "        print(f'Conditions not implemented for band {band}.')\n",
    "        return\n",
    "\n",
    "    for column, (min_val, max_val) in conditions.items():\n",
    "        if min_val is not None:\n",
    "            params_field = params_field[params_field[column] > min_val]\n",
    "        if max_val is not None:\n",
    "            params_field = params_field[params_field[column] < max_val]\n",
    "\n",
    "    # Remove streaks\n",
    "    params_field = params_field[\n",
    "        (params_field['axis_ratio'] >= 0.17) | (params_field['n_pix'] <= 1000)\n",
    "    ]\n",
    "\n",
    "    # Remove previous index and reset\n",
    "    params_field = params_field.reset_index(drop=True)\n",
    "\n",
    "    print(\n",
    "        f'Filtered out {len(data.loc[data[\"lsb\"] == 1]) - len(params_field.loc[params_field[\"lsb\"] == 1])} real dwarfs from a total of {len(df_dwarf)}.'\n",
    "    )\n",
    "    print(\n",
    "        f'Filtered out {len(data.loc[data[\"lsb\"] == 0]) - len(params_field.loc[params_field[\"lsb\"] == 0])} other objects from a total of {len(data.loc[data[\"lsb\"] == 0])}'\n",
    "    )\n",
    "\n",
    "    return params_field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e23e9c-bd27-4f25-80a0-1fce8a891b54",
   "metadata": {},
   "source": [
    "### r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b938dd-15df-4986-b330-0fc166ccdafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_r = gather_training_data(data_dir, in_dict=band_dict, band='cfis_lsb-r', n_neighbors=20)\n",
    "# training_data_r.to_csv(os.path.join(table_dir, 'training_data_r_20x_rf_v3.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc30fc-f992-46d2-bba9-725050992299",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file_r = 'training_data_r_20x_rf_v3.csv'\n",
    "training_data_r = pd.read_csv(os.path.join(table_dir, training_data_file_r))\n",
    "\n",
    "df_dwarf_r = training_data_r.loc[training_data_r['lsb'] == 1].reset_index(drop=True)\n",
    "df_other_r = training_data_r.loc[training_data_r['lsb'] != 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17a6ec-d805-41c3-a5e3-f506b850e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = df_dwarf_g.agg(['min', 'max'])\n",
    "min_max_df = min_max[\n",
    "    [\n",
    "        'r_10_arcsec',\n",
    "        'r_25_arcsec',\n",
    "        're_arcsec',\n",
    "        'r_fwhm_arcsec',\n",
    "        'r_75_arcsec',\n",
    "        'r_90_arcsec',\n",
    "        'r_100_arcsec',\n",
    "        'mu_max',\n",
    "        'mu_median',\n",
    "        'mu_mean',\n",
    "        'axis_ratio',\n",
    "        'mag',\n",
    "        'mu',\n",
    "        'total_flux',\n",
    "    ]\n",
    "]\n",
    "\n",
    "%matplotlib widget\n",
    "plt.figure()\n",
    "plt.hist(df_dwarf_r['r_10_arcsec'].values, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaffddc-8589-489b-b298-cde4da4df47f",
   "metadata": {},
   "source": [
    "### g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d3a23d-5a4e-44ea-b75d-890d12ac049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands_to_check = ['g']\n",
    "# df_avail = check_availability(dwarf_cat_new, bands_to_check)\n",
    "# tile_list_dwarfs = list(set(df_avail['tile'].values))\n",
    "# tile_list = transform_list(tile_list_dwarfs)\n",
    "# print(f'Using the new catalog, there are {len(df_avail)} known dwarfs in {bands_to_check} in the footprint.')\n",
    "\n",
    "# training_data_g = gather_training_data(data_dir, in_dict=band_dict, band='whigs-g', n_neighbors=20, dwarf_tiles=tile_list)\n",
    "# training_data_g.to_csv(os.path.join(table_dir, 'training_data_g_20x_rf_v3.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04337d5-784c-4378-8a16-0f863842744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file_g = 'training_data_g_20x_rf_v3.csv'\n",
    "training_data_g = pd.read_csv(os.path.join(table_dir, training_data_file_g))\n",
    "\n",
    "df_dwarf_g = training_data_g.loc[training_data_g['lsb'] == 1].reset_index(drop=True)\n",
    "df_other_g = training_data_g.loc[training_data_g['lsb'] != 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558bf60-15c9-48a5-863c-aceaa422ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_dwarf_g['r_25_arcsec'].values <= 0.4).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea783e7-417b-4300-825e-97208fe27fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(df_dwarf_g['r_100_arcsec'].values, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e36a60-9c61-4f69-adc6-9c535c429103",
   "metadata": {},
   "source": [
    "### i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cefdab-6001-4a40-83ba-6757a7c33982",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_to_check = ['i']\n",
    "df_avail = check_availability(dwarf_cat_new, bands_to_check)\n",
    "tile_list_dwarfs = list(set(df_avail['tile'].values))\n",
    "tile_list = transform_list(tile_list_dwarfs)\n",
    "print(\n",
    "    f'Using the new catalog, there are {len(df_avail)} known dwarfs in {bands_to_check} in the footprint.'\n",
    ")\n",
    "\n",
    "# training_data = gather_training_data(data_dir, in_dict=band_dict, band='ps-i', n_neighbors=20, dwarf_tiles=tile_list)\n",
    "# training_data.to_csv(os.path.join(table_dir, 'training_data_i_20x_rf_v3.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae9f88-9859-4494-aac9-f4481143565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file_i = 'training_data_i_20x_rf_v3.csv'\n",
    "training_data_i = pd.read_csv(os.path.join(table_dir, training_data_file_i))\n",
    "\n",
    "df_dwarf_i = training_data_i.loc[training_data_i['lsb'] == 1].reset_index(drop=True)\n",
    "df_other_i = training_data_i.loc[training_data_i['lsb'] != 1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f6e9d-a3ca-4d8a-ad69-9aff68621c0c",
   "metadata": {},
   "source": [
    "### Cross match and combine to master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39115827-7909-453b-a835-4849fbe925c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def match_coordinates(reference_coords, target_coords, max_sep=15.0):\n",
    "    \"\"\"\n",
    "    Match reference coordinates to target coordinates.\n",
    "\n",
    "    Args:\n",
    "        reference_coords (SkyCoord): Reference coordinates\n",
    "        target_coords (SkyCoord): Target coordinates to match against\n",
    "        max_sep (float): Maximum separation in arcseconds\n",
    "\n",
    "    Returns:\n",
    "        matched_ref_indices (array): Indices of matched reference coordinates\n",
    "        matched_target_indices (array): Indices of matched target coordinates\n",
    "    \"\"\"\n",
    "    idx, d2d, _ = reference_coords.match_to_catalog_3d(target_coords)\n",
    "    mask = d2d < max_sep * u.arcsec\n",
    "    return np.where(mask)[0], idx[mask]\n",
    "\n",
    "\n",
    "def process_tile(tile_folder, bands, in_dict):\n",
    "    all_data = dict.fromkeys(bands)\n",
    "\n",
    "    for band in bands:\n",
    "        zfill = in_dict[band]['zfill']\n",
    "        file_prefix = in_dict[band]['name']\n",
    "        delimiter = in_dict[band]['delimiter']\n",
    "        suffix = in_dict[band]['suffix']\n",
    "        file_pattern = f'{file_prefix}{delimiter}*{suffix}_rebin_star_mask*_det_params.parquet'\n",
    "        file_path = os.path.join(tile_folder, band, file_pattern)\n",
    "        matching_files = glob(file_path)\n",
    "\n",
    "        if matching_files:\n",
    "            df = pd.read_parquet(matching_files[0])\n",
    "            df.columns = [\n",
    "                f'{col}_{band}' if col not in ['ID_known', 'ra', 'dec', 'lsb'] else col\n",
    "                for col in df.columns\n",
    "            ]\n",
    "            all_data[band] = df\n",
    "        else:\n",
    "            print(f'No matching file found for band {band} in folder {tile_folder}')\n",
    "            return None\n",
    "\n",
    "    # Identify dwarf galaxies present in all bands with lsb == 1\n",
    "    dwarf_ids = set(\n",
    "        all_data[bands[0]][\n",
    "            (all_data[bands[0]]['lsb'] == 1) & (all_data[bands[0]]['ID_known'].notna())\n",
    "        ]['ID_known']\n",
    "    )\n",
    "    for band in bands[1:]:\n",
    "        dwarf_ids &= set(\n",
    "            all_data[band][(all_data[band]['lsb'] == 1) & (all_data[band]['ID_known'].notna())][\n",
    "                'ID_known'\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Get the full data for matched dwarfs\n",
    "    final_dwarfs = all_data[bands[0]][all_data[bands[0]]['ID_known'].isin(dwarf_ids)]\n",
    "    for band in bands[1:]:\n",
    "        band_data = all_data[band][all_data[band]['ID_known'].isin(dwarf_ids)]\n",
    "        final_dwarfs = pd.merge(\n",
    "            final_dwarfs, band_data, on=['ID_known', 'ra', 'dec', 'lsb'], suffixes=('', f'_{band}')\n",
    "        )\n",
    "\n",
    "    # Process non-dwarf objects\n",
    "    non_dwarf_data = {\n",
    "        band: all_data[band][~all_data[band]['ID_known'].isin(dwarf_ids)] for band in bands\n",
    "    }\n",
    "\n",
    "    # Match non-dwarf objects using coordinates\n",
    "    r_band = bands[0]  # Assuming the first band is r-band\n",
    "    r_band_coords = SkyCoord(\n",
    "        non_dwarf_data[r_band]['ra'], non_dwarf_data[r_band]['dec'], unit=u.deg\n",
    "    )\n",
    "\n",
    "    matched_indices = {r_band: np.arange(len(non_dwarf_data[r_band]))}\n",
    "    for band in bands[1:]:\n",
    "        target_coords = SkyCoord(\n",
    "            non_dwarf_data[band]['ra'], non_dwarf_data[band]['dec'], unit=u.deg\n",
    "        )\n",
    "        matched_r_indices, matched_target_indices = match_coordinates(r_band_coords, target_coords)\n",
    "        matched_indices[band] = (matched_r_indices, matched_target_indices)\n",
    "\n",
    "    # Find common matches across all bands for non-dwarf objects\n",
    "    common_r_indices = matched_indices[r_band]\n",
    "    for band in bands[1:]:\n",
    "        common_r_indices = np.intersect1d(common_r_indices, matched_indices[band][0])\n",
    "\n",
    "    # Update matched indices for all bands based on common matches\n",
    "    final_indices = {r_band: common_r_indices}\n",
    "    for band in bands[1:]:\n",
    "        mask = np.isin(matched_indices[band][0], common_r_indices)\n",
    "        final_indices[band] = matched_indices[band][1][mask]\n",
    "\n",
    "    # Create the final dataframe with matched non-dwarf objects\n",
    "    final_non_dwarfs = non_dwarf_data[r_band].iloc[final_indices[r_band]].reset_index(drop=True)\n",
    "    for band in bands[1:]:\n",
    "        band_data = non_dwarf_data[band].iloc[final_indices[band]].reset_index(drop=True)\n",
    "        final_non_dwarfs = pd.concat([final_non_dwarfs, band_data.add_suffix(f'_{band}')], axis=1)\n",
    "\n",
    "    # Combine matched dwarf and non-dwarf objects\n",
    "    final_data = pd.concat([final_dwarfs, final_non_dwarfs], ignore_index=True)\n",
    "\n",
    "    # Remove duplicate columns (ra, dec, lsb from other bands)\n",
    "    columns_to_drop = [col for col in final_data.columns if col.startswith(('ra_', 'dec_', 'lsb_'))]\n",
    "    final_data = final_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Add tile information\n",
    "    final_data['tile'] = os.path.basename(tile_folder)\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def gather_photometric_data(parent_folder, bands, in_dict, output_file, tile_folders=None):\n",
    "    all_data = []\n",
    "    if tile_folders is not None:\n",
    "        tile_folders = transform_list(tile_folders)\n",
    "    else:\n",
    "        tile_folders = [\n",
    "            f for f in os.listdir(parent_folder) if os.path.isdir(os.path.join(parent_folder, f))\n",
    "        ]\n",
    "\n",
    "    for tile_folder in tqdm(tile_folders, desc='Processing tiles'):\n",
    "        tile_path = os.path.join(parent_folder, tile_folder)\n",
    "        result = process_tile(tile_path, bands, in_dict)\n",
    "        return result\n",
    "        if result is not None:\n",
    "            all_data.append(result)\n",
    "            print(f'Processed data for tile {tile_folder}')\n",
    "        else:\n",
    "            print(f'Skipped tile {tile_folder} due to missing data')\n",
    "\n",
    "    # Combine all processed data into a single DataFrame\n",
    "    final_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Save the final DataFrame to a single parquet file\n",
    "    final_data.to_parquet(output_file)\n",
    "    print(f'All data processed and saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0783c22-b811-4103-9edc-444587f376c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def match_coordinates(reference_coords, target_coords, max_sep=15.0):\n",
    "    \"\"\"\n",
    "    Match reference coordinates to target coordinates.\n",
    "\n",
    "    Args:\n",
    "        reference_coords (SkyCoord): Reference coordinates\n",
    "        target_coords (SkyCoord): Target coordinates to match against\n",
    "        max_sep (float): Maximum separation in arcseconds\n",
    "\n",
    "    Returns:\n",
    "        matched_ref_indices (array): Indices of matched reference coordinates\n",
    "        matched_target_indices (array): Indices of matched target coordinates\n",
    "    \"\"\"\n",
    "    idx, d2d, _ = reference_coords.match_to_catalog_3d(target_coords)\n",
    "    mask = d2d < max_sep * u.arcsec\n",
    "    return np.where(mask)[0], idx[mask]\n",
    "\n",
    "\n",
    "# def process_tile(tile_folder, bands, in_dict):\n",
    "#     all_data = {band: None for band in bands}\n",
    "#     tilename = os.path.basename(os.path.normpath(tile_folder))\n",
    "#     print(f\"Processing tile: {tilename}\")\n",
    "\n",
    "#     # Read data for all bands\n",
    "#     for band in bands:\n",
    "#         zfill = in_dict[band]['zfill']\n",
    "#         file_prefix = in_dict[band]['name']\n",
    "#         delimiter = in_dict[band]['delimiter']\n",
    "#         suffix = in_dict[band]['suffix']\n",
    "#         file_pattern = f\"{file_prefix}{delimiter}*{suffix}_rebin_star_mask*_det_params.parquet\"\n",
    "#         file_path = os.path.join(tile_folder, band, file_pattern)\n",
    "#         matching_files = glob(file_path)\n",
    "\n",
    "#         if matching_files:\n",
    "#             df = pd.read_parquet(matching_files[0])\n",
    "#             print(f\"Tile {tilename}, Band {band}: Initial objects = {len(df)}\")\n",
    "#             if df.empty:\n",
    "#                 print(f'Tile {tilename}: dataframe empty for band {band}.')\n",
    "#                 return None\n",
    "#             df['ID_known'] = df['ID_known'].fillna('').astype(str)\n",
    "#             if band != bands[0]:  # not r-band\n",
    "#                 df = df.drop(columns=['ra', 'dec'])  # We'll use r-band ra, dec\n",
    "#             df.columns = [f\"{col}_{band}\" if col not in ['ID_known', 'ra', 'dec', 'lsb'] else col for col in df.columns]\n",
    "#             all_data[band] = df\n",
    "#         else:\n",
    "#             print(f\"No matching file found for band {band} in folder {tile_folder}\")\n",
    "#             return None\n",
    "\n",
    "#     # Identify dwarf galaxies present in all bands with lsb == 1 and non-empty ID_known\n",
    "#     dwarf_ids = set(all_data[bands[0]][(all_data[bands[0]]['lsb'] == 1) & (all_data[bands[0]]['ID_known'] != '')]['ID_known'])\n",
    "#     for band in bands[1:]:\n",
    "#         band_dwarf_ids = set(all_data[band][(all_data[band]['lsb'] == 1) & (all_data[band]['ID_known'] != '')]['ID_known'])\n",
    "#         dwarf_ids &= band_dwarf_ids\n",
    "\n",
    "#     print(f'Tile: {tilename}, number of common dwarf ids: {len(dwarf_ids)}')\n",
    "\n",
    "#     # Process dwarf galaxies\n",
    "#     final_dwarfs = all_data[bands[0]][all_data[bands[0]]['ID_known'].isin(dwarf_ids)]\n",
    "#     for band in bands[1:]:\n",
    "#         band_data = all_data[band][all_data[band]['ID_known'].isin(dwarf_ids)]\n",
    "#         final_dwarfs = pd.merge(final_dwarfs, band_data, on='ID_known', suffixes=('', f'_{band}'))\n",
    "\n",
    "#     print(f\"Tile {tilename}: Matched dwarfs = {len(final_dwarfs)}\")\n",
    "\n",
    "#     # Process non-dwarf objects\n",
    "#     non_dwarf_data = {band: all_data[band][~all_data[band]['ID_known'].isin(dwarf_ids)] for band in bands}\n",
    "#     for band in bands:\n",
    "#         print(f\"Tile {tilename}, Band {band}: Non-dwarf objects = {len(non_dwarf_data[band])}\")\n",
    "\n",
    "#     # Match non-dwarf objects using coordinates\n",
    "#     r_band = bands[0]  # Assuming the first band is r-band\n",
    "#     r_band_coords = SkyCoord(non_dwarf_data[r_band][f'ra_{band}'], non_dwarf_data[r_band][f'dec_{band}'], unit=u.deg)\n",
    "\n",
    "#     matched_indices = {r_band: np.arange(len(non_dwarf_data[r_band]))}\n",
    "#     for band in bands[1:]:\n",
    "#         target_coords = SkyCoord(non_dwarf_data[band][f'ra_{band}'], non_dwarf_data[band][f'dec_{band}'], unit=u.deg)\n",
    "#         matched_r_indices, matched_target_indices = match_coordinates(r_band_coords, target_coords)\n",
    "#         matched_indices[band] = (matched_r_indices, matched_target_indices)\n",
    "#         print(f\"Tile {tilename}, Band {band}: Matched non-dwarf objects = {len(matched_r_indices)}\")\n",
    "\n",
    "#     # Find common matches across all bands for non-dwarf objects\n",
    "#     common_r_indices = matched_indices[r_band]\n",
    "#     for band in bands[1:]:\n",
    "#         common_r_indices = np.intersect1d(common_r_indices, matched_indices[band][0])\n",
    "\n",
    "#     print(f\"Tile {tilename}: Common non-dwarf matches across all bands = {len(common_r_indices)}\")\n",
    "\n",
    "#     # Create the final dataframe with matched non-dwarf objects\n",
    "#     final_non_dwarfs = non_dwarf_data[r_band].iloc[common_r_indices].reset_index(drop=True)\n",
    "#     for band in bands[1:]:\n",
    "#         band_indices = matched_indices[band][1][np.isin(matched_indices[band][0], common_r_indices)]\n",
    "#         band_data = non_dwarf_data[band].iloc[band_indices].reset_index(drop=True)\n",
    "#         final_non_dwarfs = pd.concat([final_non_dwarfs, band_data.add_suffix(f'_{band}')], axis=1)\n",
    "\n",
    "#     print(f\"Tile {tilename}: Final matched non-dwarfs = {len(final_non_dwarfs)}\")\n",
    "\n",
    "#     # Combine matched dwarf and non-dwarf objects\n",
    "#     final_data = pd.concat([final_dwarfs, final_non_dwarfs], ignore_index=True)\n",
    "\n",
    "#     # Add tile information\n",
    "#     final_data['tile'] = tilename\n",
    "\n",
    "#     print(f\"Tile {tilename}: Total objects = {len(final_data)}, Dwarfs = {len(final_dwarfs)}, Non-dwarfs = {len(final_non_dwarfs)}\")\n",
    "\n",
    "#     return final_data\n",
    "\n",
    "\n",
    "def process_tile(tile_folder, bands, in_dict):\n",
    "    all_data = dict.fromkeys(bands)\n",
    "    tilename = os.path.basename(os.path.normpath(tile_folder))\n",
    "    print(f'Processing tile: {tilename}')\n",
    "\n",
    "    # Read data for all bands\n",
    "    for band in bands:\n",
    "        zfill = in_dict[band]['zfill']\n",
    "        file_prefix = in_dict[band]['name']\n",
    "        delimiter = in_dict[band]['delimiter']\n",
    "        suffix = in_dict[band]['suffix']\n",
    "        file_pattern = f'{file_prefix}{delimiter}*{suffix}_rebin_star_mask*_det_params.parquet'\n",
    "        file_path = os.path.join(tile_folder, band, file_pattern)\n",
    "        matching_files = glob(file_path)\n",
    "\n",
    "        if matching_files:\n",
    "            df = pd.read_parquet(matching_files[0])\n",
    "            print(f'Tile {tilename}, Band {band}: Initial objects = {len(df)}')\n",
    "            if df.empty:\n",
    "                print(f'Tile {tilename}: dataframe empty for band {band}.')\n",
    "                return None\n",
    "            df['ID_known'] = df['ID_known'].fillna('').astype(str)\n",
    "            df.columns = [f'{col}_{band}' if col not in ['ID_known'] else col for col in df.columns]\n",
    "            all_data[band] = df\n",
    "        else:\n",
    "            print(f'No matching file found for band {band} in folder {tile_folder}')\n",
    "            return None\n",
    "\n",
    "    # Identify dwarf galaxies present in all bands with lsb == 1 and non-empty ID_known\n",
    "    dwarf_ids = set(\n",
    "        all_data[bands[0]][\n",
    "            (all_data[bands[0]][f'lsb_{bands[0]}'] == 1) & (all_data[bands[0]]['ID_known'] != '')\n",
    "        ]['ID_known']\n",
    "    )\n",
    "    for band in bands[1:]:\n",
    "        band_dwarf_ids = set(\n",
    "            all_data[band][\n",
    "                (all_data[band][f'lsb_{band}'] == 1) & (all_data[band]['ID_known'] != '')\n",
    "            ]['ID_known']\n",
    "        )\n",
    "        dwarf_ids &= band_dwarf_ids\n",
    "\n",
    "    print(f'Tile: {tilename}, number of common dwarf ids: {len(dwarf_ids)}')\n",
    "\n",
    "    # Process dwarf galaxies\n",
    "    final_dwarfs = all_data[bands[0]][all_data[bands[0]]['ID_known'].isin(dwarf_ids)]\n",
    "    for band in bands[1:]:\n",
    "        band_data = all_data[band][all_data[band]['ID_known'].isin(dwarf_ids)]\n",
    "        final_dwarfs = pd.merge(final_dwarfs, band_data, on='ID_known', suffixes=('', f'_{band}'))\n",
    "\n",
    "    print(f'Tile {tilename}: Matched dwarfs = {len(final_dwarfs)}')\n",
    "\n",
    "    # Process non-dwarf objects\n",
    "    non_dwarf_data = {\n",
    "        band: all_data[band][~all_data[band]['ID_known'].isin(dwarf_ids)] for band in bands\n",
    "    }\n",
    "    for band in bands:\n",
    "        print(f'Tile {tilename}, Band {band}: Non-dwarf objects = {len(non_dwarf_data[band])}')\n",
    "\n",
    "    # Match non-dwarf objects using coordinates\n",
    "    r_band = bands[0]  # Assuming the first band is r-band\n",
    "    r_band_coords = SkyCoord(\n",
    "        non_dwarf_data[r_band][f'ra_{r_band}'], non_dwarf_data[r_band][f'dec_{r_band}'], unit=u.deg\n",
    "    )\n",
    "\n",
    "    matched_indices = {r_band: np.arange(len(non_dwarf_data[r_band]))}\n",
    "    for band in bands[1:]:\n",
    "        target_coords = SkyCoord(\n",
    "            non_dwarf_data[band][f'ra_{band}'], non_dwarf_data[band][f'dec_{band}'], unit=u.deg\n",
    "        )\n",
    "        matched_r_indices, matched_target_indices = match_coordinates(r_band_coords, target_coords)\n",
    "        matched_indices[band] = (matched_r_indices, matched_target_indices)\n",
    "        print(f'Tile {tilename}, Band {band}: Matched non-dwarf objects = {len(matched_r_indices)}')\n",
    "\n",
    "    # Find common matches across all bands for non-dwarf objects\n",
    "    common_r_indices = matched_indices[r_band]\n",
    "    for band in bands[1:]:\n",
    "        common_r_indices = np.intersect1d(common_r_indices, matched_indices[band][0])\n",
    "\n",
    "    print(f'Tile {tilename}: Common non-dwarf matches across all bands = {len(common_r_indices)}')\n",
    "\n",
    "    # Create the final dataframe with matched non-dwarf objects\n",
    "    final_non_dwarfs = non_dwarf_data[r_band].iloc[common_r_indices].reset_index(drop=True)\n",
    "    for band in bands[1:]:\n",
    "        band_indices = matched_indices[band][1][np.isin(matched_indices[band][0], common_r_indices)]\n",
    "        band_data = non_dwarf_data[band].iloc[band_indices].reset_index(drop=True)\n",
    "        final_non_dwarfs = pd.merge(\n",
    "            final_non_dwarfs,\n",
    "            band_data,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            suffixes=('', f'_{band}'),\n",
    "        )\n",
    "\n",
    "    print(f'Tile {tilename}: Final matched non-dwarfs = {len(final_non_dwarfs)}')\n",
    "\n",
    "    # Combine matched dwarf and non-dwarf objects\n",
    "    final_data = pd.concat([final_dwarfs, final_non_dwarfs], ignore_index=True)\n",
    "\n",
    "    # Add tile information\n",
    "    final_data['tile'] = tilename\n",
    "\n",
    "    print(\n",
    "        f'Tile {tilename}: Total objects = {len(final_data)}, Dwarfs = {len(final_dwarfs)}, Non-dwarfs = {len(final_non_dwarfs)}'\n",
    "    )\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def gather_photometric_data(parent_folder, bands, in_dict, output_file, tile_folders=None):\n",
    "    all_data = []\n",
    "    if tile_folders is not None:\n",
    "        tile_folders = transform_list(tile_folders)\n",
    "    else:\n",
    "        tile_folders = [\n",
    "            f for f in os.listdir(parent_folder) if os.path.isdir(os.path.join(parent_folder, f))\n",
    "        ]\n",
    "\n",
    "    for tile_folder in tqdm(tile_folders, desc='Processing tiles'):\n",
    "        tile_path = os.path.join(parent_folder, tile_folder)\n",
    "        result = process_tile(tile_path, bands, in_dict)\n",
    "        if result is not None:\n",
    "            all_data.append(result)\n",
    "            # print(f\"Processed data for tile {tile_folder}\")\n",
    "        else:\n",
    "            print(f'Skipped tile {tile_folder} due to missing data')\n",
    "\n",
    "    # Combine all processed data into a single DataFrame\n",
    "    final_data = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Save the final DataFrame to a single parquet file\n",
    "    final_data.to_parquet(output_file)\n",
    "    print(f'All data processed and saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdde7b5-28b9-458e-87de-49a8e51ebabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bands = ['cfis_lsb-r', 'whigs-g', 'ps-i']\n",
    "output_file = 'umap_data_master1.parquet'\n",
    "test_result = gather_photometric_data(\n",
    "    data_dir, bands, band_dict, output_file, tile_folders=tile_list_dwarfs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f09f8-ee5a-4df9-bd5f-d1da337f2a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['cfis_lsb-r', 'whigs-g', 'ps-i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32880a94-273d-495c-a8a0-780c5fa93e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "[band_dict[band]['band'] for band in bands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250e1f1-4482-4145-98cc-1557f12ffac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = pd.read_parquet('umap_data_master.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9aff0e-e1fe-4c97-ba32-770be5f82f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "master['ID_known'].loc[master['ID_known'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2474a9-890a-420d-b6fc-6d7d7d1389b8",
   "metadata": {},
   "source": [
    "### Check band availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e29f66d-38db-46e9-a5e5-d827f2dbe40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands_to_check = ['r']\n",
    "# df_avail_old = check_availability(dwarf_cat_old, bands_to_check)\n",
    "# print(f'Using the old catalog, there are {len(df_avail_old)} known dwarfs in {bands_to_check} in the footprint.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b9286-11e2-4c31-87a7-52fea8718811",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_to_check = ['r', 'g', 'i']\n",
    "df_avail = check_availability(dwarf_cat_new, bands_to_check)\n",
    "tile_list_dwarfs = list(set(df_avail['tile'].values))\n",
    "print(\n",
    "    f'Using the new catalog, there are {len(df_avail)} known dwarfs in {bands_to_check} in the footprint.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cbedeb-a941-4551-9172-5ae19f1960f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_avail.to_csv('dwarf_tiles_gri.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd0c793-01a9-4b35-8ef3-a840232ac92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tile_list_dwarfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751b4c6-260c-4684-b6b2-deef7a462665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_lin(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "def func_p2(x, a, b, c):\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "\n",
    "def func_p3(x, a, b, c, d):\n",
    "    return a * x**3 + b * x**2 + c * x + d\n",
    "\n",
    "\n",
    "def func_exp(x, a, b):\n",
    "    return a * np.exp(-b * x)\n",
    "\n",
    "\n",
    "def plot_params(\n",
    "    df_dwarf,\n",
    "    df_other,\n",
    "    param1,\n",
    "    param2,\n",
    "    func_type='linear',\n",
    "    func_params=None,\n",
    "    func_params1=None,\n",
    "    annotate=False,\n",
    "    plot_func=True,\n",
    "    print_filtered=False,\n",
    "):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    if isinstance(param1, tuple):\n",
    "        xval_other = np.divide(\n",
    "            df_other[param1[0]].values,\n",
    "            df_other[param1[1]].values,\n",
    "            where=df_other[param1[1]].values != 0,\n",
    "            out=np.zeros_like(df_other[param1[0]].values),\n",
    "        )\n",
    "        yval_other = df_other[param2].values\n",
    "        xval_dwarf = np.divide(\n",
    "            df_dwarf[param1[0]].values,\n",
    "            df_dwarf[param1[1]].values,\n",
    "            where=df_dwarf[param1[1]].values != 0,\n",
    "            out=np.zeros_like(df_dwarf[param1[0]].values),\n",
    "        )\n",
    "        yval_dwarf = df_dwarf[param2].values\n",
    "    elif isinstance(param2, tuple):\n",
    "        xval_other = np.log10(df_other[param1].values)\n",
    "        yval_other = np.log10(\n",
    "            np.divide(\n",
    "                df_other[param2[0]].values,\n",
    "                df_other[param2[1]].values,\n",
    "                where=df_other[param2[1]].values != 0,\n",
    "                out=np.zeros_like(df_other[param2[0]].values),\n",
    "            )\n",
    "        )\n",
    "        xval_dwarf = np.log10(df_dwarf[param1].values)\n",
    "        yval_dwarf = np.log10(\n",
    "            np.divide(\n",
    "                df_dwarf[param2[0]].values,\n",
    "                df_dwarf[param2[1]].values,\n",
    "                where=df_dwarf[param2[1]].values != 0,\n",
    "                out=np.zeros_like(df_dwarf[param2[0]].values),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        xval_other = df_other[param1].values\n",
    "        yval_other = df_other[param2].values\n",
    "        xval_dwarf = df_dwarf[param1].values\n",
    "        yval_dwarf = df_dwarf[param2].values\n",
    "\n",
    "    plt.scatter(xval_other, yval_other, marker='o', s=6, color='red', alpha=0.3, label='Other')\n",
    "    plt.scatter(xval_dwarf, yval_dwarf, marker='o', s=10, color='green', alpha=0.3, label='Dwarf')\n",
    "\n",
    "    filtered_dwarfs = 0\n",
    "    x_range = np.linspace(df_other[param1].min(), df_other[param1].max(), 100)\n",
    "\n",
    "    if func_type == 'linear':\n",
    "        y_func = func_lin(x_range, *func_params)\n",
    "        func_label = f'y = {func_params[0]:.2f}x + {func_params[1]:.2f}'\n",
    "        if func_params1 is not None:\n",
    "            y_func1 = func_lin(x_range, *func_params1)\n",
    "    elif func_type == 'quadratic':\n",
    "        y_func = func_p2(x_range, *func_params)\n",
    "        func_label = f'y = {func_params[0]:.2f}x² + {func_params[1]:.2f}x + {func_params[2]:.2f}'\n",
    "    elif func_type == 'cubic':\n",
    "        y_func = func_p3(x_range, *func_params)\n",
    "        func_label = f'y = {func_params[0]:.2f}x³ + {func_params[1]:.2f}x² + {func_params[2]:.2f}x + {func_params[3]:.2f}'\n",
    "    elif func_type == 'exp':\n",
    "        y_func = func_exp(x_range, *func_params)\n",
    "        func_label = f'y = {func_params[0]:.2f}e^({func_params[1]:.2f}*x)'\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported function type. Choose 'linear', 'quadratic', or 'cubic'.\")\n",
    "\n",
    "    if plot_func:\n",
    "        plt.plot(x_range, y_func, 'b-', label=func_label)\n",
    "        if func_params1 is not None:\n",
    "            plt.plot(x_range, y_func1, 'b-')\n",
    "\n",
    "    if not isinstance(param1, tuple) and not isinstance(param2, tuple):\n",
    "        for i, row in df_dwarf.iterrows():\n",
    "            x_val = row[param1]\n",
    "            y_val = row[param2]\n",
    "\n",
    "            if func_type == 'linear':\n",
    "                y_boundary = func_lin(x_val, *func_params)\n",
    "                if func_params1 is not None:\n",
    "                    y_boundary1 = func_lin(x_val, *func_params1)\n",
    "            elif func_type == 'quadratic':\n",
    "                y_boundary = func_p2(x_val, *func_params)\n",
    "            elif func_type == 'cubic':\n",
    "                y_boundary = func_p3(x_val, *func_params)\n",
    "            elif func_type == 'exp':\n",
    "                y_boundary = func_exp(x_val, *func_params)\n",
    "\n",
    "            if y_val < y_boundary:\n",
    "                text = f'{row[\"ID_known\"]} ra: {row[\"ra\"]:.4f}, dec: {row[\"dec\"]:.4f}'\n",
    "                if print_filtered:\n",
    "                    print(text)\n",
    "                filtered_dwarfs += 1\n",
    "                if annotate:\n",
    "                    plt.annotate(\n",
    "                        text,\n",
    "                        (x_val, y_val),\n",
    "                        xytext=(2, 2),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=10,\n",
    "                        alpha=1,\n",
    "                    )\n",
    "\n",
    "        filtered_others = 0\n",
    "\n",
    "        for i, row_other in df_other.iterrows():\n",
    "            x_val_other = row_other[param1]\n",
    "            y_val_other = row_other[param2]\n",
    "\n",
    "            if func_type == 'linear':\n",
    "                y_boundary_other = func_lin(x_val_other, *func_params)\n",
    "                if func_params1 is not None:\n",
    "                    y_boundary1 = func_lin(x_val_other, *func_params1)\n",
    "            elif func_type == 'quadratic':\n",
    "                y_boundary = func_p2(x_val, *func_params)\n",
    "            elif func_type == 'cubic':\n",
    "                y_boundary = func_p3(x_val, *func_params)\n",
    "            elif func_type == 'exp':\n",
    "                y_boundary = func_exp(x_val, *func_params)\n",
    "\n",
    "            if y_val_other < y_boundary_other:\n",
    "                filtered_others += 1\n",
    "\n",
    "        print(f'Filtered out {filtered_dwarfs} real dwarfs and {filtered_others} non-dwarfs.')\n",
    "\n",
    "    # plt.ylim(0,10**4)\n",
    "    # plt.xlabel(param1)\n",
    "    # plt.ylabel(param2)\n",
    "    plt.ylim(np.min(yval_other) - 0.1, np.max(yval_other) + 0.1)\n",
    "    plt.title(f'Parameter Plot: {param1} vs {param2}')\n",
    "    # plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938bef6b-5da0-4eb1-954b-cec53b5dcd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "band = 'cfis_lsb-r'\n",
    "\n",
    "if band == 'whigs-g':\n",
    "    a, b = 0.7063, 9.4420\n",
    "    a1, b1 = 4.1000, -78.0000\n",
    "    df_dwarf, df_other = df_dwarf_g, df_other_g\n",
    "    training_data = training_data_g\n",
    "elif band == 'cfis_lsb-r':\n",
    "    a, b = 0.6155, 11.3832\n",
    "    a1, b1 = 1.3500, -6.4000\n",
    "    df_dwarf, df_other = df_dwarf_r, df_other_r\n",
    "    training_data = training_data_r\n",
    "elif band == 'ps-i':\n",
    "    a, b = 0.5351, 13.1425\n",
    "    a1, b1 = 3.5000, -61.0000\n",
    "    df_dwarf, df_other = df_dwarf_i, df_other_i\n",
    "    training_data = training_data_i\n",
    "\n",
    "param1, param2 = 'mag', 'mu'  # ('r_100_arcsec','r_90_arcsec') # 0.5952, 12.5157\n",
    "plot_params(\n",
    "    df_dwarf,\n",
    "    df_other,\n",
    "    param1,\n",
    "    param2,\n",
    "    func_type='linear',\n",
    "    func_params=[a, b],\n",
    "    func_params1=[a1, b1],\n",
    "    annotate=False,\n",
    "    plot_func=True,\n",
    "    print_filtered=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32455548-f607-4a72-9691-417f98db6e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def sigmoid_exp_func(x, a, b, c, d, e):\n",
    "    return a / (1 + np.exp(b * (x - c))) * np.exp(-d * x) + e\n",
    "\n",
    "\n",
    "def steep_fall(x, a, b):\n",
    "    return a * np.exp(-b * x) / x\n",
    "\n",
    "\n",
    "def find_separator(\n",
    "    X,\n",
    "    y,\n",
    "    param1,\n",
    "    param2,\n",
    "    func_type='modified-exp',\n",
    "    degree=3,\n",
    "    positive_weight=10,\n",
    "    conservativeness=0.95,\n",
    "):\n",
    "    sorted_indices = np.argsort(X[:, 0])\n",
    "    X_sorted = X[sorted_indices]\n",
    "    y_sorted = y[sorted_indices]\n",
    "\n",
    "    decision_points = []\n",
    "    weights = []\n",
    "    for i in range(1, len(y_sorted)):\n",
    "        if y_sorted[i] != y_sorted[i - 1]:\n",
    "            mid_point = (X_sorted[i] + X_sorted[i - 1]) / 2\n",
    "            decision_points.append(mid_point)\n",
    "            weights.append(positive_weight if y_sorted[i] == 1 else 1)\n",
    "    decision_points = np.array(decision_points)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    if func_type == 'linear':\n",
    "        coeffs = np.polyfit(decision_points[:, 0], decision_points[:, 1], 1, w=weights)\n",
    "        sep_func = lambda x: np.maximum(coeffs[0] * x + coeffs[1], 0)\n",
    "        equation = f'max({coeffs[0]:.4f}*{param1} + {coeffs[1]:.4f}, 0)'\n",
    "\n",
    "    elif func_type == 'polynomial':\n",
    "\n",
    "        def poly_exp_func(x, *coeffs):\n",
    "            return np.exp(np.polyval(coeffs, x))\n",
    "\n",
    "        p0 = np.polyfit(decision_points[:, 0], np.log(decision_points[:, 1]), degree)\n",
    "\n",
    "        popt, _ = curve_fit(\n",
    "            poly_exp_func,\n",
    "            decision_points[:, 0],\n",
    "            decision_points[:, 1],\n",
    "            p0=p0,\n",
    "            sigma=1 / weights,\n",
    "            maxfev=10000,\n",
    "        )\n",
    "\n",
    "        sep_func = lambda x: poly_exp_func(x, *popt)\n",
    "        equation = (\n",
    "            'exp('\n",
    "            + ' + '.join([f'{popt[i]:.4f}*{param1}^{len(popt) - i - 1}' for i in range(len(popt))])\n",
    "            + ')'\n",
    "        )\n",
    "\n",
    "    elif func_type == 'exponential':\n",
    "\n",
    "        def exp_func(x, a, b, c):\n",
    "            return a * np.exp(-b * x) + c\n",
    "\n",
    "        y_min, y_max = np.min(X[:, 1]), np.max(X[:, 1])\n",
    "        x_min, x_max = np.min(X[:, 0]), np.max(X[:, 0])\n",
    "\n",
    "        a_guess = y_max - y_min\n",
    "        b_guess = 1 / (x_max - x_min)\n",
    "        c_guess = y_min\n",
    "\n",
    "        lower_bounds = [0, 0, 0]\n",
    "        upper_bounds = [np.inf, np.inf, y_min]\n",
    "\n",
    "        popt, _ = curve_fit(\n",
    "            exp_func,\n",
    "            decision_points[:, 0],\n",
    "            decision_points[:, 1],\n",
    "            p0=[a_guess, b_guess, c_guess],\n",
    "            bounds=(lower_bounds, upper_bounds),\n",
    "            maxfev=10000,\n",
    "        )\n",
    "\n",
    "        sep_func = lambda x: exp_func(x, *popt)\n",
    "        equation = f'{popt[0]:.4f} * exp(-{popt[1]:.4f} * {param1}) + {popt[2]:.4f}'\n",
    "\n",
    "    elif func_type == 'modified-exp':\n",
    "\n",
    "        def modified_exp_func(x, a, b, c, d):\n",
    "            return a * np.exp(-b * (x**c)) + d\n",
    "\n",
    "        y_min, y_max = np.min(X[:, 1]), np.max(X[:, 1])\n",
    "        x_min, x_max = np.min(X[:, 0]), np.max(X[:, 0])\n",
    "\n",
    "        a_guess = y_max - y_min\n",
    "        b_guess = 1 / (x_max - x_min)\n",
    "        c_guess = 1\n",
    "        d_guess = y_min\n",
    "\n",
    "        lower_bounds = [0, 0, 0.1, 0]\n",
    "        upper_bounds = [np.inf, np.inf, 2, y_min]\n",
    "\n",
    "        try:\n",
    "            popt, _ = curve_fit(\n",
    "                modified_exp_func,\n",
    "                decision_points[:, 0],\n",
    "                decision_points[:, 1],\n",
    "                p0=[a_guess, b_guess, c_guess, d_guess],\n",
    "                bounds=(lower_bounds, upper_bounds),\n",
    "                maxfev=50000,\n",
    "            )  # Increased max iterations\n",
    "\n",
    "            sep_func = lambda x: modified_exp_func(x, *popt)\n",
    "            equation = (\n",
    "                f'{popt[0]:.4f} * exp(-{popt[1]:.4f} * ({param1} ** {popt[2]:.4f})) + {popt[3]:.4f}'\n",
    "            )\n",
    "        except RuntimeError:\n",
    "            print('Modified exponential fit failed. Falling back to simple exponential.')\n",
    "            return find_separator(\n",
    "                X, y, param1, param2, 'exponential', degree, positive_weight, conservativeness\n",
    "            )\n",
    "    elif func_type == 'sigmoid-exp':\n",
    "        y_min, y_max = np.min(X[:, 1]), np.max(X[:, 1])\n",
    "        x_min, x_max = np.min(X[:, 0]), np.max(X[:, 0])\n",
    "\n",
    "        a_guess = y_max - y_min\n",
    "        b_guess = 1\n",
    "        c_guess = (x_max + x_min) / 2\n",
    "        d_guess = 0.1\n",
    "        e_guess = y_min\n",
    "\n",
    "        lower_bounds = [0, 0, x_min, 0, 0]\n",
    "        upper_bounds = [np.inf, np.inf, x_max, np.inf, y_min]\n",
    "\n",
    "        try:\n",
    "            popt, _ = curve_fit(\n",
    "                sigmoid_exp_func,\n",
    "                decision_points[:, 0],\n",
    "                decision_points[:, 1],\n",
    "                p0=[a_guess, b_guess, c_guess, d_guess, e_guess],\n",
    "                bounds=(lower_bounds, upper_bounds),\n",
    "                maxfev=50000,\n",
    "            )\n",
    "\n",
    "            sep_func = lambda x: sigmoid_exp_func(x, *popt)\n",
    "            equation = f'{popt[0]:.4f} / (1 + exp({popt[1]:.4f} * ({param1} - {popt[2]:.4f}))) * exp(-{popt[3]:.4f} * {param1}) + {popt[4]:.4f}'\n",
    "        except RuntimeError:\n",
    "            print('Sigmoid-exponential fit failed. Falling back to modified exponential.')\n",
    "            return find_separator(\n",
    "                X, y, param1, param2, 'modified-exp', degree, positive_weight, conservativeness\n",
    "            )\n",
    "    elif func_type == 'steep-fall':\n",
    "        y_min, y_max = np.min(X[:, 1]), np.max(X[:, 1])\n",
    "        x_min, x_max = np.min(X[:, 0]), np.max(X[:, 0])\n",
    "\n",
    "        a_guess = y_max - y_min\n",
    "        b_guess = 1 / (x_max - x_min)\n",
    "\n",
    "        lower_bounds = [0, 0]\n",
    "        upper_bounds = [np.inf, np.inf]\n",
    "\n",
    "        try:\n",
    "            popt, _ = curve_fit(\n",
    "                steep_fall,\n",
    "                decision_points[:, 0],\n",
    "                decision_points[:, 1],\n",
    "                p0=[a_guess, b_guess],\n",
    "                bounds=(lower_bounds, upper_bounds),\n",
    "                maxfev=50000,\n",
    "            )\n",
    "\n",
    "            sep_func = lambda x: steep_fall(x, *popt)\n",
    "            equation = f'{popt[0]:.4f} * exp(-{popt[1]:.4f} * {param1}) / {param1}'\n",
    "        except RuntimeError:\n",
    "            print('Steep fall fit failed. Falling back to exponential.')\n",
    "            return find_separator(\n",
    "                X, y, param1, param2, 'exponential', degree, positive_weight, conservativeness\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid function type. Choose 'linear', 'polynomial', 'exponential', or 'modified-exp'.\"\n",
    "        )\n",
    "\n",
    "    # Apply conservativeness adjustment\n",
    "    positive_examples = X[y == 1]\n",
    "    distances = positive_examples[:, 1] - sep_func(positive_examples[:, 0])\n",
    "    adjustment = np.percentile(distances[distances < 0], (1 - conservativeness) * 100)\n",
    "\n",
    "    # Adjust the function\n",
    "    if func_type == 'linear':\n",
    "        coeffs[1] += adjustment\n",
    "        sep_func = lambda x: np.maximum(coeffs[0] * x + coeffs[1], 0)\n",
    "        equation = f'max({coeffs[0]:.4f}*{param1} + {coeffs[1]:.4f}, 0)'\n",
    "    elif func_type == 'polynomial':\n",
    "        popt[-1] += adjustment\n",
    "        sep_func = lambda x: poly_exp_func(x, *popt)\n",
    "        equation = (\n",
    "            'exp('\n",
    "            + ' + '.join([f'{popt[i]:.4f}*{param1}^{len(popt) - i - 1}' for i in range(len(popt))])\n",
    "            + ')'\n",
    "        )\n",
    "    elif func_type in ['exponential', 'modified-exp', 'sigmoid-exp', 'steep-fall']:\n",
    "        popt[0] *= 1 + adjustment / popt[0]\n",
    "        if func_type == 'exponential':\n",
    "            sep_func = lambda x: exp_func(x, *popt)\n",
    "            equation = f'{popt[0]:.4f} * exp(-{popt[1]:.4f} * {param1}) + {popt[2]:.4f}'\n",
    "        elif func_type == 'modified-exp':\n",
    "            sep_func = lambda x: modified_exp_func(x, *popt)\n",
    "            equation = (\n",
    "                f'{popt[0]:.4f} * exp(-{popt[1]:.4f} * ({param1} ** {popt[2]:.4f})) + {popt[3]:.4f}'\n",
    "            )\n",
    "        elif func_type == 'sigmoid-exp':\n",
    "            sep_func = lambda x: sigmoid_exp_func(x, *popt)\n",
    "            equation = f'{popt[0]:.4f} / (1 + exp({popt[1]:.4f} * ({param1} - {popt[2]:.4f}))) * exp(-{popt[3]:.4f} * {param1}) + {popt[4]:.4f}'\n",
    "        else:  # steep-fall\n",
    "            sep_func = lambda x: steep_fall(x, *popt)\n",
    "            equation = f'{popt[0]:.4f} * exp(-{popt[1]:.4f} * {param1}) / {param1}'\n",
    "\n",
    "    return sep_func, equation\n",
    "\n",
    "\n",
    "def evaluate_separator(X, y, sep_func):\n",
    "    y_pred = sep_func(X[:, 0]) > X[:, 1]\n",
    "    true_positives = np.sum((y == 1) & (y_pred == 1))\n",
    "    false_negatives = np.sum((y == 1) & (y_pred == 0))\n",
    "    true_negatives = np.sum((y == 0) & (y_pred == 0))\n",
    "    false_positives = np.sum((y == 0) & (y_pred == 1))\n",
    "\n",
    "    sensitivity = true_positives / (true_positives + false_negatives)\n",
    "    specificity = true_negatives / (true_negatives + false_positives)\n",
    "    positive_predictive_value = (\n",
    "        true_positives / (true_positives + false_positives)\n",
    "        if (true_positives + false_positives) > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return sensitivity, specificity, positive_predictive_value\n",
    "\n",
    "\n",
    "def plot_separator(X, y, sep_func, param1, param2, func_type, save_plot=True):\n",
    "    plt.close('all')\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(\n",
    "        X[y == 0][:, 0], X[y == 0][:, 1], c=colors['red'], label='Non-dwarf candidates', alpha=0.5\n",
    "    )\n",
    "    plt.scatter(\n",
    "        X[y == 1][:, 0], X[y == 1][:, 1], c=colors['green'], label='Literature dwarfs', alpha=0.5\n",
    "    )\n",
    "\n",
    "    x_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 1000)\n",
    "    plt.plot(\n",
    "        x_range, sep_func(x_range), color='black', lw=3, label=f'{func_type.capitalize()} Separator'\n",
    "    )\n",
    "\n",
    "    xinterval = X[:, 0].max() - X[:, 0].min()\n",
    "    yinterval = X[:, 1].max() - X[:, 1].min()\n",
    "\n",
    "    plt.xlim(X[:, 0].min() - (xinterval / 10), X[:, 0].max() + (xinterval / 10))\n",
    "    plt.ylim(X[:, 1].min() - (yinterval / 10), X[:, 1].max() + (yinterval / 10))\n",
    "    print(\n",
    "        f'xlim: {X[:, 0].min() - (X[:, 0].mean() / 10)}/{X[:, 0].max() + (X[:, 0].mean() / 10)}, ylim: {X[:, 1].min() - (X[:, 1].mean() / 10)}/{X[:, 1].max() + (X[:, 1].mean() / 10)}'\n",
    "    )\n",
    "    plt.xlabel(r'$m_{i}$', fontsize=18)\n",
    "    # <\\mu_{r}>_{e}\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.ylabel(r'$R_{90}/R_{75}$', fontsize=18)\n",
    "    plt.yticks(fontsize=12)\n",
    "    # plt.legend(fontsize=13)\n",
    "    # plt.title(f'{func_type.capitalize()} Separation Line')\n",
    "    if save_plot:\n",
    "        plt.savefig(\n",
    "            '/arc/home/heestersnick/dwarforge/figures/filter_plot_r_90_r_75_mag_i.png',\n",
    "            dpi=300,\n",
    "            bbox_inches='tight',\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def adjust_parameters(func_type, popt, param1):\n",
    "    print('\\nCurrent parameters:')\n",
    "    for i, param in enumerate(popt):\n",
    "        print(f'Parameter {i}: {param:.6f}')\n",
    "\n",
    "    while True:\n",
    "        index = input(\n",
    "            \"\\nEnter the index of the parameter you want to adjust (or 'done' to finish): \"\n",
    "        )\n",
    "        if index.lower() == 'done':\n",
    "            break\n",
    "        try:\n",
    "            index = int(index)\n",
    "            if 0 <= index < len(popt):\n",
    "                new_value = float(\n",
    "                    input(f'Enter new value for parameter {index} (current: {popt[index]:.6f}): ')\n",
    "                )\n",
    "                popt[index] = new_value\n",
    "            else:\n",
    "                print('Invalid index. Please try again.')\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number or 'done'.\")\n",
    "\n",
    "    if func_type == 'linear':\n",
    "        sep_func = lambda x: np.maximum(popt[0] * x + popt[1], 0)\n",
    "        equation = f'max({popt[0]:.4f}*{param1} + {popt[1]:.4f}, 0)'\n",
    "    elif func_type == 'polynomial':\n",
    "        sep_func = lambda x: np.exp(np.polyval(popt, x))\n",
    "        equation = (\n",
    "            'exp('\n",
    "            + ' + '.join([f'{popt[i]:.4f}*{param1}^{len(popt) - i - 1}' for i in range(len(popt))])\n",
    "            + ')'\n",
    "        )\n",
    "    elif func_type == 'exponential':\n",
    "        sep_func = lambda x: popt[0] * np.exp(-popt[1] * x) + popt[2]\n",
    "        equation = f'{popt[0]:.4f} * exp(-{popt[1]:.4f} * {param1}) + {popt[2]:.4f}'\n",
    "    elif func_type == 'modified-exp':\n",
    "        sep_func = lambda x: popt[0] * np.exp(-popt[1] * (x ** popt[2])) + popt[3]\n",
    "        equation = (\n",
    "            f'{popt[0]:.4f} * exp(-{popt[1]:.4f} * ({param1} ** {popt[2]:.4f})) + {popt[3]:.4f}'\n",
    "        )\n",
    "    elif func_type == 'sigmoid-exp':\n",
    "        sep_func = lambda x: sigmoid_exp_func(x, *popt)\n",
    "        equation = f'{popt[0]:.4f} / (1 + exp({popt[1]:.4f} * ({param1} - {popt[2]:.4f}))) * exp(-{popt[3]:.4f} * {param1}) + {popt[4]:.4f}'\n",
    "    elif func_type == 'steep-fall':\n",
    "        sep_func = lambda x: steep_fall(x, *popt)\n",
    "        equation = f'{popt[0]:.4f} * exp(-{popt[1]:.4f} * {param1}) / {param1}'\n",
    "\n",
    "    return sep_func, equation, popt\n",
    "\n",
    "\n",
    "def add_fraction_to_df(df, param1, param2, log=False):\n",
    "    df_mod = df.copy()\n",
    "    if log:\n",
    "        df_mod[f'log_{param1}_{param2}'] = np.log(\n",
    "            np.divide(\n",
    "                df_mod[param1].values.astype(np.float32),\n",
    "                df_mod[param2].values.astype(np.float32),\n",
    "                where=df_mod[param2] != 0,\n",
    "                out=np.zeros_like(df_mod[param1].values.astype(np.float32)),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        df_mod[f'{param1}_{param2}'] = df_mod[param1].values.astype(np.float32) / df_mod[\n",
    "            param2\n",
    "        ].values.astype(np.float32)\n",
    "    return df_mod\n",
    "\n",
    "\n",
    "def add_log_to_df(df, param):\n",
    "    df_mod = df.copy()\n",
    "    df_mod[f'log_{param}'] = np.log(df_mod[param].values)\n",
    "    return df_mod\n",
    "\n",
    "\n",
    "def main(df, param1, param2, class_column):\n",
    "    X = df[[param1, param2]].values\n",
    "    y = df[class_column].values\n",
    "\n",
    "    func_types = [\n",
    "        'linear',\n",
    "        'polynomial',\n",
    "        'exponential',\n",
    "        'modified-exp',\n",
    "        'sigmoid-exp',\n",
    "        'steep-fall',\n",
    "    ]\n",
    "    print('Available function types:', ', '.join(func_types))\n",
    "    chosen_type = input('Enter the function type you want to use: ').lower()\n",
    "\n",
    "    if chosen_type not in func_types:\n",
    "        print('Invalid function type. Defaulting to modified-exp.')\n",
    "        chosen_type = 'modified-exp'\n",
    "\n",
    "    conservativeness = float(input('Enter conservativeness (0-1, default 0.95): ') or 0.95)\n",
    "    positive_weight = float(input('Enter positive weight (default 10): ') or 10)\n",
    "\n",
    "    if chosen_type == 'polynomial':\n",
    "        degree = int(input('Enter the degree of the polynomial (default is 3): ') or 3)\n",
    "        sep_func, equation = find_separator(\n",
    "            X,\n",
    "            y,\n",
    "            param1,\n",
    "            param2,\n",
    "            chosen_type,\n",
    "            degree=degree,\n",
    "            positive_weight=positive_weight,\n",
    "            conservativeness=conservativeness,\n",
    "        )\n",
    "    else:\n",
    "        sep_func, equation = find_separator(\n",
    "            X,\n",
    "            y,\n",
    "            param1,\n",
    "            param2,\n",
    "            chosen_type,\n",
    "            positive_weight=positive_weight,\n",
    "            conservativeness=conservativeness,\n",
    "        )\n",
    "\n",
    "    # Extract parameters from the equation\n",
    "    if chosen_type == 'linear':\n",
    "        # More robust extraction method for linear function\n",
    "        parts = equation.split('max(')[1].split(')')[0].split(',')\n",
    "        linear_part = parts[0].strip()\n",
    "        if '*' in linear_part:\n",
    "            coefficient, rest = linear_part.split('*')\n",
    "            coefficient = float(coefficient)\n",
    "        else:\n",
    "            coefficient = 1.0\n",
    "            rest = linear_part\n",
    "\n",
    "        rest_parts = rest.split('+')\n",
    "        if len(rest_parts) > 1:\n",
    "            intercept = float(rest_parts[1].strip())\n",
    "        else:\n",
    "            intercept = 0.0\n",
    "\n",
    "        popt = [coefficient, intercept]\n",
    "        print(f'Extracted linear parameters: coefficient = {coefficient}, intercept = {intercept}')\n",
    "    elif chosen_type == 'polynomial':\n",
    "        popt = [float(x.split('*')[0]) for x in equation.split('exp(')[1].split(')')[0].split('+')]\n",
    "    elif chosen_type == 'exponential':\n",
    "        match = re.search(\n",
    "            r'(\\d+\\.\\d+)\\s*\\*\\s*exp\\(-(\\d+\\.\\d+)\\s*\\*\\s*'\n",
    "            + re.escape(param1)\n",
    "            + r'\\)\\s*\\+\\s*(\\d+\\.\\d+)',\n",
    "            equation,\n",
    "        )\n",
    "        if match:\n",
    "            popt = [float(match.group(1)), float(match.group(2)), float(match.group(3))]\n",
    "        else:\n",
    "            raise ValueError('Unable to extract parameters from the equation.')\n",
    "    elif chosen_type == 'modified-exp':\n",
    "        match = re.search(\n",
    "            r'(\\d+\\.\\d+)\\s*\\*\\s*exp\\(-(\\d+\\.\\d+)\\s*\\*\\s*\\('\n",
    "            + re.escape(param1)\n",
    "            + r'\\s*\\*\\*\\s*(\\d+\\.\\d+)\\)\\)\\s*\\+\\s*(\\d+\\.\\d+)',\n",
    "            equation,\n",
    "        )\n",
    "        if match:\n",
    "            popt = [\n",
    "                float(match.group(1)),\n",
    "                float(match.group(2)),\n",
    "                float(match.group(3)),\n",
    "                float(match.group(4)),\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError('Unable to extract parameters from the equation.')\n",
    "    elif chosen_type == 'sigmoid-exp':\n",
    "        match = re.search(\n",
    "            r'(\\d+\\.\\d+)\\s*/\\s*\\(1\\s*\\+\\s*exp\\((\\d+\\.\\d+)\\s*\\*\\s*\\('\n",
    "            + re.escape(param1)\n",
    "            + r'\\s*-\\s*(\\d+\\.\\d+)\\)\\)\\)\\s*\\*\\s*exp\\(-(\\d+\\.\\d+)\\s*\\*\\s*'\n",
    "            + re.escape(param1)\n",
    "            + r'\\)\\s*\\+\\s*(\\d+\\.\\d+)',\n",
    "            equation,\n",
    "        )\n",
    "        if match:\n",
    "            popt = [\n",
    "                float(match.group(1)),\n",
    "                float(match.group(2)),\n",
    "                float(match.group(3)),\n",
    "                float(match.group(4)),\n",
    "                float(match.group(5)),\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError('Unable to extract parameters from the equation.')\n",
    "    elif chosen_type == 'steep-fall':\n",
    "        match = re.search(\n",
    "            r'(\\d+\\.\\d+)\\s*\\*\\s*exp\\(-(\\d+\\.\\d+)\\s*\\*\\s*'\n",
    "            + re.escape(param1)\n",
    "            + r'\\)\\s*/\\s*'\n",
    "            + re.escape(param1),\n",
    "            equation,\n",
    "        )\n",
    "        if match:\n",
    "            popt = [float(match.group(1)), float(match.group(2))]\n",
    "        else:\n",
    "            raise ValueError('Unable to extract parameters from the equation.')\n",
    "\n",
    "    while True:\n",
    "        sensitivity, specificity, ppv = evaluate_separator(X, y, sep_func)\n",
    "\n",
    "        print(f'\\n{chosen_type.capitalize()} separation line equation:')\n",
    "        print(f'{param2} = {equation}')\n",
    "        print(f'\\nSensitivity (True Positive Rate): {sensitivity:.4f}')\n",
    "        print(f'Specificity (True Negative Rate): {specificity:.4f}')\n",
    "        print(f'Positive Predictive Value: {ppv:.4f}')\n",
    "\n",
    "        plot_separator(X, y, sep_func, param1, param2, chosen_type)\n",
    "\n",
    "        satisfied = input('\\nAre you satisfied with the fit? (yes/no): ').lower()\n",
    "        if satisfied == 'yes':\n",
    "            break\n",
    "\n",
    "        sep_func, equation, popt = adjust_parameters(chosen_type, popt, param1)\n",
    "\n",
    "    print('\\nFinal separator function:')\n",
    "    print(f'{param2} = {equation}')\n",
    "\n",
    "\n",
    "# Define column names\n",
    "PARAM1 = 'mag'\n",
    "log_param1 = False\n",
    "PARAM2 = 'r_90_arcsec_r_75_arcsec'\n",
    "log_param2 = False\n",
    "CLASS_COLUMN = 'lsb'\n",
    "# Run the analysisparam\n",
    "if __name__ == '__main__':\n",
    "    training_data_mod = training_data_i[training_data_i['r_10_arcsec'] > 0.0].reset_index(drop=True)\n",
    "    training_data_mod = training_data_mod[training_data_mod['r_25_arcsec'] > 0.0].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    # training_data_mod = training_data_i[training_data_i['mu_median'] > 0.].reset_index(drop=True)\n",
    "    training_data_mod = add_fraction_to_df(\n",
    "        training_data_mod, 'r_90_arcsec', 'r_75_arcsec', log=log_param2\n",
    "    )\n",
    "    # training_data_mod = add_fraction_to_df(training_data_mod, 're_arcsec', 'r_10_arcsec', log=False)\n",
    "    if log_param1:\n",
    "        training_data_mod = add_log_to_df(training_data_mod, 'mag')\n",
    "    main(training_data_mod, PARAM1, PARAM2, CLASS_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910fca2-bfa4-477d-9566-d14a7ed21bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_candidates(df, band):\n",
    "    df_mod = df.copy()\n",
    "    df_dwarf = df.loc[df['lsb'] == 1].reset_index(drop=True)\n",
    "\n",
    "    # Define band-specific conditions\n",
    "    band_conditions = {\n",
    "        'cfis_lsb-r': {\n",
    "            'basic': {\n",
    "                'mu': (22.0, None),  # (min, max), None means no limit\n",
    "                're_arcsec': (1.6, 55.0),\n",
    "                'axis_ratio': (0.17, None),\n",
    "                'r_10_arcsec': (0.353, 18.2),\n",
    "                'r_25_arcsec': (0.4, 32.1),\n",
    "                'r_75_arcsec': (2.16, 102.1),\n",
    "                'r_90_arcsec': (2.5, 145.1),\n",
    "                'r_100_arcsec': (2.8, 254.1),\n",
    "                'r_fwhm_arcsec': (0.4, 13.8),\n",
    "                'mu_median': (0.34, 28.7),\n",
    "                'mu_mean': (0.4, 65.1),\n",
    "                'mu_max': (2.0, 6255.0),\n",
    "                'total_flux': (55, None),\n",
    "                'mag': (12.17, 25.7),\n",
    "            },\n",
    "            'complex': [\n",
    "                lambda df_mod: (df_mod['mu'] > (0.6155 * df_mod['mag'] + 11.3832))\n",
    "                & (df_mod['mu'] > (1.35 * df_mod['mag'] - 6.4)),\n",
    "                lambda df_mod: (df_mod['mu'] / df_mod['r_75_arcsec'])\n",
    "                < np.maximum(1.51 * df_mod['mag'] - 24.6, 3.0),\n",
    "                lambda df_mod: np.log(df_mod['r_90_arcsec'] / df_mod['r_75_arcsec'])\n",
    "                < (0.295 * np.log(df_mod['r_100_arcsec']) - 0.13),\n",
    "            ],\n",
    "        },\n",
    "        'whigs-g': {\n",
    "            'basic': {\n",
    "                'mu': (22.51, None),  # (min, max), None means no limit\n",
    "                're_arcsec': (1.6, None),\n",
    "                'axis_ratio': (0.17, None),\n",
    "                'r_10_arcsec': (0.4, 8.1),\n",
    "                'r_25_arcsec': (0.72, 15.0),\n",
    "                'r_75_arcsec': (2.17, 42.0),\n",
    "                'r_90_arcsec': (2.47, 54.0),\n",
    "                'r_100_arcsec': (2.8, 79.0),\n",
    "                'r_fwhm_arcsec': (0.417, 13.5),\n",
    "                'mu_median': (0.0072, 0.8),\n",
    "                'mu_mean': (0.017, 2.01),\n",
    "                'mu_max': (0.066, 226.0),\n",
    "                'total_flux': (1.77, None),\n",
    "                'mag': (16.0, 26.5),\n",
    "            },\n",
    "            'complex': [\n",
    "                lambda df_mod: (df_mod['mu'] > (0.7063 * df_mod['mag'] + 9.4420))\n",
    "                & (df_mod['mu'] > (4.1 * df_mod['mag'] - 78.0)),\n",
    "                lambda df_mod: np.log(df_mod['r_100_arcsec'] / df_mod['r_90_arcsec'])\n",
    "                < (-1.2615 * np.log(df_mod['mag']) + 4.3000),\n",
    "                lambda df_mod: np.log(df_mod['mag'] / df_mod['r_75_arcsec'])\n",
    "                < np.maximum(6.4500 * np.log(df_mod['mag']) + -17.9000, 0.75),\n",
    "            ],\n",
    "        },\n",
    "        'ps-i': {\n",
    "            'basic': {\n",
    "                'mu': (22.0, 29.5),  # (min, max), None means no limit\n",
    "                're_arcsec': (1.6, 40.0),\n",
    "                'axis_ratio': (0.17, None),\n",
    "                'r_10_arcsec': (0.4, 14.95),\n",
    "                'r_25_arcsec': (0.58, 25.5),\n",
    "                'r_75_arcsec': (2.1, 58.0),\n",
    "                'r_90_arcsec': (2.5, 77.0),\n",
    "                'r_100_arcsec': (2.7, 117.0),\n",
    "                'r_fwhm_arcsec': (0.4, 20.8),\n",
    "                'mu_median': (0.445, 40.7),\n",
    "                'mu_mean': (0.565, 100.8),\n",
    "                'mu_max': (2.47, 4465.0),\n",
    "                'total_flux': (66.0, None),\n",
    "                'mag': (13.1, 26.0),\n",
    "            },\n",
    "            'complex': [\n",
    "                lambda df_mod: (df_mod['r_90_arcsec'] / df_mod['r_75_arcsec'])\n",
    "                < (0.0100 * df_mod['r_100_arcsec'] + 1.4000),\n",
    "                lambda df_mod: (df_mod['r_100_arcsec'] / df_mod['r_90_arcsec'])\n",
    "                < (-0.0850 * df_mod['mag'] + 3.3000),\n",
    "                lambda df_mod: (df_mod['mag'] / df_mod['r_75_arcsec'])\n",
    "                < np.maximum(1.4 * df_mod['mag'] - 23, 2.5),\n",
    "                lambda df_mod: (df_mod['r_90_arcsec'] / df_mod['r_75_arcsec'])\n",
    "                < (-0.0700 * df_mod['mag'] + 2.97),\n",
    "                lambda df_mod: (df_mod['r_75_arcsec'] / df_mod['r_25_arcsec'])\n",
    "                < (0.2 * df_mod['mag'] + 1.0),\n",
    "                lambda df_mod: (df_mod['r_75_arcsec'] / df_mod['r_25_arcsec'])\n",
    "                < (-1.1 * df_mod['mag'] + 30.5),\n",
    "                lambda df_mod: np.log(df_mod['r_100_arcsec'] / df_mod['r_75_arcsec'])\n",
    "                < (-2.7 * np.log(df_mod['mag']) + 9.05),\n",
    "                lambda df_mod: np.log(df_mod['r_75_arcsec'] / df_mod['r_25_arcsec'])\n",
    "                < (-0.9631 * np.log(df_mod['mag']) + 4.5500),\n",
    "                lambda df_mod: np.log(df_mod['r_75_arcsec'] / df_mod['r_25_arcsec'])\n",
    "                < (0.8 * np.log(df_mod['mag']) - 0.81),\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if band not in band_conditions:\n",
    "        logger.error(f'Conditions not implemented for band {band}.')\n",
    "        return None\n",
    "\n",
    "    conditions = band_conditions[band]\n",
    "\n",
    "    # Apply basic conditions\n",
    "    for column, (min_val, max_val) in conditions['basic'].items():\n",
    "        if min_val is not None:\n",
    "            df_mod = df_mod[df_mod[column] > min_val]\n",
    "        if max_val is not None:\n",
    "            df_mod = df_mod[df_mod[column] < max_val]\n",
    "\n",
    "    # Apply complex conditions\n",
    "    for condition in conditions['complex']:\n",
    "        df_mod = df_mod[condition]\n",
    "\n",
    "    # Reset index\n",
    "    df_mod = df_mod.reset_index(drop=True)\n",
    "\n",
    "    print(\n",
    "        f'Filtered out {len(df.loc[df[\"lsb\"] == 1]) - len(df_mod.loc[df_mod[\"lsb\"] == 1])}/{len(df_dwarf)} dwarfs.'\n",
    "    )\n",
    "    print(\n",
    "        f'Filtered out {len(df.loc[df[\"lsb\"] == 0]) - len(df_mod.loc[df_mod[\"lsb\"] == 0])}/{len(df.loc[df[\"lsb\"] == 0])} other objects.'\n",
    "    )\n",
    "\n",
    "    return df_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9630851-1ebb-4d83-9469-303a50bb8697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max = df_dwarf_i.agg(['min', 'max'])\n",
    "# min_max_df = min_max[['r_10_arcsec', 'r_25_arcsec', 're_arcsec', 'r_fwhm_arcsec', 'r_75_arcsec', 'r_90_arcsec', 'r_100_arcsec', 'mu_max', 'mu_median', 'mu_mean', 'axis_ratio', 'mag', 'mu', 'total_flux']]\n",
    "# min_max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faa4baa-4352-41e3-a2d2-ca6d777cc68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "# plt.figure()\n",
    "# plt.hist(df_dwarf_i['r_10_arcsec'].values, bins=100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b014dc6-a5bf-4126-b1a0-8a1ce2884bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = 'whigs-g'\n",
    "\n",
    "if band == 'whigs-g':\n",
    "    a, b = 0.7063, 9.4420\n",
    "    a1, b1 = 4.1000, -78.0000\n",
    "    df_dwarf, df_other = df_dwarf_g, df_other_g\n",
    "    training_data = training_data_g\n",
    "elif band == 'cfis_lsb-r':\n",
    "    a, b = 0.6155, 11.3832\n",
    "    a1, b1 = 1.3500, -6.4000\n",
    "    df_dwarf, df_other = df_dwarf_r, df_other_r\n",
    "    training_data = training_data_r\n",
    "elif band == 'ps-i':\n",
    "    a, b = 0.5351, 13.1425\n",
    "    a1, b1 = 3.5000, -61.0000\n",
    "    df_dwarf, df_other = df_dwarf_i, df_other_i\n",
    "    training_data = training_data_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97406790-1f92-4dea-ad83-ab052c161756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = check_param_filter(training_data, a, b, band=band, a1=a1, b1=b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ab355-597f-4dc2-baf5-3ec1657c256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_candidates(training_data_r, band='cfis_lsb-r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5144b-ed2b-4aa3-b67d-cb5cd91daa57",
   "metadata": {},
   "source": [
    "# Check cutouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f478a8-de0d-4429-a6bb-43784f46df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_file = read_h5(\n",
    "    '/arc/projects/unions/ssl/data/raw/tiles/dwarforge/378_270/gri/378_270_matched_cutouts.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4731a0-c3ac-4df1-ae4b-9e2dc5888469",
   "metadata": {},
   "outputs": [],
   "source": [
    "378_270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67545df7-9739-46a9-96d1-321a3bcfae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_file['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359b9b6-ae05-4ae3-9353-6a9dbf57ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutouts = cutout_file['images'][:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc03c2c-09bd-4ab2-adc8-1c8d860b2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cutouts(cutouts, num_cutouts=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19432f15-fd98-4c7a-a482-e2d2c5af0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "from vos import Client\n",
    "\n",
    "\n",
    "def gather_cutout_data(parent_dir_source, parent_dir_destination, band='cfis_lsb-r'):\n",
    "    pattern = os.path.join(parent_dir_source, '*_*', band, '*.r_cutouts.h5')\n",
    "    #     pattern = os.path.join(parent_dir_source, band, \"*.r_cutouts.h5\")\n",
    "    cutout_files = Client().glob(pattern)\n",
    "    print(len(cutout_files))\n",
    "    for file in tqdm(cutout_files[:10]):\n",
    "        try:\n",
    "            filename = os.path.basename(file)\n",
    "            name, ext = os.path.splitext(filename)\n",
    "            tile_numbers = filename.split('.')[1:3]\n",
    "            tile_id = f'{tile_numbers[0].zfill(3)}_{tile_numbers[1].zfill(3)}'\n",
    "            dir_destination = os.path.join(parent_dir_destination, tile_id, band)\n",
    "            os.makedirs(dir_destination, exist_ok=True)\n",
    "            temp_path = os.path.join(dir_destination, name + '_temp' + ext)\n",
    "            final_path = os.path.join(dir_destination, filename)\n",
    "\n",
    "            print(f'Downloading {filename} for band {band}...')\n",
    "\n",
    "            result = subprocess.run(\n",
    "                f'vcp -v {file} {temp_path}', shell=True, stderr=subprocess.PIPE, text=True\n",
    "            )\n",
    "\n",
    "            result.check_returncode()\n",
    "\n",
    "            os.rename(temp_path, final_path)\n",
    "\n",
    "            print(f'Successfully downloaded {filename} for band {band} to {dir_destination}.')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error downloading file {file}: {str(e)}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e842b98-9ead-4329-84fc-89d5af92679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir_source = 'arc:/projects/unions/ssl/data/raw/tiles/dwarforge'\n",
    "parent_dir_destination = '/home/nick/astro/DwarForge/data'\n",
    "\n",
    "gather_cutout_data(parent_dir_source, parent_dir_destination, band='cfis_lsb-r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c2716-a506-4b7f-a1c0-c3514ff620cd",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348b98e-cbb0-465d-bb8a-6db59c48b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_file(file_path):\n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "        # print(f'File {os.path.split(file_path)[1]} has been deleted successfully')\n",
    "    except FileNotFoundError:\n",
    "        print(f'File {os.path.split(file_path)[1]} does not exist')\n",
    "    except PermissionError:\n",
    "        print(f'Permission denied: unable to delete {os.path.split(file_path)[1]}')\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred while deleting the file: {e}')\n",
    "\n",
    "\n",
    "def delete_segmaps(parent_folder, in_dict, band='cfis_lsb-r'):\n",
    "    zfill = in_dict[band]['zfill']\n",
    "    file_prefix = in_dict[band]['name']\n",
    "    delimiter = in_dict[band]['delimiter']\n",
    "    suffix = in_dict[band]['suffix']\n",
    "    pattern = os.path.join(\n",
    "        parent_folder, '*_*', band, f'{file_prefix}{delimiter}*{suffix}_rebin_star_mask*_seg.fits'\n",
    "    )\n",
    "\n",
    "    segmap_filepaths = []\n",
    "    for file in tqdm(glob(pattern)):\n",
    "        # segmap_filepaths.append(file)\n",
    "        delete_file(file)\n",
    "\n",
    "    return segmap_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26db7e27-f323-4e90-9195-0811af43fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_paths = delete_segmaps(data_dir, in_dict=band_dict, band='ps-i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab3b5f-2d9f-4955-9783-3807f06e6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seg_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443347a-45db-45b7-9f7d-1b787f283689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DwarForge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
