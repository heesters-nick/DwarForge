{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b6832-e55f-4513-9753-ff9c826a5346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from functools import partial\n",
    "from multiprocessing import Manager, Pool\n",
    "\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.coordinates import SkyCoord, match_coordinates_sky\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a8443-bcaf-4020-815f-29e96d635acf",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453df67-85fe-4ad8-923c-dfa0ec2f7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/arc/projects/unions/ssl/data/raw/tiles/dwarforge'\n",
    "table_dir = '/arc/home/heestersnick/dwarforge/tables'\n",
    "train_df = pd.read_csv(os.path.join(table_dir, 'train_df.csv'))\n",
    "class_df = pd.read_csv(os.path.join(table_dir, 'class_df_v2.csv'))\n",
    "all_dwarfs = pd.read_csv(os.path.join(table_dir, 'all_known_dwarfs_v3_processed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298b924f-1bcd-41bb-b7c4-53e9b950f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_coordinates(detected_ra, detected_dec, train_ra, train_dec, max_separation=10.0):\n",
    "    \"\"\"\n",
    "    Match coordinates within max_separation arcseconds\n",
    "    Returns boolean array of matches and indices of matches\n",
    "    \"\"\"\n",
    "    if len(train_ra) == 0:\n",
    "        return np.zeros(len(detected_ra), dtype=bool), np.array([])\n",
    "\n",
    "    detected_coords = SkyCoord(ra=detected_ra * u.degree, dec=detected_dec * u.degree)\n",
    "    train_coords = SkyCoord(ra=train_ra * u.degree, dec=train_dec * u.degree)\n",
    "\n",
    "    idx, sep, _ = detected_coords.match_to_catalog_sky(train_coords)\n",
    "    matches = sep.arcsec <= max_separation\n",
    "\n",
    "    return matches, idx\n",
    "\n",
    "\n",
    "def process_single_file(file_path, train_df, mismatches_list):\n",
    "    \"\"\"\n",
    "    Read and process a single parquet file, adding training flag and tracking matches\n",
    "    Returns: tuple of (processed_df, match_stats)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Initialize match statistics for this tile\n",
    "        match_stats = {\n",
    "            'dwarfs_found': 0,  # objects with real IDs\n",
    "            'non_dwarfs_found': 0,  # objects with non_dwarf IDs\n",
    "        }\n",
    "\n",
    "        # Extract tile from file path and add as column\n",
    "        tile = os.path.basename(os.path.dirname(os.path.dirname(file_path)))\n",
    "        df['tile'] = tile\n",
    "\n",
    "        # Initialize training flag column\n",
    "        df['in_training_data'] = 0\n",
    "        # Initialize expert classification\n",
    "        df['visual_class'] = np.nan\n",
    "\n",
    "        # Check if this tile exists in training data\n",
    "        if tile in train_df['tile'].unique():\n",
    "            # Get training data for this tile\n",
    "            tile_train_data = train_df[train_df['tile'] == tile]\n",
    "\n",
    "            # Split training data into dwarfs and non-dwarfs\n",
    "            dwarfs = tile_train_data[\n",
    "                ~tile_train_data['known_id'].str.startswith('non_dwarf', na=False)\n",
    "            ]\n",
    "            non_dwarfs = tile_train_data[\n",
    "                tile_train_data['known_id'].str.startswith('non_dwarf', na=False)\n",
    "            ]\n",
    "\n",
    "            # Process dwarf objects\n",
    "            if len(dwarfs) > 0:\n",
    "                dwarf_matches, dwarf_idx = match_coordinates(\n",
    "                    df['ra'].values,\n",
    "                    df['dec'].values,\n",
    "                    dwarfs['ra'].values,\n",
    "                    dwarfs['dec'].values,\n",
    "                )\n",
    "                df.loc[dwarf_matches, 'in_training_data'] = 1\n",
    "                # Add visual classification for matched objects\n",
    "                df.loc[dwarf_matches, 'visual_class'] = dwarfs.iloc[dwarf_idx[dwarf_matches]][\n",
    "                    'label'\n",
    "                ].values\n",
    "                match_stats['dwarfs_found'] = np.count_nonzero(dwarf_matches)\n",
    "\n",
    "                # Check ID matches for dwarfs\n",
    "                coord_matched_indices = np.where(dwarf_matches)[0]\n",
    "                for _, train_obj in dwarfs.iterrows():\n",
    "                    id_matches = df['ID_known'] == train_obj['known_id']\n",
    "                    id_matched_indices = np.where(id_matches)[0]\n",
    "\n",
    "                    if len(id_matched_indices) > 1:\n",
    "                        print(\n",
    "                            f\"\\nFound multiple matches for ID {train_obj['known_id']} in tile {tile}\"\n",
    "                        )\n",
    "                        print(f'Found in rows: {id_matched_indices}')\n",
    "\n",
    "                    matched_by_coords = any(i in coord_matched_indices for i in id_matched_indices)\n",
    "\n",
    "                    if not matched_by_coords or (len(id_matched_indices) == 0):\n",
    "                        mismatches_list.append(\n",
    "                            {\n",
    "                                'train_id': train_obj['known_id'],\n",
    "                                'train_ra': train_obj['ra'],\n",
    "                                'train_dec': train_obj['dec'],\n",
    "                                'train_tile': tile,\n",
    "                                'coord_matched': matched_by_coords,\n",
    "                                'id_matched': len(id_matched_indices) > 0,\n",
    "                                'n_id_matches': len(id_matched_indices),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            # Process non-dwarf objects\n",
    "            if len(non_dwarfs) > 0:\n",
    "                non_dwarf_matches, non_dwarf_idx = match_coordinates(\n",
    "                    df['ra'].values,\n",
    "                    df['dec'].values,\n",
    "                    non_dwarfs['ra'].values,\n",
    "                    non_dwarfs['dec'].values,\n",
    "                )\n",
    "                df.loc[non_dwarf_matches, 'in_training_data'] = 1\n",
    "                # df.loc[non_dwarf_matches, 'ID_known'] = non_dwarfs.iloc[non_dwarf_idx[non_dwarf_matches]]['known_id'].values\n",
    "                # Add visual classification for matched objects\n",
    "                df.loc[non_dwarf_matches, 'visual_class'] = non_dwarfs.iloc[\n",
    "                    non_dwarf_idx[non_dwarf_matches]\n",
    "                ]['label'].values\n",
    "                match_stats['non_dwarfs_found'] = np.count_nonzero(non_dwarf_matches)\n",
    "\n",
    "        return df, match_stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {file_path}: {e}')\n",
    "        return pd.DataFrame(), {'dwarfs_found': 0, 'non_dwarfs_found': 0}\n",
    "\n",
    "\n",
    "def gather_data(base_dir, train_df, num_processes=4):\n",
    "    \"\"\"\n",
    "    Gather data from all parquet files and track mismatches\n",
    "    \"\"\"\n",
    "    # Create the pattern to match the parquet files\n",
    "    pattern = os.path.join(base_dir, '*_*', 'gri', '*_matched_detections.parquet')\n",
    "\n",
    "    # Get list of all matching files\n",
    "    parquet_files = glob.glob(pattern)\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise ValueError(f'No parquet files found matching pattern: {pattern}')\n",
    "\n",
    "    print(f'Found {len(parquet_files)} parquet files')\n",
    "\n",
    "    # Create manager for sharing mismatches list between processes\n",
    "    with Manager() as manager:\n",
    "        mismatches_list = manager.list()\n",
    "\n",
    "        # Create partial function with train_df and mismatches_list\n",
    "        process_file = partial(\n",
    "            process_single_file, train_df=train_df, mismatches_list=mismatches_list\n",
    "        )\n",
    "\n",
    "        # Process files with progress bar\n",
    "        dfs = []\n",
    "        total_dwarfs_found = 0\n",
    "        total_non_dwarfs_found = 0\n",
    "\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            for df, stats in tqdm(\n",
    "                pool.imap(process_file, parquet_files),\n",
    "                total=len(parquet_files),\n",
    "                desc='Processing files',\n",
    "                unit='file',\n",
    "            ):\n",
    "                dfs.append(df)\n",
    "                total_dwarfs_found += stats['dwarfs_found']\n",
    "                total_non_dwarfs_found += stats['non_dwarfs_found']\n",
    "\n",
    "        # Convert mismatches list to DataFrame\n",
    "        mismatches_df = pd.DataFrame(list(mismatches_list))\n",
    "\n",
    "    print('\\nCombining dataframes...')\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Get total counts from training set\n",
    "    dwarfs = train_df[~train_df['known_id'].str.startswith('non_dwarf', na=False)]\n",
    "    non_dwarfs = train_df[train_df['known_id'].str.startswith('non_dwarf', na=False)]\n",
    "\n",
    "    print('\\nTraining object statistics:')\n",
    "    print('Dwarfs (objects with real IDs):')\n",
    "    print(f'  - Total in training set: {len(dwarfs)}')\n",
    "    print(f'  - Found in results: {total_dwarfs_found}')\n",
    "    print(f'  - Missing: {len(dwarfs) - total_dwarfs_found}')\n",
    "\n",
    "    print('\\nNon-dwarfs:')\n",
    "    print(f'  - Total in training set: {len(non_dwarfs)}')\n",
    "    print(f'  - Found in results: {total_non_dwarfs_found}')\n",
    "    print(f'  - Missing: {len(non_dwarfs) - total_non_dwarfs_found}')\n",
    "\n",
    "    return combined_df, mismatches_df\n",
    "\n",
    "\n",
    "def count_intervals(values):\n",
    "    # Create bins from 0 to 1 with 0.05 intervals\n",
    "    bins = np.arange(0, 1.05, 0.05)\n",
    "\n",
    "    # Use numpy's histogram function\n",
    "    counts, _ = np.histogram(values, bins=bins)\n",
    "\n",
    "    # Print the counts for each interval\n",
    "    for i in range(len(counts)):\n",
    "        start = round(bins[i], 2)\n",
    "        end = round(bins[i + 1], 2)\n",
    "        print(f'{start:.2f} - {end:.2f}: {counts[i]}')\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bc86f-f41f-44a8-ba58-0d97df6fc7bd",
   "metadata": {},
   "source": [
    "# Gather all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37741d6f-4cd0-43c5-86ae-5a05c529a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files\n",
    "result_df, mismatch_df = gather_data(data_dir, class_df, num_processes=16)\n",
    "\n",
    "# Save the combined dataset\n",
    "# result_df.to_parquet(\"unions_lsb_catalog_all.parquet\")\n",
    "\n",
    "print(f'Combined shape: {result_df.shape}')\n",
    "print(f\"Number of training objects: {result_df['in_training_data'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af83fd-f098-446c-baae-6434a88c1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906327a-8182-4c04-b513-883eedd00e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_na = result_df[result_df['ID_known'].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8ec53-c31c-47c9-a197-c959326fd054",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_na[non_na['ID_known'].str.startswith('1.24')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ef8dc-4936-47b7-a5d5-077f8d69ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = class_df[~class_df['known_id'].isin(result_df['ID_known'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce9b04-7a8b-4479-851b-483d2cd577c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_df[result_df['ID_known'].notna()]['ID_known'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9971d2f-e7e8-4934-a383-7808f8f43f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing[missing['known_id'].str.startswith('non_dwarf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1130227-250e-44cb-9564-93c451302f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['ID_known'] == '1234502+293314']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e37c39-28ac-4b7a-8fd1-471c5f7530e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['ID_known'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c09c8-b3d8-475c-bba2-00383ab15a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove objects that were used to train the model\n",
    "# result_no_train = result_df[(result_df['in_training_data'] == 0) & (result_df['lsb'].isna())].reset_index(drop=True)\n",
    "# save to file\n",
    "# result_no_train.to_parquet(os.path.join(table_dir, 'combined_no_train.parquet'), index=False)\n",
    "# remove rows with nan predictions\n",
    "result_df = result_df[~result_df['zoobot_pred_v2'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d7dcf-593d-4958-96fb-3ee442f592fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['zoobot_pred'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add674ea-40af-4693-b7a8-5ee0e8d15982",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(result_df['zoobot_pred_v2'], bins=np.arange(0, 1.05, 0.05), ec='black')\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(figures, 'prediction_dist_example.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1eff5-68b1-4dbd-986d-0ec90793154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(result_df['zoobot_pred'], bins=np.arange(0, 1.05, 0.05), ec='black')\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(figures, 'prediction_dist_example.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9f704-cb29-462e-8e81-b212d982c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print counts in 0.05 bins from 0 to 1\n",
    "counts = count_intervals(result_df['zoobot_pred_v2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5fe14-c1a2-49f6-826e-42737a712b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_string_matches(arr1, arr2):\n",
    "    valid_strings = set(arr2)\n",
    "\n",
    "    # Initialize counters\n",
    "    matches = 0\n",
    "    nones = 0\n",
    "    nans = 0\n",
    "    non_matches = 0\n",
    "    non_matches_list = []\n",
    "    matches_list = []\n",
    "\n",
    "    for x in arr1:\n",
    "        if x is None:\n",
    "            nones += 1\n",
    "        elif isinstance(x, float) and np.isnan(x):\n",
    "            nans += 1\n",
    "        elif x in valid_strings:\n",
    "            matches += 1\n",
    "            matches_list.append(x)\n",
    "        else:\n",
    "            non_matches += 1\n",
    "            non_matches_list.append(x)\n",
    "\n",
    "    print(f'Matches: {matches}')\n",
    "    print(f'None values: {nones}')\n",
    "    print(f'NaN values: {nans}')\n",
    "    print(f'Non-matching strings: {non_matches}')\n",
    "\n",
    "    return matches_list, non_matches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75c3d3-47bd-4b80-be23-21ab2b9eb6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches, non_matches = analyze_string_matches(result_df['ID_known'], class_df['known_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96359ad-5417-4f77-a325-1058d10d5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['ID_known'] == non_matches[16]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d202c-d13e-4aa7-a39f-aed58433f7d2",
   "metadata": {},
   "source": [
    "# Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245ee3b-bb39-4570-a4db-c262b79bacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, max_separation_arcsec=10.0, priority_column='zoobot_pred'):\n",
    "    \"\"\"\n",
    "    Remove duplicate entries from the DataFrame using Friends-of-Friends clustering.\n",
    "    Returns both deduplicated DataFrame and groups DataFrame for inspection.\n",
    "    \"\"\"\n",
    "    # Convert RA/Dec to SkyCoord\n",
    "    coords = SkyCoord(ra=df['ra'].values * u.degree, dec=df['dec'].values * u.degree)\n",
    "\n",
    "    # Convert to 3D Cartesian coordinates on the unit sphere\n",
    "    x = np.cos(coords.dec.radian) * np.cos(coords.ra.radian)\n",
    "    y = np.cos(coords.dec.radian) * np.sin(coords.ra.radian)\n",
    "    z = np.sin(coords.dec.radian)\n",
    "    points = np.column_stack((x, y, z))\n",
    "\n",
    "    # Compute maximum chord length for clustering\n",
    "    max_angle_rad = (max_separation_arcsec * u.arcsec).to(u.radian).value\n",
    "    max_chord_length = 2 * np.sin(max_angle_rad / 2)\n",
    "\n",
    "    # Build KD-tree for efficient neighbor search\n",
    "    tree = cKDTree(points)\n",
    "\n",
    "    # Find all pairs within the threshold distance\n",
    "    pairs = tree.query_pairs(max_chord_length, output_type='set')\n",
    "\n",
    "    # Union-Find to create clusters\n",
    "    parent = np.arange(len(df))\n",
    "\n",
    "    def find(node):\n",
    "        while parent[node] != node:\n",
    "            parent[node] = parent[parent[node]]  # Path compression\n",
    "            node = parent[node]\n",
    "        return node\n",
    "\n",
    "    for node1, node2 in pairs:\n",
    "        root1 = find(node1)\n",
    "        root2 = find(node2)\n",
    "        if root1 != root2:\n",
    "            parent[root2] = root1\n",
    "\n",
    "    # Assign cluster labels\n",
    "    cluster_labels = np.array([find(i) for i in range(len(parent))])\n",
    "    df = df.copy()\n",
    "    df['_cluster'] = cluster_labels\n",
    "\n",
    "    # Create groups information\n",
    "    groups_info = []\n",
    "    same_tile_duplicates = 0\n",
    "    overlap_duplicates = 0\n",
    "\n",
    "    # Process each cluster with more than one object\n",
    "    for cluster_id in tqdm(np.unique(cluster_labels), desc='Processing clusters'):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        if np.sum(cluster_mask) > 1:  # Only process groups with multiple objects\n",
    "            cluster_objects = df[cluster_mask]\n",
    "            best_idx = cluster_objects[priority_column].idxmax()\n",
    "\n",
    "            # Count duplicate types\n",
    "            unique_tiles = cluster_objects['tile'].unique()\n",
    "            if len(unique_tiles) == 1:\n",
    "                same_tile_duplicates += len(cluster_objects) - 1\n",
    "            else:\n",
    "                overlap_duplicates += len(cluster_objects) - 1\n",
    "\n",
    "            # Calculate separations from best object\n",
    "            best_coord = SkyCoord(\n",
    "                ra=df.loc[best_idx, 'ra'] * u.degree,\n",
    "                dec=df.loc[best_idx, 'dec'] * u.degree,\n",
    "            )\n",
    "\n",
    "            # Add information for each object in the cluster\n",
    "            for idx, row in cluster_objects.iterrows():\n",
    "                obj_coord = SkyCoord(ra=row['ra'] * u.degree, dec=row['dec'] * u.degree)\n",
    "                sep_to_best = obj_coord.separation(best_coord).arcsec\n",
    "\n",
    "                groups_info.append(\n",
    "                    {\n",
    "                        'cluster_id': cluster_id,\n",
    "                        'object_id': idx,\n",
    "                        'ra': row['ra'],\n",
    "                        'dec': row['dec'],\n",
    "                        'tile': row['tile'],\n",
    "                        'separation_to_best': sep_to_best,\n",
    "                        f'{priority_column}': row[priority_column],\n",
    "                        'is_best': idx == best_idx,\n",
    "                        'duplicate_type': ('same_tile' if len(unique_tiles) == 1 else 'overlap'),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Create groups DataFrame\n",
    "    groups_df = pd.DataFrame(groups_info)\n",
    "\n",
    "    # Print summary\n",
    "    total_duplicates = same_tile_duplicates + overlap_duplicates\n",
    "    print('\\nDuplicate Detection Summary:')\n",
    "    print(f'Total objects in input: {len(df)}')\n",
    "    print(f'Total duplicate objects found: {total_duplicates}')\n",
    "    print(f'  - Same tile duplicates: {same_tile_duplicates}')\n",
    "    print(f'  - Tile overlap duplicates: {overlap_duplicates}')\n",
    "    print(f'Objects after deduplication: {len(df) - total_duplicates}')\n",
    "\n",
    "    # Select best entry in each cluster\n",
    "    dedup_df = df.loc[df.groupby('_cluster')[priority_column].idxmax()]\n",
    "    dedup_df = dedup_df.drop(columns=['_cluster']).reset_index(drop=True)\n",
    "\n",
    "    return dedup_df, groups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832692f-a569-44d4-b6db-325536e1a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduped_catalog, group_cat = remove_duplicates(result_no_train, max_separation_arcsec=10.0)\n",
    "\n",
    "# save to file\n",
    "deduped_catalog.to_parquet(os.path.join(table_dir, 'combined_no_dups.parquet'), index=False)\n",
    "# group_cat.to_parquet(os.path.join(table_dir, 'duplicate_groups.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4d431-be14-430f-a044-413ce9034b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print counts in 0.05 bins from 0 to 1\n",
    "counts = count_intervals(deduped_catalog['zoobot_pred'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdccf73-62d4-4c3e-84f0-1b25c34ac170",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a5d37-7956-4feb-a6ed-875adb87ce19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First get unique cluster IDs where at least one object has high prediction score\n",
    "high_score_clusters = group_cat[group_cat['zoobot_pred'] > 0.8]['cluster_id'].unique()\n",
    "\n",
    "# Then look at all objects in these clusters\n",
    "for cluster_id in high_score_clusters[:20]:\n",
    "    group = group_cat[group_cat['cluster_id'] == cluster_id]\n",
    "    print(f'\\nCluster {cluster_id}:')\n",
    "    print('Number of objects in group:', len(group))\n",
    "    print(group[['ra', 'dec', 'tile', 'zoobot_pred', 'duplicate_type']])\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af1e0a-47c2-43da-9e02-5ce2bc77ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_known = deduped_catalog[\n",
    "    deduped_catalog['class_label'].isna() & deduped_catalog['zspec'].isna()\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f9218-c28a-4e0c-a88a-a086e1595f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702d1a8-ae2d-47c6-9d15-48d1b61b5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "not_known.to_parquet(os.path.join(table_dir, 'combined_unknown.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9a1b60-feca-4b1e-b0f2-d9e610e6670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_catalog[deduped_catalog['zoobot_pred'] > 0.7][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c261228-1d0e-4332-b438-9eed2e7dcab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.hist(not_known['zoobot_pred'], bins=np.arange(0, 1.05, 0.05), ec='black')\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Count')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(figures, 'prediction_dist_example.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb7b6a-3047-428c-9796-46b6f282e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print counts in 0.05 bins from 0 to 1\n",
    "counts = count_intervals(not_known['zoobot_pred'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ecf79a-499e-42aa-a411-ed2ef9093b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(counts[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0f339-1c89-4004-a083-887e005d259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_known[(not_known['zoobot_pred'] > 0.8) & (not_known['zoobot_pred'] < 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d6a4be-b7cb-4d70-80c6-eb09ed410eb6",
   "metadata": {},
   "source": [
    "# Remove duplicates based on effective radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5086cf6-af28-4178-8b50-ed9357243404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, radius_factor=1.5, min_separation=1.0, priority_column='zoobot_pred_v2'):\n",
    "    \"\"\"\n",
    "    Remove duplicates using KD-tree with effective radius-based filtering.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing astronomical objects\n",
    "    - radius_factor: Factor to multiply effective radius by for threshold\n",
    "    - min_separation: Minimum separation to use when r_eff is very small/nan\n",
    "    - priority_column: Column used to select which duplicate to keep\n",
    "\n",
    "    Returns:\n",
    "    - dedup_df: Deduplicated DataFrame\n",
    "    - groups_df: DataFrame containing information about duplicate groups\n",
    "    \"\"\"\n",
    "    # Calculate maximum effective radius for each object\n",
    "    df = df.copy()\n",
    "    df['max_r_eff'] = df.apply(\n",
    "        lambda row: max(\n",
    "            [\n",
    "                row['re_arcsec_cfis_lsb-r'],\n",
    "                row['re_arcsec_whigs-g'],\n",
    "                row['re_arcsec_ps-i'],\n",
    "            ],\n",
    "            default=min_separation,\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    df['max_r_eff'] = df['max_r_eff'].fillna(min_separation)\n",
    "\n",
    "    # Use maximum possible threshold for KD-tree initial search\n",
    "    max_possible_threshold = df['max_r_eff'].max() * radius_factor\n",
    "    print(f'Maximum search threshold: {max_possible_threshold:.2f} arcsec')\n",
    "\n",
    "    # Convert coordinates to 3D Cartesian\n",
    "    coords = SkyCoord(ra=df['ra'].values * u.degree, dec=df['dec'].values * u.degree)\n",
    "    x = np.cos(coords.dec.radian) * np.cos(coords.ra.radian)\n",
    "    y = np.cos(coords.dec.radian) * np.sin(coords.ra.radian)\n",
    "    z = np.sin(coords.dec.radian)\n",
    "    points = np.column_stack((x, y, z))\n",
    "\n",
    "    # Set up KD-tree with maximum possible threshold\n",
    "    max_angle_rad = (max_possible_threshold * u.arcsec).to(u.radian).value\n",
    "    max_chord_length = 2 * np.sin(max_angle_rad / 2)\n",
    "    tree = cKDTree(points)\n",
    "\n",
    "    # Get initial pairs from KD-tree\n",
    "    pairs = tree.query_pairs(max_chord_length, output_type='set')\n",
    "\n",
    "    # Vectorized filtering of pairs\n",
    "    filtered_pairs = set()\n",
    "    if pairs:  # Only process if pairs were found\n",
    "        pairs_array = np.array(list(pairs))\n",
    "        i, j = pairs_array.T\n",
    "        r_eff1 = df['max_r_eff'].values[i]\n",
    "        r_eff2 = df['max_r_eff'].values[j]\n",
    "        max_r_eff = np.maximum(r_eff1, r_eff2)\n",
    "        separations = coords[i].separation(coords[j]).arcsec\n",
    "\n",
    "        # Apply condition\n",
    "        mask = separations <= max_r_eff * radius_factor\n",
    "        filtered_pairs = set(map(tuple, pairs_array[mask]))\n",
    "\n",
    "        # Store separations for groups information\n",
    "        df['pair_separation'] = np.nan\n",
    "        df.loc[pairs_array[mask][:, 0], 'pair_separation'] = separations[mask]\n",
    "\n",
    "    # Efficient Union-Find implementation\n",
    "    parent = np.arange(len(df))\n",
    "\n",
    "    def find(node):\n",
    "        if parent[node] != node:\n",
    "            parent[node] = find(parent[node])  # Path compression\n",
    "        return parent[node]\n",
    "\n",
    "    # Group filtered pairs\n",
    "    for node1, node2 in filtered_pairs:\n",
    "        root1 = find(node1)\n",
    "        root2 = find(node2)\n",
    "        if root1 != root2:\n",
    "            parent[root2] = root1\n",
    "\n",
    "    # Assign cluster labels more efficiently\n",
    "    cluster_labels = np.array([find(i) for i in range(len(parent))])\n",
    "    df['_cluster'] = cluster_labels\n",
    "\n",
    "    # Pre-calculate unique clusters and their sizes\n",
    "    unique_clusters, cluster_counts = np.unique(cluster_labels, return_counts=True)\n",
    "    multi_object_clusters = unique_clusters[cluster_counts > 1]\n",
    "\n",
    "    # Initialize arrays for groups information\n",
    "    groups_info = []\n",
    "    same_tile_duplicates = 0\n",
    "    overlap_duplicates = 0\n",
    "\n",
    "    # Process clusters with multiple objects\n",
    "    for cluster_id in tqdm(multi_object_clusters, desc='Processing clusters'):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        cluster_objects = df[cluster_mask]\n",
    "        best_idx = cluster_objects[priority_column].idxmax()\n",
    "\n",
    "        # Count duplicate types\n",
    "        unique_tiles = cluster_objects['tile'].unique()\n",
    "        if len(unique_tiles) == 1:\n",
    "            same_tile_duplicates += len(cluster_objects) - 1\n",
    "        else:\n",
    "            overlap_duplicates += len(cluster_objects) - 1\n",
    "\n",
    "        # Vectorized separation calculation\n",
    "        best_coord = SkyCoord(\n",
    "            ra=df.loc[best_idx, 'ra'] * u.degree, dec=df.loc[best_idx, 'dec'] * u.degree\n",
    "        )\n",
    "\n",
    "        cluster_coords = SkyCoord(\n",
    "            ra=cluster_objects['ra'].values * u.degree,\n",
    "            dec=cluster_objects['dec'].values * u.degree,\n",
    "        )\n",
    "        separations = best_coord.separation(cluster_coords).arcsec\n",
    "\n",
    "        # Add information for each object in cluster\n",
    "        for (idx, row), sep in zip(cluster_objects.iterrows(), separations):\n",
    "            groups_info.append(\n",
    "                {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'object_id': idx,\n",
    "                    'ra': row['ra'],\n",
    "                    'dec': row['dec'],\n",
    "                    'tile': row['tile'],\n",
    "                    'max_r_eff': row['max_r_eff'],\n",
    "                    'separation_to_best': sep,\n",
    "                    f'{priority_column}': row[priority_column],\n",
    "                    'is_best': idx == best_idx,\n",
    "                    'duplicate_type': ('same_tile' if len(unique_tiles) == 1 else 'overlap'),\n",
    "                    'threshold_used': row['max_r_eff'] * radius_factor,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create groups DataFrame\n",
    "    groups_df = pd.DataFrame(groups_info)\n",
    "\n",
    "    # Calculate total duplicates\n",
    "    total_duplicates = same_tile_duplicates + overlap_duplicates\n",
    "\n",
    "    # Print summary\n",
    "    print('\\nDuplicate Detection Summary:')\n",
    "    print(f'Total objects in input: {len(df)}')\n",
    "    print(f'Total duplicate objects found: {total_duplicates}')\n",
    "    print(f'  - Same tile duplicates: {same_tile_duplicates}')\n",
    "    print(f'  - Tile overlap duplicates: {overlap_duplicates}')\n",
    "    print(f'Objects after deduplication: {len(df) - total_duplicates}')\n",
    "\n",
    "    # Select best entry in each cluster\n",
    "    dedup_df = df.loc[df.groupby('_cluster')[priority_column].idxmax()]\n",
    "    dedup_df = dedup_df.drop(columns=['_cluster', 'max_r_eff', 'pair_separation']).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    return dedup_df, groups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc80ae-790d-45bd-b165-3112d416157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df, groups_df = remove_duplicates(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b4699-f655-452c-b9c5-52e5d12858b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df[result_df['zoobot_pred_v2'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c6438-d595-4edc-9cf2-490fda93b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.hist(dedup_df['mu_whigs-g'], bins=np.arange(21, 30, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4940097-db19-411d-93ee-ec2d1594a4d7",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First get unique cluster IDs where at least one object has high prediction score\n",
    "groups_same_tile = groups_df[groups_df['duplicate_type'] == 'same_tile']\n",
    "high_score_clusters = groups_same_tile[groups_same_tile['zoobot_pred'] > 0.6]['cluster_id'].unique()\n",
    "\n",
    "# Then look at all objects in these clusters\n",
    "for cluster_id in high_score_clusters[:50]:\n",
    "    group = groups_same_tile[groups_same_tile['cluster_id'] == cluster_id]\n",
    "    print(f'\\nCluster {cluster_id}:')\n",
    "    print('Number of objects in group:', len(group))\n",
    "    print(group[['ra', 'dec', 'tile', 'zoobot_pred', 'duplicate_type']])\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4e443-ca30-4726-a4ab-a76261a19667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print counts in 0.05 bins from 0 to 1\n",
    "counts = count_intervals(dedup_df['zoobot_pred_v2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260bad35-5cba-4dcc-ba01-acd88732ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(counts[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c77cd8-d84a-42d7-b26c-80a0eecb0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_train_dedup = dedup_df[\n",
    "    (dedup_df['in_training_data'] == 0) & (dedup_df['lsb'].isna())\n",
    "].reset_index(drop=True)\n",
    "not_known = no_train_dedup[\n",
    "    no_train_dedup['class_label'].isna() & no_train_dedup['zspec'].isna()\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2fa3f7-3dc6-4121-a2e5-9913ee2f1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print counts in 0.05 bins from 0 to 1\n",
    "counts = count_intervals(dedup_df['zoobot_pred'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf6491-9ca7-4570-a5e4-21624b5a2b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(counts[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c9349-d481-4509-a200-d2bbf8b0ba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_filled.to_parquet(os.path.join(table_dir, 'unions_master_v3.parquet'), index=False)\n",
    "# groups_df.to_parquet(os.path.join(table_dir, 'duplicate_groups.parquet'), index=False)\n",
    "# not_known.to_parquet(os.path.join(table_dir, 'combined_unknown.parquet'), index=False)\n",
    "# no_train_dedup.to_parquet(os.path.join(table_dir, 'no_train_no_duplicates.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6da80-3083-4e79-aef7-5a62a7206cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dedup_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1490326-c073-4d52-9669-1dfd8066f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(dedup_filled['ID_known'].notna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7fcdb-c028-4df7-a27b-12f16c81c1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd74c847-24c5-4b10-b70f-0d41f07bfdfa",
   "metadata": {},
   "source": [
    "### Add missing IDs for dwarfs that were in multiple tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e567d3-0205-4246-a28f-71e5b76f4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_ids(\n",
    "    df_target: pd.DataFrame,\n",
    "    df_catalog: pd.DataFrame,\n",
    "    ra_col: str = 'ra',\n",
    "    dec_col: str = 'dec',\n",
    "    id_col_target: str = 'ID_known',\n",
    "    id_col_catalog: str = 'ID',\n",
    "    tolerance_arcsec: float = 10.0,\n",
    "    coord_frame: str = 'icrs',\n",
    "    verbose: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fills missing IDs in a target DataFrame by cross-matching with a catalog.\n",
    "\n",
    "    Matches based on celestial coordinates (RA, Dec) within a specified tolerance.\n",
    "    Modifies the target DataFrame in place.\n",
    "\n",
    "    Args:\n",
    "        df_target: DataFrame to fill missing IDs in. Assumed to have NaN/None\n",
    "                   for missing values in the target ID column.\n",
    "        df_catalog: Catalog DataFrame with coordinates and IDs to match against.\n",
    "        ra_col: Name of the Right Ascension column (in decimal degrees).\n",
    "        dec_col: Name of the Declination column (in decimal degrees).\n",
    "        id_col_target: Name of the ID column in df_target to check and fill.\n",
    "        id_col_catalog: Name of the ID column in df_catalog to use for filling.\n",
    "        tolerance_arcsec: Matching tolerance in arcseconds.\n",
    "        coord_frame: Astropy coordinate frame (e.g., 'icrs', 'galactic').\n",
    "        verbose: If True, print status messages.\n",
    "\n",
    "    Returns:\n",
    "        The modified df_target DataFrame (modified in place).\n",
    "    \"\"\"\n",
    "    # 1. Identify rows in the target DataFrame with missing IDs\n",
    "    df_target = df_target.copy()\n",
    "    missing_mask = df_target[id_col_target].isna()\n",
    "    df_target_missing = df_target.loc[missing_mask]\n",
    "\n",
    "    if df_target_missing.empty:\n",
    "        if verbose:\n",
    "            print('No missing IDs found in the target DataFrame.')\n",
    "        return df_target\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Found {len(df_target_missing)} rows with missing IDs.')\n",
    "\n",
    "    # 2. Create Astropy SkyCoord objects\n",
    "    try:\n",
    "        coords_target = SkyCoord(\n",
    "            ra=df_target_missing[ra_col].values * u.degree,\n",
    "            dec=df_target_missing[dec_col].values * u.degree,\n",
    "            frame=coord_frame,\n",
    "        )\n",
    "        coords_catalog = SkyCoord(\n",
    "            ra=df_catalog[ra_col].values * u.degree,\n",
    "            dec=df_catalog[dec_col].values * u.degree,\n",
    "            frame=coord_frame,\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        print(f'Error: Column not found - {e}. Check column names.')\n",
    "        return df_target  # Return unmodified df on error\n",
    "    except Exception as e:\n",
    "        print(f'Error creating SkyCoord objects: {e}')\n",
    "        return df_target\n",
    "\n",
    "    # 3. Perform cross-matching\n",
    "    # idx: index *in coords_catalog* of the nearest neighbor\n",
    "    # sep2d: on-sky separation\n",
    "    idx, sep2d, _ = match_coordinates_sky(coords_target, coords_catalog)\n",
    "\n",
    "    # 4. Filter matches by tolerance\n",
    "    tolerance = tolerance_arcsec * u.arcsec\n",
    "    valid_match_mask = sep2d <= tolerance\n",
    "\n",
    "    # 5. Get indices and IDs for update\n",
    "    # Indices in the *original* df_target corresponding to successful matches\n",
    "    df_target_indices_to_update = df_target_missing.index[valid_match_mask]\n",
    "    # Indices in df_catalog for the matched objects\n",
    "    df_catalog_indices_for_ids = idx[valid_match_mask]\n",
    "\n",
    "    if len(df_target_indices_to_update) > 0:\n",
    "        # Get the actual IDs from the catalog to fill in\n",
    "        ids_to_fill = df_catalog.loc[df_catalog_indices_for_ids, id_col_catalog].values\n",
    "\n",
    "        # 6. Update df_target (in place) using .loc\n",
    "        df_target.loc[df_target_indices_to_update, id_col_target] = ids_to_fill\n",
    "        if verbose:\n",
    "            print(f'Successfully updated {len(df_target_indices_to_update)} IDs.')\n",
    "    elif verbose:\n",
    "        print(f'No matches found within the {tolerance_arcsec} arcsec tolerance.')\n",
    "\n",
    "    return df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab6b93-603d-4b52-bc32-94baef37ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_filled = fill_missing_ids(df_target=dedup_df, df_catalog=all_dwarfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546c1e5-ffbc-472b-9e17-fdc8c8fb0c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(dedup_df['ID_known'].notna()), np.count_nonzero(dedup_filled['ID_known'].notna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13c6912-73d6-4b48-9bae-59d29ee36295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_filled[dedup_filled['ID_known'].notna()][100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360b0ad-8074-4e52-83d0-a9a4409923db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df[['ID_cfis_lsb-r', 'ID_whigs-g', 'ID_ps-i']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23ac5b-ae01-49c8-8904-78c821e82d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_detections = 0\n",
    "\n",
    "for row in dedup_df[['ID_cfis_lsb-r', 'ID_whigs-g', 'ID_ps-i']].iterrows():\n",
    "    row.count()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58c8027-3aa8-4f63-910c-ca3bcc6c79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df.iloc[:].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b5364-3355-443a-bfe3-bdc9811eda75",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_id_cols = ['ID_cfis_lsb-r', 'ID_whigs-g', 'ID_ps-i']\n",
    "\n",
    "# 1. Check for non-NaN values in the specified columns\n",
    "detections_bool = dedup_df[band_id_cols].notna()\n",
    "# print(\"\\n--- Detection Boolean Mask ---\")\n",
    "# print(detections_bool)\n",
    "\n",
    "# 2. Sum the boolean values row-wise (True=1, False=0) to count detections per row\n",
    "detections_per_row = detections_bool.sum(axis=1)\n",
    "# print(\"\\n--- Detections Per Row ---\")\n",
    "# print(detections_per_row)\n",
    "\n",
    "# 3. Count how many rows have exactly 1 detection\n",
    "count_single_band_detections = (detections_per_row == 1).sum()\n",
    "\n",
    "print(f'\\nNumber of rows with detection in exactly one band: {count_single_band_detections}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fd4db-a5f6-4520-b018-7b39c9bcd42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_filled[dedup_filled['ID_known'] == '121853+654443']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d898302-582d-44ab-80f0-0949af5edfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
