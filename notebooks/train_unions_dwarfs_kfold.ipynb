{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja7Fm81_vCHM"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!pip install pytorch-lightning\n",
        "!pip install timm\n",
        "!pip install pyro-ppl\n",
        "!pip install zoobot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzTZ8Pibe4J_"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "from google.colab import drive, userdata\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO, format='%(asctime)s - %(levelname)s - [%(name)s] - %(message)s', force=True\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import h5py\n",
        "import huggingface_hub\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import timm\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import wandb\n",
        "from IPython.display import clear_output\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from zoobot.pytorch.training.finetune import FinetuneableZoobotClassifier\n",
        "\n",
        "### Color blind palette ###\n",
        "colors = {\n",
        "    'blue': '#377eb8',\n",
        "    'orange': '#ff7f00',\n",
        "    'green': '#4daf4a',\n",
        "    'pink': '#f781bf',\n",
        "    'brown': '#a65628',\n",
        "    'purple': '#984ea3',\n",
        "    'gray': '#999999',\n",
        "    'red': '#e41a1c',\n",
        "    'yellow': '#dede00',\n",
        "}\n",
        "\n",
        "SEED = 42\n",
        "pl.seed_everything(SEED, workers=True)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DTYPE = torch.float32\n",
        "EPSILON = 1e-8\n",
        "\n",
        "# print(f\"Current device: {torch.cuda.current_device()}\")  # Shows index of current device\n",
        "# print(f\"Device name: {torch.cuda.get_device_name()}\")    # Shows name of the GPU\n",
        "# print(f\"Available devices: {torch.cuda.device_count()}\") # Shows number of available GPUs\n",
        "\n",
        "# mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bS35S2ELrv5"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgxSNlutwf6F"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path):\n",
        "    \"\"\"\n",
        "    Loads data from an HDF5 file.\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to the HDF5 file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (data, label)\n",
        "    \"\"\"\n",
        "    # Load the Data\n",
        "    with h5py.File(data_path, 'r') as f:\n",
        "        # You can now access the data within the HDF5 file using the variable `f`\n",
        "        label = f['label'][...].astype(np.float32)\n",
        "        data = np.nan_to_num(f['images'][...], nan=0.0).astype(np.float32)\n",
        "        obj_id = f['known_id'][...].astype(str)\n",
        "    logger.info(f'Data loaded. Image shape: {data.shape}, Label shape: {label.shape}')\n",
        "    return data, label, obj_id\n",
        "\n",
        "\n",
        "def init_wandb(key_name='WANDB_API_KEY'):\n",
        "    try:\n",
        "        api_key = userdata.get(key_name)\n",
        "        if not api_key:\n",
        "            logger.error('WANDB_API_KEY not found in Colab Secrets. Please add it.')\n",
        "            # Handle error - maybe raise an exception or skip W&B logging\n",
        "            wandb_enabled = False\n",
        "        else:\n",
        "            wandb.login(key=api_key)\n",
        "            logger.info('Successfully logged into W&B.')\n",
        "            wandb_enabled = True\n",
        "            # Finish any lingering runs from previous executions in the same session\n",
        "            if wandb.run is not None:\n",
        "                logger.warning('Detected an existing W&B run. Finishing it...')\n",
        "                wandb.finish()\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error during W&B login: {e}')\n",
        "        wandb_enabled = False\n",
        "    return wandb_enabled\n",
        "\n",
        "\n",
        "def init_hf():\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    try:\n",
        "        logger.info('Attempting Hugging Face Hub login...')\n",
        "        huggingface_hub.login(token=hf_token)\n",
        "        logger.info('Hugging Face Hub login successful.')\n",
        "    except Exception as e:\n",
        "        logger.error(f'Error during Hugging Face Hub login: {e}')\n",
        "\n",
        "\n",
        "def prepare_kfold_splits(labels, test_size=0.1, n_splits=10, num_strat_bins=5, seed=42):\n",
        "    \"\"\"Performs initial test split and sets up K-Fold generator.\"\"\"\n",
        "    num_samples = len(labels)\n",
        "    all_indices = np.arange(num_samples)\n",
        "\n",
        "    # Bin labels for stratification\n",
        "    bin_edges = np.linspace(0, 1, num_strat_bins + 1)\n",
        "    binned_labels = np.digitize(labels, bins=bin_edges[1:-1])\n",
        "    logger.info(f'Labels binned into {num_strat_bins} bins for stratification.')\n",
        "    logger.info(f'Bin counts (overall): {np.bincount(binned_labels, minlength=num_strat_bins)}')\n",
        "\n",
        "    # Initial Train/Val vs. Test split\n",
        "    train_val_indices, test_indices = train_test_split(\n",
        "        all_indices, test_size=test_size, random_state=seed, stratify=binned_labels\n",
        "    )\n",
        "    binned_labels_train_val = binned_labels[train_val_indices]\n",
        "    logger.info(f'Initial split: Train/Val={len(train_val_indices)}, Test={len(test_indices)}')\n",
        "    logger.info(\n",
        "        f'Bin counts (Test): {np.bincount(binned_labels[test_indices], minlength=num_strat_bins)}'\n",
        "    )\n",
        "\n",
        "    # K-Fold setup for the Train/Validation set\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    logger.info(f'StratifiedKFold initialized with {n_splits} splits.')\n",
        "\n",
        "    # Return the KFold generator, the train/val indices, their binned labels, and test indices\n",
        "    return (\n",
        "        skf.split(np.zeros(len(train_val_indices)), binned_labels_train_val),\n",
        "        train_val_indices,\n",
        "        test_indices,\n",
        "    )\n",
        "\n",
        "\n",
        "def create_weighted_sampler(train_labels, num_weighting_bins=10, epsilon=1e-6, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Creates a WeightedRandomSampler for handling imbalance in continuous labels\n",
        "    by weighting samples based on their label bin frequency.\n",
        "\n",
        "    Args:\n",
        "        train_labels (np.ndarray): Continuous labels for the training set only.\n",
        "        num_weighting_bins (int): Number of bins to use for calculating weights.\n",
        "        epsilon (float): Small value to add to bin counts to avoid division by zero.\n",
        "        alpha (float): Dampening exponent (0 < alpha <= 1). 1=full inverse, <1 less aggressive.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.data.WeightedRandomSampler: Sampler instance for the training DataLoader.\n",
        "    \"\"\"\n",
        "    num_train_samples = len(train_labels)\n",
        "\n",
        "    # Calculate histogram of training labels\n",
        "    hist, bin_edges = np.histogram(train_labels, bins=num_weighting_bins, range=(0, 1))\n",
        "\n",
        "    # Calculate weight per bin (inverse frequency)\n",
        "    counts_dampened = np.power(hist + epsilon, alpha)\n",
        "    weights_per_bin = 1.0 / counts_dampened\n",
        "    weights_per_bin = weights_per_bin / np.sum(weights_per_bin)  # normalize\n",
        "    logger.info(f'Sampler bin weights: {weights_per_bin}')\n",
        "\n",
        "    # Find which bin each training label belongs to\n",
        "    # Ensure labels exactly equal to 1.0 fall into the last bin correctly\n",
        "    # Using right=True includes the right edge, then clip ensures index stays within bounds\n",
        "    bin_indices_train = np.digitize(train_labels, bin_edges[1:], right=True)\n",
        "\n",
        "    # Assign weight to each sample based on its bin\n",
        "    sample_weights = weights_per_bin[bin_indices_train]\n",
        "\n",
        "    # Convert weights to a torch Tensor\n",
        "    train_sample_weights_tensor = torch.DoubleTensor(\n",
        "        sample_weights\n",
        "    )  # PyTorch expects DoubleTensor for weights\n",
        "\n",
        "    # Create the sampler\n",
        "    train_sampler = WeightedRandomSampler(\n",
        "        weights=train_sample_weights_tensor, num_samples=num_train_samples, replacement=True\n",
        "    )\n",
        "    return train_sampler\n",
        "\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, inputs, labels, transform=None, preprocess=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs (list or ndarray): The input features.\n",
        "            labels (list or ndarray): The labels corresponding to the inputs.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "            preprocess (callable, optional): Optional preprocessing to be applied on a sample.\n",
        "        \"\"\"\n",
        "\n",
        "        # If there's a preprocessing function, apply it\n",
        "        if preprocess is not None:\n",
        "            inputs = preprocess(inputs)\n",
        "\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.tensor(self.inputs[idx], dtype=DTYPE)\n",
        "        label = torch.tensor(self.labels[idx], dtype=DTYPE).squeeze()  # Ensure labels are 1D\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "### Lightning Module ###\n",
        "class ZooBot_lightning(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        zoobot_size,\n",
        "        zoobot_blocks,\n",
        "        learning_rate,\n",
        "        learning_decay,\n",
        "        weight_decay,\n",
        "        label_smoothing,\n",
        "        loss_type='kld',\n",
        "        focal_gamma=2.0,\n",
        "    ):\n",
        "        super(ZooBot_lightning, self).__init__()\n",
        "        self.save_hyperparameters()  # Saves all arguments for checkpointing\n",
        "\n",
        "        if self.hparams.loss_type not in ['focal', 'kld']:\n",
        "            raise ValueError(\n",
        "                f\"Invalid loss_type: {self.hparams.loss_type}. Choose 'focal' or 'kld'.\"\n",
        "            )\n",
        "\n",
        "        # Define the model\n",
        "        self.model = FinetuneableZoobotClassifier(\n",
        "            name=f'hf_hub:mwalmsley/zoobot-encoder-convnext_{zoobot_size}',\n",
        "            n_blocks=zoobot_blocks,  # Finetune this many blocks.x\n",
        "            learning_rate=learning_rate,  # use a low learning rate\n",
        "            lr_decay=learning_decay,  # reduce the learning rate from lr to lr^0.5 for deeper blocks\n",
        "            num_classes=2,  # Number of output classes\n",
        "        )\n",
        "\n",
        "        self.train_step_outputs = []\n",
        "        self.valid_step_outputs = []\n",
        "\n",
        "        self.validation_outputs = []\n",
        "        self.validation_targets = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    def _compute_focal_loss(self, probabilities, targets):\n",
        "        \"\"\"\n",
        "        Computes the Focal loss.\n",
        "        Args:\n",
        "            probabilities: Predicted probabilities (shape: [batch_size, 2]).\n",
        "            targets: True labels (shape: [batch_size]).\n",
        "        Returns:\n",
        "            Mean Focal loss for the batch.\n",
        "        \"\"\"\n",
        "        gamma = self.hparams.focal_gamma\n",
        "        p_pred = probabilities[:, 1]\n",
        "        p_pred_clamped = p_pred.clamp(min=EPSILON, max=1.0 - EPSILON)\n",
        "        loss_positive = -targets * torch.pow(1.0 - p_pred, gamma) * torch.log(p_pred_clamped)\n",
        "        loss_negative = (\n",
        "            -(1.0 - targets) * torch.pow(p_pred, gamma) * torch.log(1.0 - p_pred_clamped)\n",
        "        )\n",
        "        loss_unreduced = loss_positive + loss_negative\n",
        "        return loss_unreduced.mean()\n",
        "\n",
        "    def _compute_kld_loss(self, logits, targets):\n",
        "        \"\"\"\n",
        "        Computes the KL Divergence loss.\n",
        "        Args:\n",
        "            logits: Raw logits output from the model (shape: [batch_size, 2]).\n",
        "            targets: Soft labels (probabilities for positive class) (shape: [batch_size]).\n",
        "        Returns:\n",
        "            Mean KL divergence loss for the batch.\n",
        "        \"\"\"\n",
        "        # Create the target probability distribution [P(class 0), P(class 1)]\n",
        "        target_dist = torch.stack([1.0 - targets, targets], dim=1)\n",
        "\n",
        "        # Calculate log probabilities from logits\n",
        "        log_probabilities = F.log_softmax(logits, dim=1)\n",
        "\n",
        "        # Calculate KL divergence\n",
        "        # Use log_target=False because target_dist contains probabilities, not log-probabilities\n",
        "        loss = F.kl_div(log_probabilities, target_dist, reduction='batchmean', log_target=False)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "\n",
        "        p_true = self.apply_label_smoothing(labels, alpha=self.hparams.label_smoothing)\n",
        "\n",
        "        # Get model outputs\n",
        "        logits = self(images)  # Logits shape [batch, 2]\n",
        "\n",
        "        if self.hparams.loss_type == 'focal':\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            loss = self._compute_focal_loss(probabilities, p_true)\n",
        "        elif self.hparams.loss_type == 'kld':\n",
        "            loss = self._compute_kld_loss(logits, p_true)\n",
        "        else:\n",
        "            # This should not happen due to check in __init__ but good practice\n",
        "            raise ValueError(f'Invalid loss_type specified: {self.hparams.loss_type}')\n",
        "\n",
        "        self.train_step_outputs.append(loss)\n",
        "        return loss\n",
        "\n",
        "    def apply_label_smoothing(self, labels, alpha=None):\n",
        "        \"\"\"Apply label smoothing with strength alpha.\"\"\"\n",
        "        # Move probabilities slightly toward 0.5\n",
        "        if alpha is None or alpha == 0.0:\n",
        "            return labels\n",
        "        else:\n",
        "            return labels * (1 - alpha) + 0.5 * alpha\n",
        "\n",
        "    def expected_calibration_error(self, preds, soft_labels, n_bins=5):\n",
        "        \"\"\"\n",
        "        Calculate ECE for soft labels.\n",
        "\n",
        "        Args:\n",
        "            preds: Predicted probabilities (tensor)\n",
        "            soft_labels: Soft label probabilities (tensor)\n",
        "            n_bins: Number of bins to use\n",
        "\n",
        "        Returns:\n",
        "            ece: The Expected Calibration Error\n",
        "            bin_data: Dictionary with detailed bin information\n",
        "        \"\"\"\n",
        "        # Convert to numpy for easier manipulation\n",
        "        preds_np = preds.detach().cpu().numpy()\n",
        "        labels_np = soft_labels.detach().cpu().numpy()\n",
        "\n",
        "        # Create equal-width bins across prediction range\n",
        "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "        bin_indices = np.digitize(preds_np, bin_boundaries) - 1\n",
        "        bin_indices = np.clip(bin_indices, 0, n_bins - 1)  # Handle edge cases\n",
        "\n",
        "        # Initialize arrays for bin statistics\n",
        "        bin_sizes = np.zeros(n_bins)\n",
        "        bin_avg_preds = np.zeros(n_bins)\n",
        "        bin_avg_labels = np.zeros(n_bins)\n",
        "\n",
        "        # Calculate statistics for each bin\n",
        "        for bin_idx in range(n_bins):\n",
        "            mask = bin_indices == bin_idx\n",
        "            bin_sizes[bin_idx] = mask.sum()\n",
        "\n",
        "            if bin_sizes[bin_idx] > 0:\n",
        "                bin_avg_preds[bin_idx] = preds_np[mask].mean()\n",
        "                bin_avg_labels[bin_idx] = labels_np[mask].mean()\n",
        "\n",
        "        # Calculate ECE with proper weighting\n",
        "        total_samples = len(preds_np)\n",
        "        ece = np.sum((bin_sizes / total_samples) * np.abs(bin_avg_preds - bin_avg_labels))\n",
        "\n",
        "        # Return ECE and bin data for visualizations\n",
        "        bin_data = {\n",
        "            'sizes': bin_sizes,\n",
        "            'avg_preds': bin_avg_preds,\n",
        "            'avg_labels': bin_avg_labels,\n",
        "            'boundaries': bin_boundaries,\n",
        "        }\n",
        "\n",
        "        return ece, bin_data\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        logits = self(images)\n",
        "\n",
        "        # Calculate probabilities\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        positive_class_probs = probabilities[:, 1]  # Dwarf probability\n",
        "\n",
        "        # 1. Calculate the loss\n",
        "        if self.hparams.loss_type == 'focal':\n",
        "            # Use original labels (not smoothed) for validation loss\n",
        "            val_loss = self._compute_focal_loss(probabilities, labels)\n",
        "        elif self.hparams.loss_type == 'kld':\n",
        "            # Use original labels (not smoothed) for validation loss\n",
        "            val_loss = self._compute_kld_loss(logits, labels)\n",
        "        else:\n",
        "            raise ValueError(f'Invalid loss_type specified: {self.hparams.loss_type}')\n",
        "\n",
        "        # 2. Brier Score (MSE)\n",
        "        brier_score = F.mse_loss(positive_class_probs, labels)\n",
        "\n",
        "        # 3. Calculate soft-label ECE\n",
        "        ece, bin_data = self.expected_calibration_error(positive_class_probs, labels)\n",
        "\n",
        "        # Store bin data for visualization\n",
        "        if batch_idx == 0:  # Only store once per epoch\n",
        "            self.bin_data = bin_data\n",
        "\n",
        "        # Store outputs and labels for visualization\n",
        "        self.validation_outputs.append(positive_class_probs.detach().cpu().numpy())\n",
        "        self.validation_targets.append(labels.detach().cpu().numpy())\n",
        "\n",
        "        # Store the main loss for epoch end calculations\n",
        "        self.valid_step_outputs.append(val_loss.item())\n",
        "\n",
        "        # Log all metrics\n",
        "        self.log_dict(\n",
        "            {\n",
        "                'valid_loss': val_loss,\n",
        "                'val_brier': brier_score,\n",
        "                'val_calibration': ece,\n",
        "            },\n",
        "            prog_bar=True,\n",
        "        )\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        # train_loss = sum(self.train_step_outputs) / len(self.train_step_outputs)\n",
        "        train_loss = torch.stack(self.train_step_outputs).mean()\n",
        "        self.log('train_loss', train_loss)\n",
        "        self.train_step_outputs.clear()\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        valid_loss = sum(self.valid_step_outputs) / len(self.valid_step_outputs)\n",
        "        self.log('valid_loss', valid_loss)\n",
        "        self.valid_step_outputs.clear()\n",
        "\n",
        "        # Concatenate stored outputs and targets\n",
        "        outputs = np.concatenate(self.validation_outputs)\n",
        "        targets = np.concatenate(self.validation_targets)\n",
        "\n",
        "        # 1. Predictions vs True Values\n",
        "        fig1 = plt.figure(figsize=(6, 6))\n",
        "        h = plt.hist2d(\n",
        "            targets, outputs, bins=[25, 25], range=[[0, 1], [0, 1]], cmap='viridis', norm='log'\n",
        "        )\n",
        "        plt.plot([0, 1], [0, 1], c='r', linestyle='--')\n",
        "        plt.axhline(targets.mean(), linestyle='--', color='b')\n",
        "        plt.colorbar(h[3])\n",
        "        plt.ylabel('Model Predictions')\n",
        "        plt.xlabel('Expert Classifications')\n",
        "        plt.title('Prediction vs Truth Distribution')\n",
        "        plt.tight_layout()\n",
        "        self.logger.experiment.log({'Prediction Distribution': wandb.Image(fig1)})\n",
        "        plt.close(fig1)\n",
        "\n",
        "        # 2. Reliability (Calibration) Curve\n",
        "        n_bins = 5  # Number of bins for reliability plot\n",
        "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "\n",
        "        # Bin data based on PREDICTED probabilities (outputs)\n",
        "        bin_indices = np.digitize(\n",
        "            outputs, bin_edges[1:], right=False\n",
        "        )  # bin_edges[1:] -> bins [0, n_bins-1]\n",
        "\n",
        "        mean_predicted = np.zeros(n_bins)\n",
        "        mean_true = np.zeros(n_bins)\n",
        "        counts = np.zeros(n_bins, dtype=int)\n",
        "\n",
        "        for i in range(n_bins):\n",
        "            bin_mask = bin_indices == i\n",
        "            counts[i] = np.sum(bin_mask)\n",
        "            if counts[i] > 0:\n",
        "                mean_predicted[i] = outputs[bin_mask].mean()\n",
        "                mean_true[i] = targets[bin_mask].mean()\n",
        "            # else: means remain 0, counts remain 0\n",
        "\n",
        "        # Filter out bins with zero counts to avoid plotting artifacts\n",
        "        valid_bins_mask = counts > 0\n",
        "        mean_predicted_valid = mean_predicted[valid_bins_mask]\n",
        "        mean_true_valid = mean_true[valid_bins_mask]\n",
        "        counts_valid = counts[valid_bins_mask]\n",
        "\n",
        "        # Create the plot\n",
        "        fig2 = plt.figure(figsize=(6, 6))\n",
        "        plt.plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')  # Diagonal line\n",
        "\n",
        "        if len(mean_predicted_valid) > 0:  # Only plot if there's valid data\n",
        "            # Plot with conventional axes: Prediction (Confidence) on X, Truth (Accuracy) on Y\n",
        "            plt.plot(mean_predicted_valid, mean_true_valid, 'bo-', label='Model Calibration')\n",
        "\n",
        "            # Add sample count annotations\n",
        "            for i in range(len(mean_predicted_valid)):\n",
        "                # Adjust text position slightly for clarity\n",
        "                plt.text(\n",
        "                    mean_predicted_valid[i],\n",
        "                    mean_true_valid[i] + 0.02,\n",
        "                    f'n={counts_valid[i]}',\n",
        "                    ha='center',\n",
        "                    va='bottom',\n",
        "                    fontsize=8,\n",
        "                )\n",
        "        else:\n",
        "            plt.text(0.5, 0.5, 'No data in validation bins', ha='center', va='center')\n",
        "\n",
        "        plt.xlabel('Mean Predicted Probability (Confidence)')  # Conventional X-axis label\n",
        "        plt.ylabel('Mean True Label (Accuracy)')  # Conventional Y-axis label\n",
        "        plt.title('Reliability Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.xlim([-0.05, 1.05])  # Add padding\n",
        "        plt.ylim([-0.05, 1.05])\n",
        "        plt.tight_layout()\n",
        "        self.logger.experiment.log({'Reliability Curve': wandb.Image(fig2)})\n",
        "        plt.close(fig2)\n",
        "\n",
        "        if hasattr(self, 'bin_data') and self.bin_data:\n",
        "            avg_preds = self.bin_data['avg_preds']\n",
        "            avg_labels = self.bin_data['avg_labels']\n",
        "            sizes = self.bin_data['sizes']\n",
        "            n_bins_bar = len(avg_preds)  # Get number of bins from data\n",
        "\n",
        "            fig3 = plt.figure(figsize=(7, 6))  # Slightly wider for annotations\n",
        "            bar_width = 0.35\n",
        "            bin_indices_bar = np.arange(n_bins_bar)\n",
        "\n",
        "            # Bars for average true labels\n",
        "            plt.bar(\n",
        "                bin_indices_bar - bar_width / 2,\n",
        "                avg_labels,\n",
        "                bar_width,\n",
        "                label='Avg Label (Truth)',\n",
        "                color=colors['blue'],\n",
        "            )\n",
        "            # Bars for average predictions\n",
        "            plt.bar(\n",
        "                bin_indices_bar + bar_width / 2,\n",
        "                avg_preds,\n",
        "                bar_width,\n",
        "                label='Avg Prediction',\n",
        "                color=colors['orange'],\n",
        "            )\n",
        "\n",
        "            # Add sample count annotations above bars\n",
        "            for i in range(n_bins_bar):\n",
        "                if sizes[i] > 0:  # Only annotate if count > 0\n",
        "                    # Position text above the taller bar\n",
        "                    y_pos = max(avg_labels[i], avg_preds[i]) + 0.02\n",
        "                    plt.text(\n",
        "                        bin_indices_bar[i],\n",
        "                        y_pos,\n",
        "                        f'n={int(sizes[i])}',\n",
        "                        ha='center',\n",
        "                        va='bottom',\n",
        "                        fontsize=8,\n",
        "                    )\n",
        "\n",
        "            # Add ideal calibration line (y=x equivalent for bins)\n",
        "            # Get bin boundaries for x-axis labels\n",
        "            bin_boundaries = self.bin_data.get('boundaries', np.linspace(0, 1, n_bins_bar + 1))\n",
        "            bin_centers_approx = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n",
        "            plt.plot(\n",
        "                bin_indices_bar, bin_centers_approx, 'r--', label='Perfect Calibration'\n",
        "            )  # Plot against bin index\n",
        "\n",
        "            # Get ECE value from logged metrics if available\n",
        "            ece_metric_name = 'val_calibration'  # Make sure this matches the name used in validation_step log_dict\n",
        "            ece_value = self.trainer.callback_metrics.get(ece_metric_name, None)\n",
        "            title = 'Calibration Bar Chart'\n",
        "            if ece_value is not None:\n",
        "                title += f' (ECE = {float(ece_value):.4f})'  # Ensure ECE is float\n",
        "\n",
        "            plt.xlabel('Prediction Bin Index')\n",
        "            # Use bin boundaries for clearer x-axis ticks\n",
        "            tick_labels = [\n",
        "                f'{bin_boundaries[i]:.1f}-{bin_boundaries[i+1]:.1f}' for i in range(n_bins_bar)\n",
        "            ]\n",
        "            plt.xticks(ticks=bin_indices_bar, labels=tick_labels, rotation=45, ha='right')\n",
        "            plt.ylabel('Probability')\n",
        "            plt.ylim([-0.05, 1.05])\n",
        "            plt.title(title)\n",
        "            plt.legend()\n",
        "            plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
        "            plt.tight_layout()\n",
        "            self.logger.experiment.log({'Calibration': wandb.Image(fig3)})\n",
        "            plt.close(fig3)\n",
        "\n",
        "        # 3. Error Distribution\n",
        "        fig4 = plt.figure(figsize=(6, 6))\n",
        "        errors = outputs - targets\n",
        "        plt.hist(errors, bins=50, density=True)\n",
        "        plt.xlabel('Prediction Error')\n",
        "        plt.ylabel('Density')\n",
        "        plt.title('Error Distribution')\n",
        "        plt.tight_layout()\n",
        "        self.logger.experiment.log({'Error Distribution': wandb.Image(fig4)})\n",
        "        plt.close(fig4)\n",
        "\n",
        "        # Clear stored data\n",
        "        self.validation_outputs.clear()\n",
        "        self.validation_targets.clear()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configures the optimizer (AdamW) and LR scheduler (ReduceLROnPlateau).\n",
        "\n",
        "        Selects parameters for optimization based on self.hparams.zoobot_blocks,\n",
        "        applying learning rate decay (self.hparams.learning_decay) to deeper blocks,\n",
        "        mimicking the logic from FinetuneableZoobotAbstract.\n",
        "        \"\"\"\n",
        "        # Retrieve hyperparameters\n",
        "        try:\n",
        "            lr = self.hparams.learning_rate\n",
        "            lr_decay_factor = self.hparams.learning_decay\n",
        "            num_blocks_to_tune = self.hparams.zoobot_blocks\n",
        "            # Use weight_decay from hparams\n",
        "            weight_decay = self.hparams.weight_decay\n",
        "        except AttributeError as e:\n",
        "            logger.error(\n",
        "                f'Optimizer config failed: Missing hyperparameter. Ensure learning_rate, learning_decay, zoobot_blocks, (and optionally weight_decay) are saved. Error: {e}'\n",
        "            )\n",
        "            raise\n",
        "\n",
        "        # Start parameter groups: always include the head (no LR decay)\n",
        "        # Ensure self.model.head exists (it should be created by FinetuneableZoobotClassifier)\n",
        "        if not hasattr(self.model, 'head'):\n",
        "            raise AttributeError(\n",
        "                \"self.model does not have a 'head' attribute. Ensure FinetuneableZoobotClassifier initialization was successful.\"\n",
        "            )\n",
        "\n",
        "        params_to_optimize = [{'params': self.model.head.parameters(), 'lr': lr}]\n",
        "        logger.info(f'Opt: Initializing Optimizer. Base LR: {lr}, Weight Decay: {weight_decay}')\n",
        "        logger.info(f'Opt: Head parameters included with LR {lr}')\n",
        "\n",
        "        if num_blocks_to_tune > 0:\n",
        "            logger.info(f'Opt: Fine-tuning last {num_blocks_to_tune} encoder blocks/stages.')\n",
        "            logger.info(f'Opt: Encoder architecture: {type(self.model.encoder).__name__}')\n",
        "\n",
        "            # --- Parameter Group Selection Logic (adapted from FinetuneableZoobotAbstract) ---\n",
        "            if isinstance(self.model.encoder, timm.models.ConvNeXt):\n",
        "                # For ConvNeXt: stem + 4 stages\n",
        "                tuneable_blocks_or_stages = [self.model.encoder.stem] + list(\n",
        "                    self.model.encoder.stages\n",
        "                )\n",
        "                logger.info(\n",
        "                    f'Opt: Identified {len(tuneable_blocks_or_stages)} tuneable blocks/stages for ConvNeXt (stem + stages).'\n",
        "                )\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f'Opt: Encoder architecture {type(self.model.encoder).__name__} not explicitly handled in custom configure_optimizers.'\n",
        "                )\n",
        "\n",
        "            if num_blocks_to_tune > len(tuneable_blocks_or_stages):\n",
        "                logger.info(\n",
        "                    f'Opt: Requested {num_blocks_to_tune} blocks, but only {len(tuneable_blocks_or_stages)} available. Tuning all available.'\n",
        "                )\n",
        "                num_blocks_to_tune = len(tuneable_blocks_or_stages)\n",
        "\n",
        "            # Reverse to order from last layer (highest index) to first\n",
        "            tuneable_blocks_or_stages.reverse()\n",
        "            blocks_to_tune = tuneable_blocks_or_stages[:num_blocks_to_tune]\n",
        "\n",
        "            # Add parameter groups for encoder blocks with decayed LR\n",
        "            for i, block in enumerate(blocks_to_tune):\n",
        "                block_lr = lr * (\n",
        "                    lr_decay_factor**i\n",
        "                )  # Apply decay based on depth (i=0 is last block)\n",
        "                block_params = list(block.parameters())\n",
        "                if not block_params:\n",
        "                    logger.info(\n",
        "                        f'Opt: Block {i} (type {type(block).__name__}) has no parameters. Skipping.'\n",
        "                    )\n",
        "                    continue\n",
        "                params_to_optimize.append({'params': block_params, 'lr': block_lr})\n",
        "                logger.info(\n",
        "                    f'Opt: Including block {i} (type {type(block).__name__}) with LR {block_lr:.2e}'\n",
        "                )\n",
        "            # --- End of Parameter Group Selection Logic ---\n",
        "        else:\n",
        "            logger.info('Opt: num_blocks_to_tune is 0. Only training the head.')\n",
        "\n",
        "        logger.info(f'Opt: Total parameter groups for optimizer: {len(params_to_optimize)}')\n",
        "\n",
        "        # 1. Define your chosen optimizer (AdamW)\n",
        "        optimizer = optim.AdamW(\n",
        "            params_to_optimize,\n",
        "            lr=lr,  # Base LR is default for AdamW, but groups override it\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "\n",
        "        # 2. Define your chosen scheduler (ReduceLROnPlateau)\n",
        "        # Monitoring 'valid_loss' which you log in validation_step\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',  # Minimize the monitored metric (loss)\n",
        "            factor=0.75,  # Reduce LR by half when plateaued\n",
        "            patience=5,  # Number of epochs with no improvement to wait\n",
        "            min_lr=1e-6,  # Minimum learning rate\n",
        "        )\n",
        "\n",
        "        # 3. Return configuration for PyTorch Lightning\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': scheduler,\n",
        "                'monitor': 'valid_loss',  # Metric to monitor for scheduler\n",
        "                'interval': 'epoch',  # Check metric at the end of each epoch\n",
        "                'frequency': 1,  # Check every 1 epoch\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "class AddBimodalNoise(object):\n",
        "    def __init__(self, means, stds=None, weights=None, noise_scale=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            means: List of two means [mean1, mean2]\n",
        "            stds: List of two stds [std1, std2]\n",
        "            weights: List of two weights [w1, w2]\n",
        "            noise_scale: Scale factor for noise (default 0.1)\n",
        "        \"\"\"\n",
        "        self.means = torch.tensor(means)\n",
        "        self.stds = torch.tensor(stds if stds else [0.1, 0.1])\n",
        "        self.weights = torch.tensor(weights if weights else [0.5, 0.5])\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Add noise while preserving bimodal distribution.\n",
        "        \"\"\"\n",
        "        # Calculate distances to each mode\n",
        "        dist1 = torch.abs(tensor - self.means[0])\n",
        "        dist2 = torch.abs(tensor - self.means[1])\n",
        "\n",
        "        # Simple mask based on which mode is closer\n",
        "        mask = dist1 < dist2\n",
        "\n",
        "        # Generate noise for each mode\n",
        "        noise1 = torch.randn_like(tensor) * self.stds[0] * self.noise_scale\n",
        "        noise2 = torch.randn_like(tensor) * self.stds[1] * self.noise_scale\n",
        "\n",
        "        # Apply noise based on mask\n",
        "        noise = torch.where(mask, noise1, noise2)\n",
        "\n",
        "        # Ensure no NaN or inf values\n",
        "        noise = torch.nan_to_num(noise, 0.0)\n",
        "\n",
        "        return torch.clamp(tensor + noise, -1.0, 1.0)\n",
        "\n",
        "\n",
        "def load_model(model_path):\n",
        "    # 1. get hyperparameters\n",
        "    checkpoint = torch.load(model_path, map_location=DEVICE)\n",
        "    hparams = checkpoint['hyper_parameters']\n",
        "\n",
        "    # 2. Load model with those hyperparameters\n",
        "    checkpoint_model = ZooBot_lightning.load_from_checkpoint(checkpoint_path=model_path, **hparams)\n",
        "\n",
        "    # 3. Freeze all layers and set to eval mode\n",
        "    checkpoint_model.freeze()  # Freeze all layers (equivalent to requires_grad=False)\n",
        "    checkpoint_model.eval()  # Disables dropout, batchnorm updates\n",
        "\n",
        "    # 4. Device placement\n",
        "    checkpoint_model = checkpoint_model.to(DEVICE)  # After freeze()/eval()\n",
        "\n",
        "    return checkpoint_model\n",
        "\n",
        "\n",
        "class SimpleProgressCallback(Callback):\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        current_epoch = trainer.current_epoch\n",
        "        max_epochs = trainer.max_epochs\n",
        "        logger.info(f'Completed epoch {current_epoch+1}/{max_epochs}')\n",
        "        # Force display update\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        if trainer.sanity_checking:\n",
        "            return\n",
        "        logger.info(f\"Validation loss: {float(trainer.callback_metrics.get('valid_loss', 0)):.4f}\")\n",
        "\n",
        "\n",
        "def train_fold(\n",
        "    fold_idx,\n",
        "    train_fold_indices,\n",
        "    val_fold_indices,\n",
        "    all_data,\n",
        "    all_labels,\n",
        "    transform,\n",
        "    wandb_enabled,\n",
        "    with_weighted_sampler=False,\n",
        "):\n",
        "    \"\"\"Trains the model for a single fold.\"\"\"\n",
        "    fold_start_time = time.time()\n",
        "    logger.info(f'\\n===== Starting Fold {fold_idx}/{N_SPLITS} =====')\n",
        "\n",
        "    # 1. Get Data for Fold\n",
        "    X_train_fold = all_data[train_fold_indices]\n",
        "    y_train_fold = all_labels[train_fold_indices]\n",
        "    X_val_fold = all_data[val_fold_indices]\n",
        "    y_val_fold = all_labels[val_fold_indices]\n",
        "    logger.info(f'Fold {fold_idx}: Train size={len(X_train_fold)}, Val size={len(X_val_fold)}')\n",
        "\n",
        "    try:\n",
        "        if 'NUM_STRATIFICATION_BINS' not in globals():\n",
        "            raise NameError('NUM_STRATIFICATION_BINS not found in global scope.')\n",
        "\n",
        "        # Calculate bin edges (must match the ones used in prepare_kfold_splits)\n",
        "        bin_edges = np.linspace(0, 1, NUM_STRATIFICATION_BINS + 1)\n",
        "\n",
        "        # Calculate bin indices for training labels of this fold\n",
        "        # Use bins=bin_edges[1:-1] to match the definition in prepare_kfold_splits\n",
        "        train_bin_indices = np.digitize(y_train_fold, bins=bin_edges[1:-1])\n",
        "        # Count samples per bin, ensuring all bins are represented (even if empty)\n",
        "        train_bin_counts = np.bincount(train_bin_indices, minlength=NUM_STRATIFICATION_BINS)\n",
        "\n",
        "        # Calculate bin indices for validation labels of this fold\n",
        "        val_bin_indices = np.digitize(y_val_fold, bins=bin_edges[1:-1])\n",
        "        # Count samples per bin\n",
        "        val_bin_counts = np.bincount(val_bin_indices, minlength=NUM_STRATIFICATION_BINS)\n",
        "\n",
        "        # Log the counts\n",
        "        logger.info(f'Fold {fold_idx}: Stratification Bin Edges: {np.round(bin_edges, 3)}')\n",
        "        logger.info(\n",
        "            f'Fold {fold_idx}: **Train Set** Bin Counts (Bins 0 to {NUM_STRATIFICATION_BINS-1}): {train_bin_counts}'\n",
        "        )\n",
        "        logger.info(\n",
        "            f'Fold {fold_idx}: **Validation Set** Bin Counts (Bins 0 to {NUM_STRATIFICATION_BINS-1}): {val_bin_counts}'\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f'Fold {fold_idx}: Failed to calculate or log bin counts. Error: {e}')\n",
        "\n",
        "    # 2. Create Datasets\n",
        "    train_dataset_fold = SimpleDataset(X_train_fold, y_train_fold, transform=transform)\n",
        "    # No augmentation for validation set\n",
        "    val_dataset_fold = SimpleDataset(X_val_fold, y_val_fold, transform=None)\n",
        "\n",
        "    # 3. Create Weighted Sampler\n",
        "    if with_weighted_sampler:\n",
        "        train_sampler_fold = create_weighted_sampler(\n",
        "            y_train_fold,\n",
        "            num_weighting_bins=NUM_STRATIFICATION_BINS,\n",
        "            alpha=SAMPLER_ALPHA,\n",
        "        )\n",
        "        logger.info(f'Fold {fold_idx}: Created weighted sampler for training data.')\n",
        "\n",
        "    # 4. Create DataLoaders\n",
        "    train_loader_fold = DataLoader(\n",
        "        train_dataset_fold,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        sampler=train_sampler_fold if with_weighted_sampler else None,\n",
        "        shuffle=False if with_weighted_sampler else True,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        persistent_workers=True if NUM_WORKERS > 0 else False,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader_fold = DataLoader(\n",
        "        val_dataset_fold,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        persistent_workers=True if NUM_WORKERS > 0 else False,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # 5. Initialize Model, Trainer, Logger, Callbacks\n",
        "    model = ZooBot_lightning(\n",
        "        zoobot_size=ZOOBOT_SIZE,\n",
        "        zoobot_blocks=ZOOBOT_BLOCKS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        learning_decay=LEARNING_DECAY,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        label_smoothing=LABEL_SMOOTHING,\n",
        "        loss_type=LOSS_TYPE,\n",
        "        focal_gamma=FOCAL_GAMMA,\n",
        "    )\n",
        "\n",
        "    # W&B Logger - log each fold as a separate run within the same project\n",
        "    if wandb_enabled:\n",
        "        run_name = (\n",
        "            f'{ZOOBOT_SIZE}_fold{fold_idx}_lr{LEARNING_RATE}_wd{WEIGHT_DECAY}_{LOSS_TYPE}-loss'\n",
        "        )\n",
        "        wandb_logger = WandbLogger(\n",
        "            project=WANDB_PROJECT,\n",
        "            group=WANDB_GROUP,\n",
        "            name=run_name,\n",
        "        )\n",
        "        logger.info(\n",
        "            f\"Fold {fold_idx}: Initialized W&B logger for run '{run_name}' in group '{WANDB_GROUP}'.\"\n",
        "        )\n",
        "    else:\n",
        "        logger.error('W&B logging is disabled. Exiting.')\n",
        "        return\n",
        "\n",
        "    # Checkpoint Callback - save the best model for this fold\n",
        "    fold_save_dir = os.path.join(BASE_SAVE_DIR, f'fold_{fold_idx}')\n",
        "    os.makedirs(fold_save_dir, exist_ok=True)\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor='valid_loss',\n",
        "        dirpath=fold_save_dir,\n",
        "        filename='best_model-{epoch:02d}-{valid_loss:.4f}',\n",
        "        save_top_k=1,\n",
        "        mode='min',\n",
        "    )\n",
        "\n",
        "    # Early Stopping Callback\n",
        "    early_stop_callback = EarlyStopping(\n",
        "        monitor='valid_loss', patience=EARLY_STOPPING_PATIENCE, verbose=True, mode='min'\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        logger=wandb_logger,\n",
        "        callbacks=[checkpoint_callback, early_stop_callback, SimpleProgressCallback()],\n",
        "        max_epochs=EPOCHS,\n",
        "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "        devices=1,\n",
        "        log_every_n_steps=len(train_loader_fold),\n",
        "        enable_progress_bar=False,\n",
        "    )\n",
        "\n",
        "    # 6. Train the Model\n",
        "    try:\n",
        "        logger.info(f'Fold {fold_idx}: Starting training...')\n",
        "        trainer.fit(model, train_loader_fold, val_loader_fold)\n",
        "        logger.info(f'Fold {fold_idx}: Training finished.')\n",
        "\n",
        "        # 7. Store Best Checkpoint Path\n",
        "        best_model_path = checkpoint_callback.best_model_path\n",
        "        logger.info(f'Fold {fold_idx}: Best model saved at: {best_model_path}')\n",
        "        logger.info(\n",
        "            f'Fold {fold_idx}: Best validation loss: {checkpoint_callback.best_model_score:.4f}'\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f'Fold {fold_idx}: Training failed with error: {e}')\n",
        "        best_model_path = None  # Indicate failure\n",
        "    finally:\n",
        "        # 8. Clean Up\n",
        "        wandb.finish()  # Finish the W&B run for this fold\n",
        "        del model, trainer, train_loader_fold, val_loader_fold, train_dataset_fold, val_dataset_fold\n",
        "        if with_weighted_sampler:\n",
        "            del train_sampler_fold\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        logger.info(f'Fold {fold_idx}: Resources cleaned up.')\n",
        "        fold_end_time = time.time()\n",
        "        logger.info(\n",
        "            f'===== Fold {fold_idx} Duration: {(fold_end_time - fold_start_time) / 60:.2f} minutes ====='\n",
        "        )\n",
        "\n",
        "    return best_model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvp6ddT6HfXy"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG_eIbs5HkIe"
      },
      "outputs": [],
      "source": [
        "PATH_DATA = 'drive/MyDrive/training_data/h5/training_data_v2.h5'\n",
        "DATA, LABEL, OBJ_ID = load_data(PATH_DATA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upHBIIjIXbRK"
      },
      "source": [
        "### Initialize HF and W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIeDHDOhXgVB"
      },
      "outputs": [],
      "source": [
        "# initialize huggingface\n",
        "init_hf()\n",
        "# initialize W&B\n",
        "wandb_enabled = init_wandb(key_name='WANDB_API_KEY_KFOLD')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dX05fOATUEX"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfQcCbQgw8QA"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "TEST_SIZE = 0.1\n",
        "VAL_SIZE = 0.1\n",
        "N_SPLITS = 10  # K for K-Fold\n",
        "NUM_STRATIFICATION_BINS = 5\n",
        "NOISE_MEAN = [-0.312967269, 0.30682983]\n",
        "NOISE_STD = [0.079292069, 0.08915830]\n",
        "NOISE_WEIGHT = [0.51760047, 0.482399529]\n",
        "NOISE_SCALE = 0.3\n",
        "\n",
        "# W&B\n",
        "WANDB_PROJECT = 'unions_dwarfs_kfold'\n",
        "WANDB_GROUP = f\"kfold_run_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "# Hyperparameters\n",
        "LABEL_SMOOTHING = 0.01\n",
        "LEARNING_RATE = 5e-5\n",
        "LEARNING_DECAY = 0.75\n",
        "WEIGHT_DECAY = 0.05\n",
        "LOSS_TYPE = 'kld'\n",
        "FOCAL_GAMMA = None\n",
        "ZOOBOT_SIZE = 'nano'  # pico, nano, tiny, small, base, large\n",
        "ZOOBOT_BLOCKS = 5\n",
        "WITH_WEIGHTED_SAMPLER = False\n",
        "SAMPLER_ALPHA = (\n",
        "    0.5  # Controls oversampling scale, [0, 1], higher means more aggressive oversampling\n",
        ")\n",
        "\n",
        "# Training\n",
        "NUM_WORKERS = 2\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 100\n",
        "EARLY_STOPPING_PATIENCE = 20\n",
        "\n",
        "# Model\n",
        "BASE_SAVE_DIR = 'drive/MyDrive/zoobot_models/kfold'\n",
        "\n",
        "# 1. Define augmentations\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomChoice(\n",
        "            [\n",
        "                transforms.RandomRotation((0, 0)),\n",
        "                transforms.RandomRotation((90, 90)),\n",
        "                transforms.RandomRotation((180, 180)),\n",
        "                transforms.RandomRotation((270, 270)),\n",
        "            ]\n",
        "        ),  # Random rotation by multiples of 90 degrees\n",
        "        transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
        "        transforms.RandomVerticalFlip(),  # Random vertical flip\n",
        "        transforms.RandomApply([transforms.ColorJitter(brightness=(0.9, 1.1))], p=0.6),\n",
        "        transforms.RandomApply([transforms.ColorJitter(contrast=(0.9, 1.1))], p=0.4),\n",
        "        AddBimodalNoise(\n",
        "            means=NOISE_MEAN, stds=NOISE_STD, weights=NOISE_WEIGHT, noise_scale=NOISE_SCALE\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 2. Prepare test and k-fold splits\n",
        "kfold_splitter, train_val_indices, test_indices = prepare_kfold_splits(\n",
        "    labels=LABEL,\n",
        "    test_size=TEST_SIZE,\n",
        "    n_splits=N_SPLITS,\n",
        "    num_strat_bins=NUM_STRATIFICATION_BINS,\n",
        "    seed=SEED,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PxTNmOoHN_p"
      },
      "source": [
        "### Train K-fold ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xaZn2muBw_n0"
      },
      "outputs": [],
      "source": [
        "best_fold_model_paths = {}\n",
        "\n",
        "for fold_idx, (train_fold_indices_rel, val_fold_indices_rel) in enumerate(kfold_splitter):\n",
        "    # Get actual data indices from fold indices\n",
        "    train_fold_indices_abs = train_val_indices[train_fold_indices_rel]\n",
        "    val_fold_indices_abs = train_val_indices[val_fold_indices_rel]\n",
        "\n",
        "    # Train the current fold\n",
        "    best_path = train_fold(\n",
        "        fold_idx=fold_idx,\n",
        "        train_fold_indices=train_fold_indices_abs,\n",
        "        val_fold_indices=val_fold_indices_abs,\n",
        "        all_data=DATA,\n",
        "        all_labels=LABEL,\n",
        "        transform=transform,\n",
        "        wandb_enabled=wandb_enabled,\n",
        "        with_weighted_sampler=WITH_WEIGHTED_SAMPLER,\n",
        "    )\n",
        "    # Append the best model path for this fold to the best path dict\n",
        "    if best_path:\n",
        "        best_fold_model_paths[fold_idx] = best_path\n",
        "    else:\n",
        "        logger.warning(f'Fold {fold_idx} did not produce a best model path.')\n",
        "\n",
        "logger.info('\\n===== K-Fold Training Complete =====')\n",
        "logger.info(f'Best model paths per fold: {best_fold_model_paths}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3XptTaONcZu"
      },
      "outputs": [],
      "source": [
        "best_fold_model_paths = {\n",
        "    0: '/content/drive/MyDrive/zoobot_models/kfold/fold_0/best_model-epoch=07-valid_loss=0.0312.ckpt',\n",
        "    1: '/content/drive/MyDrive/zoobot_models/kfold/fold_1/best_model-epoch=39-valid_loss=0.0322.ckpt',\n",
        "    2: '/content/drive/MyDrive/zoobot_models/kfold/fold_2/best_model-epoch=14-valid_loss=0.0378.ckpt',\n",
        "    3: '/content/drive/MyDrive/zoobot_models/kfold/fold_3/best_model-epoch=75-valid_loss=0.0292.ckpt',\n",
        "    4: '/content/drive/MyDrive/zoobot_models/kfold/fold_4/best_model-epoch=14-valid_loss=0.0272.ckpt',\n",
        "    5: '/content/drive/MyDrive/zoobot_models/kfold/fold_5/best_model-epoch=19-valid_loss=0.0244.ckpt',\n",
        "    6: '/content/drive/MyDrive/zoobot_models/kfold/fold_6/best_model-epoch=33-valid_loss=0.0425.ckpt',\n",
        "    7: '/content/drive/MyDrive/zoobot_models/kfold/fold_7/best_model-epoch=21-valid_loss=0.0315.ckpt',\n",
        "    8: '/content/drive/MyDrive/zoobot_models/kfold/fold_8/best_model-epoch=45-valid_loss=0.0265.ckpt',\n",
        "    9: '/content/drive/MyDrive/zoobot_models/kfold/fold_9/best_model-epoch=41-valid_loss=0.0266.ckpt',\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q420SoJBcyU"
      },
      "source": [
        "### Evaluate ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWNfaT9Qebdw"
      },
      "outputs": [],
      "source": [
        "X_test = DATA[test_indices]\n",
        "y_test = LABEL[test_indices]\n",
        "id_test = OBJ_ID[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfKvT6IElEMO"
      },
      "outputs": [],
      "source": [
        "len(np.unique(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoAYf8YdkQDS"
      },
      "outputs": [],
      "source": [
        "def evaluate_kfold_ensemble(\n",
        "    best_fold_model_paths, test_indices, all_data, all_labels, batch_size=64\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluates an ensemble of models from K-fold training on the test set.\n",
        "\n",
        "    Args:\n",
        "        best_fold_model_paths (dict): Dictionary mapping fold indices to model checkpoint paths\n",
        "        test_indices (np.ndarray): Indices of test data\n",
        "        all_data (np.ndarray): The complete dataset\n",
        "        all_labels (np.ndarray): The complete label set\n",
        "        batch_size (int): Batch size for evaluation\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing:\n",
        "            - 'ensemble_predictions': Average predictions across all models\n",
        "            - 'individual_predictions': Predictions from each individual model\n",
        "            - 'test_labels': True labels for the test set\n",
        "            - 'metrics': Performance metrics\n",
        "    \"\"\"\n",
        "    # 1. Data Preparation\n",
        "    logger.info(f'Extracting test data (n={len(test_indices)}) from full dataset')\n",
        "    X_test = all_data[test_indices]\n",
        "    y_test = all_labels[test_indices]\n",
        "\n",
        "    test_dataset = SimpleDataset(X_test, y_test)\n",
        "\n",
        "    # Create dataloader\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False\n",
        "    )\n",
        "\n",
        "    logger.info(f'Created test DataLoader with {len(test_loader)} batches')\n",
        "\n",
        "    # 2. Model Loading and Prediction\n",
        "    individual_predictions = {}\n",
        "    num_models = len(best_fold_model_paths)\n",
        "\n",
        "    logger.info(f'Starting evaluation of {num_models} models')\n",
        "\n",
        "    for fold_idx, model_path in best_fold_model_paths.items():\n",
        "        logger.info(f'Evaluating model from fold {fold_idx}')\n",
        "\n",
        "        try:\n",
        "            # Load the model\n",
        "            logger.info(f'Loading model from {model_path}')\n",
        "            model = load_model(model_path)\n",
        "            model.eval()  # Ensure model is in evaluation mode\n",
        "\n",
        "            # Get predictions\n",
        "            fold_predictions = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for images, _ in tqdm(test_loader, desc=f'Fold {fold_idx} Predictions'):\n",
        "                    images = images.to(DEVICE)\n",
        "\n",
        "                    # Forward pass\n",
        "                    logits = model(images)\n",
        "\n",
        "                    # Get probabilities for the positive class\n",
        "                    probs = F.softmax(logits, dim=1)[:, 1]\n",
        "\n",
        "                    # Store predictions\n",
        "                    fold_predictions.append(probs.cpu().numpy())\n",
        "\n",
        "            # Concatenate batch predictions\n",
        "            fold_predictions = np.concatenate(fold_predictions)\n",
        "            individual_predictions[fold_idx] = fold_predictions\n",
        "\n",
        "            # Clean up\n",
        "            del model\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f'Error evaluating model from fold {fold_idx}: {e}')\n",
        "            continue\n",
        "\n",
        "    # 3. Ensemble Prediction\n",
        "    if not individual_predictions:\n",
        "        raise ValueError('No valid predictions from any model')\n",
        "\n",
        "    # Stack individual predictions for easier operations\n",
        "    all_preds = np.array(list(individual_predictions.values()))\n",
        "\n",
        "    # Calculate mean and standard deviation across models\n",
        "    ensemble_predictions = np.mean(all_preds, axis=0)\n",
        "    prediction_std = np.std(all_preds, axis=0)\n",
        "\n",
        "    logger.info(f'Created ensemble predictions with shape {ensemble_predictions.shape}')\n",
        "\n",
        "    # 4. Performance Evaluation\n",
        "    # Calculate metrics\n",
        "    mse = np.mean((ensemble_predictions - y_test) ** 2)  # Brier score\n",
        "    mae = np.mean(np.abs(ensemble_predictions - y_test))\n",
        "\n",
        "    # Calculate Expected Calibration Error (ECE)\n",
        "    n_bins = 10\n",
        "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_indices = np.digitize(ensemble_predictions, bin_boundaries) - 1\n",
        "    bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
        "\n",
        "    bin_sizes = np.zeros(n_bins)\n",
        "    bin_avg_preds = np.zeros(n_bins)\n",
        "    bin_avg_labels = np.zeros(n_bins)\n",
        "\n",
        "    for bin_idx in range(n_bins):\n",
        "        mask = bin_indices == bin_idx\n",
        "        bin_sizes[bin_idx] = mask.sum()\n",
        "\n",
        "        if bin_sizes[bin_idx] > 0:\n",
        "            bin_avg_preds[bin_idx] = ensemble_predictions[mask].mean()\n",
        "            bin_avg_labels[bin_idx] = y_test[mask].mean()\n",
        "\n",
        "    ece = np.sum((bin_sizes / len(ensemble_predictions)) * np.abs(bin_avg_preds - bin_avg_labels))\n",
        "\n",
        "    metrics = {\n",
        "        'brier_score': mse,\n",
        "        'mae': mae,\n",
        "        'ece': ece,\n",
        "        'bin_data': {\n",
        "            'sizes': bin_sizes,\n",
        "            'avg_preds': bin_avg_preds,\n",
        "            'avg_labels': bin_avg_labels,\n",
        "            'boundaries': bin_boundaries,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    logger.info(f'Brier Score: {mse:.4f}, MAE: {mae:.4f}, ECE: {ece:.4f}')\n",
        "\n",
        "    # 5. Return results\n",
        "    return {\n",
        "        'ensemble_predictions': ensemble_predictions,\n",
        "        'individual_predictions': individual_predictions,\n",
        "        'prediction_std': prediction_std,\n",
        "        'test_labels': y_test,\n",
        "        'metrics': metrics,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0-HrHFukwIJ"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "stats = evaluate_kfold_ensemble(\n",
        "    best_fold_model_paths=best_fold_model_paths,\n",
        "    test_indices=test_indices,\n",
        "    all_data=DATA,\n",
        "    all_labels=LABEL,\n",
        "    batch_size=64,\n",
        ")\n",
        "print(f'Finished in {time.time()-start:.2f} seconds.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjOrk6nyluEF"
      },
      "outputs": [],
      "source": [
        "pred, true = stats['ensemble_predictions'], stats['test_labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZhFX_nt4sbN"
      },
      "outputs": [],
      "source": [
        "np.mean(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdqLBaAnl4_P"
      },
      "outputs": [],
      "source": [
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jIc4qqd3YNK"
      },
      "outputs": [],
      "source": [
        "data = {'ID': OBJ_ID[test_indices], 'label_v2': LABEL[test_indices], 'zoobot_pred_v2': pred}\n",
        "df_test_v2 = pd.DataFrame(data)\n",
        "df_test_v2.to_csv('drive/MyDrive/zoobot_models/testset_eval/test_predictions_v2.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lhVEjm-l93N"
      },
      "outputs": [],
      "source": [
        "def performance_histogram(true, pred, bins=5, save_path=None):\n",
        "    color = [\n",
        "        colors['blue'],\n",
        "        colors['orange'],\n",
        "        colors['green'],\n",
        "        colors['red'],\n",
        "        colors['purple'],\n",
        "    ]\n",
        "    plt.close('all')\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i in range(bins):\n",
        "        arg_in = np.where((true >= 1 / bins * i) & (true <= 1 / bins * i + 1 / bins))[0]\n",
        "        plt.hist(\n",
        "            pred[arg_in],\n",
        "            alpha=0.5,\n",
        "            label=f'[{1/bins * i:.2f}, {1/bins * i + 1/bins:.2f}]',\n",
        "            edgecolor='k',\n",
        "            color=color[i],\n",
        "            histtype='stepfilled',\n",
        "            linewidth=2,\n",
        "        )\n",
        "    plt.semilogy()\n",
        "    plt.legend(loc='best', fontsize=15)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.ylabel('Number of objects', fontsize=18)\n",
        "    plt.xlabel('Predicted probability', fontsize=18)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_gallery(\n",
        "    data_val,\n",
        "    label_val,\n",
        "    pred,\n",
        "    bins=10,\n",
        "    per_bin=10,\n",
        "    figsize=(50, 50),\n",
        "    title_fontsize=50,\n",
        "    label_fontsize=35,\n",
        "    wspace=0.0,\n",
        "    hspace=0.1,\n",
        "    seed=None,\n",
        "    save_path=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate and save a gallery of image cutouts arranged in a grid with columns\n",
        "    corresponding to prediction bins and rows containing individual cutouts.\n",
        "    If a bin has fewer samples than requested, empty spaces will be left.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_val : array-like\n",
        "        Array of image data.\n",
        "    label_val : array-like\n",
        "        Array of labels corresponding to the images.\n",
        "    pred : array-like\n",
        "        Array of prediction values used for binning.\n",
        "    bins : int, optional\n",
        "        Number of bins (columns) to use (default is 10).\n",
        "    per_bin : int, optional\n",
        "        Number of images per bin (rows) (default is 10).\n",
        "    figsize : tuple, optional\n",
        "        Figure size in inches (default is (50, 50)).\n",
        "    title_fontsize : int, optional\n",
        "        Font size for the column titles (bin ranges).\n",
        "    label_fontsize : int, optional\n",
        "        Font size for the x-label under each image.\n",
        "    wspace : float, optional\n",
        "        The amount of width reserved for blank space between subplots (default is 0.0).\n",
        "    hspace : float, optional\n",
        "        The amount of height reserved for white space between subplots (default is 0.1).\n",
        "    seed : int or None, optional\n",
        "        Random seed for reproducibility. If provided, the same images will be chosen each time.\n",
        "    save_path: str, optional\n",
        "        Path to save the figure. If None, the figure is not saved.\n",
        "    \"\"\"\n",
        "    # Set random seed for reproducibility if provided.\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # Create a grid of subplots\n",
        "    fig, axes = plt.subplots(nrows=per_bin, ncols=bins, figsize=figsize)\n",
        "\n",
        "    # Loop over each bin (column)\n",
        "    for i in range(bins):\n",
        "        # Define the bin range.\n",
        "        bin_lower = 1.0 / bins * i\n",
        "        bin_upper = 1.0 / bins * (i + 1)\n",
        "\n",
        "        # Select indices for images whose predictions fall within the bin.\n",
        "        bin_indices = np.where((pred > bin_lower) & (pred < bin_upper))[0]\n",
        "\n",
        "        # Get the number of available samples in this bin\n",
        "        available_samples = len(bin_indices)\n",
        "\n",
        "        if available_samples > 0:\n",
        "            # Sample without replacement, but only up to the available number\n",
        "            actual_samples = min(available_samples, per_bin)\n",
        "            idx = np.random.choice(bin_indices, size=actual_samples, replace=False)\n",
        "        else:\n",
        "            # No samples in this bin\n",
        "            actual_samples = 0\n",
        "            idx = []\n",
        "\n",
        "        # Loop over each row for the given bin.\n",
        "        for j in range(per_bin):\n",
        "            ax = axes[j, i]\n",
        "\n",
        "            # Only plot if we have a sample for this position\n",
        "            if j < actual_samples:\n",
        "                cutout_rgb = data_val[idx[j]]\n",
        "                cutout_label = label_val[idx[j]]\n",
        "\n",
        "                # Display the image; note the moveaxis to get channels in correct position\n",
        "                cutout_rgb = np.moveaxis(cutout_rgb, 0, -1)\n",
        "                ax.imshow(np.clip(cutout_rgb, 0, 1))\n",
        "\n",
        "                # Set the label below the image.\n",
        "                ax.set_xlabel(f'{cutout_label:.3f}', fontsize=label_fontsize, labelpad=5)\n",
        "            else:\n",
        "                # No sample available, set a dummy image with the same aspect ratio\n",
        "                # Create a blank square with the right dimensions\n",
        "                h, w = 64, 64  # Assuming this is your image size - adjust as needed\n",
        "                blank = np.ones((h, w, 3)) * 0.9  # Light gray square\n",
        "                ax.imshow(blank)\n",
        "                ax.set_xlabel('', fontsize=label_fontsize)\n",
        "\n",
        "            # Remove ticks for all subplots\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "\n",
        "            # Only add a title to the top row, showing the bin range.\n",
        "            if j == 0:\n",
        "                ax.set_title(\n",
        "                    f'[{bin_lower:.1f}, {bin_upper:.1f}]',\n",
        "                    fontsize=title_fontsize,\n",
        "                    fontweight='bold',\n",
        "                    pad=20,\n",
        "                )\n",
        "\n",
        "    # Adjust spacing between subplots.\n",
        "    plt.subplots_adjust(wspace=wspace, hspace=hspace)\n",
        "\n",
        "    # Save the figure if a save path is provided\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def visualize_ensemble_results(results, save_dir=None):\n",
        "    \"\"\"\n",
        "    Visualizes the results from the ensemble evaluation.\n",
        "\n",
        "    Args:\n",
        "        results (dict): The results dictionary from evaluate_kfold_ensemble\n",
        "        save_dir (str, optional): Directory to save figures. If None, figures are displayed only.\n",
        "    \"\"\"\n",
        "    ensemble_predictions = results['ensemble_predictions']\n",
        "    individual_predictions = results['individual_predictions']\n",
        "    prediction_std = results['prediction_std']\n",
        "    test_labels = results['test_labels']\n",
        "    metrics = results['metrics']\n",
        "\n",
        "    # Create color-blind friendly palette\n",
        "    colors = {\n",
        "        'blue': '#377eb8',\n",
        "        'orange': '#ff7f00',\n",
        "        'green': '#4daf4a',\n",
        "        'pink': '#f781bf',\n",
        "        'brown': '#a65628',\n",
        "        'purple': '#984ea3',\n",
        "        'gray': '#999999',\n",
        "        'red': '#e41a1c',\n",
        "        'yellow': '#dede00',\n",
        "    }\n",
        "\n",
        "    # 1. Prediction Distribution (2D histogram)\n",
        "    fig1, ax1 = plt.subplots(figsize=(8, 7))\n",
        "    h = ax1.hist2d(\n",
        "        test_labels,\n",
        "        ensemble_predictions,\n",
        "        bins=[25, 25],\n",
        "        range=[[0, 1], [0, 1]],\n",
        "        cmap='viridis',\n",
        "        norm='log',\n",
        "    )\n",
        "    ax1.plot([0, 1], [0, 1], 'r--', label='Perfect Prediction')\n",
        "    ax1.axhline(test_labels.mean(), linestyle='--', color='b', label='Mean True Label')\n",
        "    fig1.colorbar(h[3], ax=ax1)\n",
        "    ax1.set_ylabel('Model Predictions')\n",
        "    ax1.set_xlabel('True Probabilities')\n",
        "    ax1.set_title(\n",
        "        f'Ensemble Prediction vs Truth Distribution\\nBrier Score: {metrics[\"brier_score\"]:.4f}'\n",
        "    )\n",
        "    ax1.legend()\n",
        "\n",
        "    if save_dir:\n",
        "        plt.savefig(f'{save_dir}/prediction_distribution.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # 2. Reliability (Calibration) Curve from metrics\n",
        "    fig2, ax2 = plt.subplots(figsize=(8, 7))\n",
        "    bin_data = metrics['bin_data']\n",
        "\n",
        "    # Filter valid bins (with samples)\n",
        "    valid_bins = bin_data['sizes'] > 0\n",
        "    bin_avg_preds_valid = bin_data['avg_preds'][valid_bins]\n",
        "    bin_avg_labels_valid = bin_data['avg_labels'][valid_bins]\n",
        "    bin_sizes_valid = bin_data['sizes'][valid_bins]\n",
        "\n",
        "    # Draw diagonal representing perfect calibration\n",
        "    ax2.plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')\n",
        "\n",
        "    # Plot calibration curve\n",
        "    ax2.plot(bin_avg_preds_valid, bin_avg_labels_valid, 'bo-', label='Model Calibration')\n",
        "\n",
        "    # Add sample count annotations\n",
        "    for i in range(len(bin_avg_preds_valid)):\n",
        "        ax2.text(\n",
        "            bin_avg_preds_valid[i],\n",
        "            bin_avg_labels_valid[i] + 0.02,\n",
        "            f'n={int(bin_sizes_valid[i])}',\n",
        "            ha='center',\n",
        "            va='bottom',\n",
        "            fontsize=8,\n",
        "        )\n",
        "\n",
        "    ax2.set_xlabel('Mean Predicted Probability (Confidence)')\n",
        "    ax2.set_ylabel('Mean True Label (Accuracy)')\n",
        "    ax2.set_title(f'Reliability Curve\\nECE: {metrics[\"ece\"]:.4f}')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "    ax2.set_xlim([-0.05, 1.05])\n",
        "    ax2.set_ylim([-0.05, 1.05])\n",
        "\n",
        "    if save_dir:\n",
        "        plt.savefig(f'{save_dir}/reliability_curve.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # 3. Individual Model Predictions\n",
        "    fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Sort test data by true label for clarity\n",
        "    sort_idx = np.argsort(test_labels)\n",
        "    sorted_test_labels = test_labels[sort_idx]\n",
        "\n",
        "    # Plot true labels\n",
        "    ax3.plot(\n",
        "        range(len(sorted_test_labels)), sorted_test_labels, 'k-', label='True Labels', linewidth=2\n",
        "    )\n",
        "\n",
        "    # Plot each model's predictions (semi-transparent)\n",
        "    for fold_idx, preds in individual_predictions.items():\n",
        "        sorted_preds = preds[sort_idx]\n",
        "        ax3.plot(range(len(sorted_preds)), sorted_preds, 'b-', alpha=0.2, linewidth=1)\n",
        "\n",
        "    # Plot ensemble predictions\n",
        "    sorted_ensemble_preds = ensemble_predictions[sort_idx]\n",
        "    ax3.plot(\n",
        "        range(len(sorted_ensemble_preds)),\n",
        "        sorted_ensemble_preds,\n",
        "        'r-',\n",
        "        label='Ensemble Prediction',\n",
        "        linewidth=2,\n",
        "    )\n",
        "\n",
        "    ax3.set_xlabel('Test Sample (sorted by true label)')\n",
        "    ax3.set_ylabel('Probability')\n",
        "    ax3.set_title('Ensemble vs Individual Model Predictions')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    if save_dir:\n",
        "        plt.savefig(f'{save_dir}/model_predictions.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # 4. Prediction Uncertainty\n",
        "    fig4, ax4 = plt.subplots(figsize=(8, 7))\n",
        "\n",
        "    # Scatter plot of predictions with error bars showing standard deviation\n",
        "    sorted_std = prediction_std[sort_idx]\n",
        "\n",
        "    # Create scatter plot where color intensity represents prediction uncertainty\n",
        "    scatter = ax4.scatter(\n",
        "        range(len(sorted_test_labels)),\n",
        "        sorted_ensemble_preds,\n",
        "        c=sorted_std,\n",
        "        cmap='viridis_r',\n",
        "        s=15,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "\n",
        "    # Plot true labels\n",
        "    ax4.plot(\n",
        "        range(len(sorted_test_labels)), sorted_test_labels, 'k-', label='True Labels', linewidth=1.5\n",
        "    )\n",
        "\n",
        "    # Add colorbar for standard deviation\n",
        "    cbar = plt.colorbar(scatter, ax=ax4)\n",
        "    cbar.set_label('Standard Deviation (Uncertainty)')\n",
        "\n",
        "    ax4.set_xlabel('Test Sample (sorted by true label)')\n",
        "    ax4.set_ylabel('Probability')\n",
        "    ax4.set_title('Ensemble Predictions with Uncertainty')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    if save_dir:\n",
        "        plt.savefig(f'{save_dir}/prediction_uncertainty.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # 5. Error Analysis\n",
        "    fig5, ax5 = plt.subplots(figsize=(8, 7))\n",
        "\n",
        "    errors = ensemble_predictions - test_labels\n",
        "    abs_errors = np.abs(errors)\n",
        "\n",
        "    # Create a histogram of errors\n",
        "    ax5.hist(errors, bins=50, alpha=0.7, color=colors['blue'])\n",
        "    ax5.axvline(0, color='r', linestyle='--', linewidth=1)\n",
        "    ax5.axvline(\n",
        "        np.mean(errors),\n",
        "        color='g',\n",
        "        linestyle='-',\n",
        "        linewidth=1,\n",
        "        label=f'Mean Error: {np.mean(errors):.4f}',\n",
        "    )\n",
        "\n",
        "    ax5.set_xlabel('Prediction Error (Predicted - True)')\n",
        "    ax5.set_ylabel('Count')\n",
        "    ax5.set_title(f'Error Distribution\\nMAE: {metrics[\"mae\"]:.4f}')\n",
        "    ax5.legend()\n",
        "\n",
        "    if save_dir:\n",
        "        plt.savefig(f'{save_dir}/error_distribution.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Show all plots\n",
        "    plt.tight_layout()\n",
        "    if not save_dir:\n",
        "        plt.show()\n",
        "\n",
        "    return [fig1, fig2, fig3, fig4, fig5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcGnDQvh8ZD9"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import to_rgba\n",
        "from matplotlib.legend_handler import HandlerErrorbar\n",
        "\n",
        "\n",
        "def performance_plot(\n",
        "    true,\n",
        "    pred,\n",
        "    n_bins=None,\n",
        "    min_samples=0,\n",
        "    title='Model Performance',\n",
        "    figsize=(8, 6),\n",
        "    show_counts=True,\n",
        "    size_by_count=True,\n",
        "    overlap_tolerance=0.0,\n",
        "    save_path=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a performance plot showing mean predicted probability vs. true label.\n",
        "    Shows sample counts to provide context for bins with few samples.\n",
        "\n",
        "    Args:\n",
        "        true (np.ndarray): Array of true labels (continuous, assumed 0-1).\n",
        "        pred (np.ndarray): Array of predicted probabilities (continuous, 0-1).\n",
        "        n_bins (int, optional):\n",
        "            - If None (default): Bins are created based on the unique values\n",
        "              present in the `true` array. X-axis shows the unique true values.\n",
        "            - If int > 0: Creates `n_bins` equally sized bins for the `true`\n",
        "              labels between 0 and 1. X-axis shows the bin centers.\n",
        "        min_samples (int): Minimum number of samples required for a bin to be plotted.\n",
        "        title (str): Title for the plot.\n",
        "        figsize (tuple): Figure size for the plot.\n",
        "        show_counts (bool): Whether to show sample counts as text annotations.\n",
        "        size_by_count (bool): Whether to size points proportionally to sample counts.\n",
        "        overlap_tolerance (float): Allowed overlap between annotation boxes (0.0-0.5).\n",
        "            - 0.0 means no overlap allowed (strict)\n",
        "            - Higher values allow more overlap (0.2 is a reasonable value)\n",
        "            - Max recommended value is 0.5 (half box overlap)\n",
        "        save_path (str, optional): Path to save the figure. If None, the figure is not saved.\n",
        "    \"\"\"\n",
        "    import matplotlib.transforms as transforms\n",
        "\n",
        "    # Ensure overlap_tolerance is within reasonable bounds\n",
        "    overlap_tolerance = max(0.0, min(0.5, overlap_tolerance))\n",
        "\n",
        "    plt.close('all')\n",
        "    # Create figure\n",
        "    fig = plt.figure(figsize=figsize, clear=True)\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    x_values = []\n",
        "    mean_pred_bin = []\n",
        "    std_pred_bin = []\n",
        "    bin_counts = []\n",
        "    x_label = ''  # Will be set based on binning method\n",
        "\n",
        "    # --- Choose Binning Strategy ---\n",
        "    if n_bins is None or n_bins <= 0:\n",
        "        # --- Mode 1: Bin by unique true label values ---\n",
        "        x_label = 'Label'\n",
        "        unique_true_values = np.unique(true)\n",
        "\n",
        "        for i in unique_true_values:\n",
        "            bin_indices = np.where(true == i)[0]\n",
        "            pred_in_bin = pred[bin_indices]\n",
        "            count = len(pred_in_bin)\n",
        "\n",
        "            if count >= min_samples:\n",
        "                x_values.append(i)\n",
        "                mean_pred_bin.append(np.mean(pred_in_bin))\n",
        "                std_pred_bin.append(np.std(pred_in_bin) if count > 1 else 0.0)\n",
        "                bin_counts.append(count)\n",
        "    else:\n",
        "        # --- Mode 2: Use n_bins equally sized bins ---\n",
        "        x_label = 'Label Bin Center'\n",
        "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
        "\n",
        "        for i in range(n_bins):\n",
        "            bin_min = bin_edges[i]\n",
        "            bin_max = bin_edges[i + 1]\n",
        "\n",
        "            # Create mask for samples within the current bin\n",
        "            if i == n_bins - 1:  # Handle the last bin edge explicitly\n",
        "                bin_mask = (true >= bin_min) & (true <= bin_max)\n",
        "            else:\n",
        "                bin_mask = (true >= bin_min) & (true < bin_max)\n",
        "\n",
        "            pred_in_bin = pred[bin_mask]\n",
        "            count = len(pred_in_bin)\n",
        "\n",
        "            if count >= min_samples:\n",
        "                bin_center = (bin_min + bin_max) / 2.0\n",
        "                x_values.append(bin_center)\n",
        "                mean_pred_bin.append(np.mean(pred_in_bin))\n",
        "                std_pred_bin.append(np.std(pred_in_bin) if count > 1 else 0.0)\n",
        "                bin_counts.append(count)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    x_values = np.array(x_values)\n",
        "    mean_pred_bin = np.array(mean_pred_bin)\n",
        "    std_pred_bin = np.array(std_pred_bin)\n",
        "    bin_counts = np.array(bin_counts)\n",
        "\n",
        "    # --- Plotting ---\n",
        "    if len(x_values) > 0:  # Check if there's anything to plot\n",
        "        # Size points by count if requested\n",
        "        if size_by_count and len(bin_counts) > 0:\n",
        "            # Scale point sizes between 20 and 200 based on counts\n",
        "            min_count = np.min(bin_counts)\n",
        "            max_count = np.max(bin_counts)\n",
        "\n",
        "            # Handle case where all bins have the same count\n",
        "            if max_count == min_count:\n",
        "                point_sizes = np.ones_like(bin_counts) * 100\n",
        "            else:\n",
        "                # Scale point sizes logarithmically to better show differences\n",
        "                log_counts = np.log1p(bin_counts)\n",
        "                log_min = np.log1p(min_count)\n",
        "                log_max = np.log1p(max_count)\n",
        "\n",
        "                # Scale between 20 and 200\n",
        "                normalized_sizes = (log_counts - log_min) / (log_max - log_min)\n",
        "                point_sizes = 20 + normalized_sizes * 380\n",
        "        else:\n",
        "            point_sizes = np.ones_like(bin_counts) * 80\n",
        "\n",
        "        # Plot error bars\n",
        "        ax.errorbar(\n",
        "            x_values,\n",
        "            mean_pred_bin,\n",
        "            yerr=std_pred_bin,\n",
        "            fmt='none',  # No markers, we'll add them explicitly\n",
        "            ecolor=colors['gray'],\n",
        "            elinewidth=1.2,\n",
        "            capsize=5,\n",
        "            alpha=1,\n",
        "            zorder=0,\n",
        "        )\n",
        "\n",
        "        # Add scatter points with size based on count\n",
        "        ax.scatter(\n",
        "            x_values,\n",
        "            mean_pred_bin,\n",
        "            s=point_sizes,\n",
        "            color=colors['blue'],\n",
        "            alpha=0.7,\n",
        "            edgecolor='darkblue',\n",
        "            linewidth=1,\n",
        "        )\n",
        "\n",
        "        ms_for_legend = 7  # A representative linear marker size for the legend\n",
        "        x_dummy = [-1e9]  # Single off-screen data point x\n",
        "        y_dummy = [-1e9]  # Single off-screen data point y\n",
        "        y_err_dummy = [0.1]  # A small, non-zero error value for the dummy point\n",
        "        dummy = ax.errorbar(\n",
        "            x_dummy,\n",
        "            y_dummy,\n",
        "            yerr=y_err_dummy,  # yerr value is a placeholder to draw the error bar part\n",
        "            fmt='o',\n",
        "            linestyle='None',  # Format for marker and error bar symbol\n",
        "            ms=ms_for_legend,\n",
        "            # Marker face color (consistent with scatter)\n",
        "            mfc=to_rgba(colors['blue'], alpha=0.7),\n",
        "            # Marker edge color (consistent with scatter)\n",
        "            mec=to_rgba('darkblue', alpha=0.7),\n",
        "            # Marker edge width (consistent with scatter)\n",
        "            mew=1,\n",
        "            # Error bar line color (consistent with actual error bars)\n",
        "            ecolor=to_rgba(colors['gray'], alpha=1),\n",
        "            elinewidth=1.2,\n",
        "            capsize=4,\n",
        "            capthick=2,\n",
        "            label='Mean  Std Dev',  # The desired label\n",
        "        )\n",
        "\n",
        "        # Add count annotations if requested, with better positioning\n",
        "        if show_counts:\n",
        "            # Create a BBoxTransform to go from display to figure coordinates\n",
        "            disp_to_fig = transforms.ScaledTranslation(0, 0, fig.dpi_scale_trans)\n",
        "\n",
        "            # Sort points by x-coordinate for left-to-right processing\n",
        "            indices = np.argsort(x_values)\n",
        "\n",
        "            # Label placement parameters\n",
        "            base_offset = 10  # in points\n",
        "\n",
        "            # Keep track of label boxes in figure coordinates\n",
        "            boxes = []  # List of (x_min, y_min, x_max, y_max) in figure coordinates\n",
        "\n",
        "            # First pass - create annotations with temporary positions\n",
        "            # We need this to get approximate dimensions\n",
        "            annotations = []\n",
        "            for idx in indices:\n",
        "                x = x_values[idx]\n",
        "                y = mean_pred_bin[idx]\n",
        "                count = bin_counts[idx]\n",
        "\n",
        "                # Create annotation but don't add it to the plot yet\n",
        "                annotation = ax.annotate(\n",
        "                    f'n={count}',\n",
        "                    xy=(x, y),\n",
        "                    xytext=(0, base_offset),\n",
        "                    textcoords='offset points',\n",
        "                    ha='center',\n",
        "                    va='bottom',\n",
        "                    fontsize=9,\n",
        "                    bbox=dict(boxstyle='round,pad=0.1', fc='white', alpha=0.7),\n",
        "                )\n",
        "\n",
        "                # Store annotation for later use\n",
        "                annotations.append(annotation)\n",
        "\n",
        "                # Remove from plot for now\n",
        "                annotation.remove()\n",
        "\n",
        "            # Function to check if two boxes overlap (with tolerance)\n",
        "            def boxes_overlap(box1, box2, tolerance):\n",
        "                # Shrink the effective box size based on tolerance\n",
        "                width1 = box1[2] - box1[0]\n",
        "                height1 = box1[3] - box1[1]\n",
        "                width2 = box2[2] - box2[0]\n",
        "                height2 = box2[3] - box2[1]\n",
        "\n",
        "                # Apply tolerance to create effective box coordinates\n",
        "                eff_box1 = (\n",
        "                    box1[0] + width1 * tolerance,\n",
        "                    box1[1] + height1 * tolerance,\n",
        "                    box1[2] - width1 * tolerance,\n",
        "                    box1[3] - height1 * tolerance,\n",
        "                )\n",
        "\n",
        "                eff_box2 = (\n",
        "                    box2[0] + width2 * tolerance,\n",
        "                    box2[1] + height2 * tolerance,\n",
        "                    box2[2] - width2 * tolerance,\n",
        "                    box2[3] - height2 * tolerance,\n",
        "                )\n",
        "\n",
        "                # Check if effective boxes overlap\n",
        "                return (\n",
        "                    eff_box1[0] < eff_box2[2]\n",
        "                    and eff_box1[2] > eff_box2[0]\n",
        "                    and eff_box1[1] < eff_box2[3]\n",
        "                    and eff_box1[3] > eff_box2[1]\n",
        "                )\n",
        "\n",
        "            # Second pass - place annotations with smart positioning\n",
        "            for i, idx in enumerate(indices):\n",
        "                x = x_values[idx]\n",
        "                y = mean_pred_bin[idx]\n",
        "                annotation = annotations[i]\n",
        "\n",
        "                # Start with base offset\n",
        "                offset = base_offset\n",
        "\n",
        "                # Try increasing offsets until no overlap\n",
        "                max_attempts = 30\n",
        "                attempts = 0\n",
        "                overlap = True\n",
        "\n",
        "                # Ensure this annotation gets added to the plot regardless of overlap\n",
        "                added_to_plot = False\n",
        "\n",
        "                while overlap and attempts < max_attempts:\n",
        "                    # Set the offset\n",
        "                    annotation.xyann = (0, offset)\n",
        "\n",
        "                    # Add to the axes temporarily to get box dimensions\n",
        "                    ax.add_artist(annotation)\n",
        "\n",
        "                    # Get the bounding box in display coordinates\n",
        "                    box = annotation.get_window_extent(fig.canvas.get_renderer())\n",
        "\n",
        "                    # Convert to figure coordinates\n",
        "                    box_fig = box.transformed(fig.transFigure.inverted())\n",
        "\n",
        "                    # Extract coordinates\n",
        "                    box_coords = (box_fig.x0, box_fig.y0, box_fig.x1, box_fig.y1)\n",
        "\n",
        "                    # Check for overlap with existing boxes\n",
        "                    overlap = False\n",
        "                    for b in boxes:\n",
        "                        # Check if boxes overlap (using tolerance)\n",
        "                        if boxes_overlap(box_coords, b, overlap_tolerance):\n",
        "                            overlap = True\n",
        "                            break\n",
        "\n",
        "                    if overlap:\n",
        "                        # Remove from axes before trying again\n",
        "                        annotation.remove()\n",
        "\n",
        "                        # Increase offset\n",
        "                        offset += 3\n",
        "                        attempts += 1\n",
        "                    else:\n",
        "                        # No overlap, keep this position\n",
        "                        boxes.append(box_coords)\n",
        "                        added_to_plot = True\n",
        "                        break\n",
        "\n",
        "                # If we couldn't find a non-overlapping position, use the last one anyway\n",
        "                if attempts >= max_attempts:\n",
        "                    # Remove the annotation if it was added during testing\n",
        "                    if annotation in ax.texts:\n",
        "                        annotation.remove()\n",
        "\n",
        "                    # Add it back with the final offset\n",
        "                    annotation.xyann = (0, offset)\n",
        "                    ax.add_artist(annotation)\n",
        "                    added_to_plot = True\n",
        "\n",
        "                    # Get final box for future reference\n",
        "                    box = annotation.get_window_extent(fig.canvas.get_renderer())\n",
        "                    box_fig = box.transformed(fig.transFigure.inverted())\n",
        "                    boxes.append((box_fig.x0, box_fig.y0, box_fig.x1, box_fig.y1))\n",
        "\n",
        "                # Final check to ensure the annotation is in the plot\n",
        "                if not added_to_plot:\n",
        "                    annotation.xyann = (0, offset)\n",
        "                    ax.add_artist(annotation)\n",
        "\n",
        "                    # Get final box for future reference\n",
        "                    box = annotation.get_window_extent(fig.canvas.get_renderer())\n",
        "                    box_fig = box.transformed(fig.transFigure.inverted())\n",
        "                    boxes.append((box_fig.x0, box_fig.y0, box_fig.x1, box_fig.y1))\n",
        "    else:\n",
        "        plt.text(\n",
        "            0.5,\n",
        "            0.5,\n",
        "            'No data points found for plotting',\n",
        "            ha='center',\n",
        "            va='center',\n",
        "            transform=plt.gca().transAxes,\n",
        "        )\n",
        "\n",
        "    # Plot the perfect calibration line (y=x)\n",
        "    ax.plot(\n",
        "        [0, 1],\n",
        "        [0, 1],\n",
        "        color=colors.get('red', '#e41a1c'),\n",
        "        linestyle='--',\n",
        "        label='Perfect Calibration',\n",
        "    )\n",
        "\n",
        "    # Add labels, title, legend, grid\n",
        "    ax.set_xlabel(x_label, fontsize=18)\n",
        "    ax.set_ylabel('Predicted Probability', fontsize=18)\n",
        "    # set x and y tick fontsize\n",
        "    ax.tick_params(axis='both', labelsize=12)\n",
        "    # ax.set_title(title, fontsize=16)\n",
        "    ax.legend(fontsize=13, handler_map={type(dummy): HandlerErrorbar(yerr_size=0.6)})\n",
        "    ax.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Set axis limits\n",
        "    ax.set_xlim([-0.05, 1.05])\n",
        "    ax.set_ylim([-0.05, 1.05])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure if a save path is provided\n",
        "    if save_path is not None:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        # plt.close()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofaD4h10mpjt"
      },
      "outputs": [],
      "source": [
        "#'drive/MyDrive/unions_dwarfs_results/figures/model_performance.png'\n",
        "performance_plot(\n",
        "    true,\n",
        "    pred,\n",
        "    save_path='drive/MyDrive/unions_dwarfs_results/figures/model_performance.pdf',\n",
        "    overlap_tolerance=0.08,\n",
        "    show_counts=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mChdZUImueo"
      },
      "outputs": [],
      "source": [
        "performance_histogram(\n",
        "    true,\n",
        "    pred,\n",
        "    bins=4,\n",
        "    save_path='drive/MyDrive/unions_dwarfs_results/figures/performance_histogram.pdf',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4PE7anSMhzj"
      },
      "outputs": [],
      "source": [
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "\n",
        "def plot_calibration_soft_labels(soft_labels, model_probabilities, n_bins=10, strategy='uniform'):\n",
        "    \"\"\"\n",
        "    Illustrates the calibration of a model trained on soft labels.\n",
        "\n",
        "    This function generates a reliability diagram and calculates the\n",
        "    Expected Calibration Error (ECE), adapted for soft labels.\n",
        "\n",
        "    Args:\n",
        "        soft_labels (np.ndarray): A 1D NumPy array of soft labels, where each\n",
        "                                   value is between 0 and 1, representing the\n",
        "                                   true probability of the positive class for each sample.\n",
        "        model_probabilities (np.ndarray): A 1D NumPy array of the model's\n",
        "                                          predicted probabilities for the\n",
        "                                          positive class for each sample.\n",
        "        n_bins (int): The number of bins to use for the reliability diagram.\n",
        "        strategy (str): Strategy used to define the widths of the bins.\n",
        "                        'uniform': All bins have identical widths.\n",
        "                        'quantile': All bins have the same number of points.\n",
        "                                    (Not yet implemented, defaults to uniform)\n",
        "                                    # TODO: Implement quantile binning if needed\n",
        "    Returns:\n",
        "        tuple: (fig, ece)\n",
        "            - fig (matplotlib.figure.Figure): The figure object for the plot.\n",
        "            - ece (float): The Expected Calibration Error adapted for soft labels.\n",
        "    \"\"\"\n",
        "    if not isinstance(soft_labels, np.ndarray) or not isinstance(model_probabilities, np.ndarray):\n",
        "        raise ValueError('soft_labels and model_probabilities must be NumPy arrays.')\n",
        "    if soft_labels.shape != model_probabilities.shape:\n",
        "        raise ValueError('soft_labels and model_probabilities must have the same shape.')\n",
        "    if np.any(soft_labels < 0) or np.any(soft_labels > 1):\n",
        "        raise ValueError('Soft labels must be between 0 and 1.')\n",
        "    if np.any(model_probabilities < 0) or np.any(model_probabilities > 1):\n",
        "        raise ValueError('Model probabilities must be between 0 and 1.')\n",
        "\n",
        "    # For multi-class, this function assumes soft_labels and model_probabilities\n",
        "    # are for a specific class of interest, or for the positive class in a\n",
        "    # binary/pseudo-binary setup.\n",
        "\n",
        "    if strategy == 'uniform':\n",
        "        bin_limits = np.linspace(0, 1, n_bins + 1)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Unknown binning strategy: {strategy}. Choose 'uniform'.\"\n",
        "        )  # or 'quantile' when implemented\n",
        "\n",
        "    bin_lowers = bin_limits[:-1]\n",
        "    bin_uppers = bin_limits[1:]\n",
        "\n",
        "    bin_centers = (bin_lowers + bin_uppers) / 2\n",
        "    binned_soft_labels_mean = np.zeros(n_bins)\n",
        "    binned_model_probs_mean = np.zeros(n_bins)\n",
        "    bin_counts = np.zeros(n_bins)\n",
        "    bin_gaps = np.zeros(n_bins)\n",
        "\n",
        "    for i in range(n_bins):\n",
        "        lower = bin_lowers[i]\n",
        "        upper = bin_uppers[i]\n",
        "\n",
        "        # Handle the last bin to include 1.0\n",
        "        if i == n_bins - 1:\n",
        "            in_bin = (model_probabilities >= lower) & (model_probabilities <= upper)\n",
        "        else:\n",
        "            in_bin = (model_probabilities >= lower) & (model_probabilities < upper)\n",
        "\n",
        "        bin_counts[i] = np.sum(in_bin)\n",
        "\n",
        "        if bin_counts[i] > 0:\n",
        "            binned_soft_labels_mean[i] = np.mean(soft_labels[in_bin])\n",
        "            binned_model_probs_mean[i] = np.mean(model_probabilities[in_bin])\n",
        "        else:\n",
        "            binned_soft_labels_mean[i] = 0\n",
        "            binned_model_probs_mean[i] = (\n",
        "                0  # or bin_centers[i] - this affects ECE slightly if bin is empty\n",
        "            )\n",
        "\n",
        "        bin_gaps[i] = np.abs(binned_soft_labels_mean[i] - binned_model_probs_mean[i])\n",
        "\n",
        "    # Calculate Expected Calibration Error (ECE) for soft labels\n",
        "    # ECE = sum_{m=1}^{M} ( |B_m| / N ) * | acc(B_m) - conf(B_m) |\n",
        "    # Here, acc(B_m) is the average soft label in bin m,\n",
        "    # and conf(B_m) is the average model probability in bin m.\n",
        "    total_samples = len(model_probabilities)\n",
        "    if total_samples == 0:\n",
        "        ece = 0.0\n",
        "    else:\n",
        "        ece = np.sum((bin_counts / total_samples) * bin_gaps)\n",
        "\n",
        "    # Plotting\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    gs = GridSpec(4, 1)  # Grid for reliability diagram and histogram\n",
        "\n",
        "    # Reliability diagram\n",
        "    ax1 = fig.add_subplot(gs[:3, 0])\n",
        "    ax1.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')\n",
        "\n",
        "    # Plot bars for the gaps\n",
        "    # The bottom of the bar will be min(avg_soft_label, avg_model_prob)\n",
        "    # The height of the bar will be the gap\n",
        "    bar_bottoms = np.minimum(binned_soft_labels_mean, binned_model_probs_mean)\n",
        "    ax1.bar(\n",
        "        bin_centers,\n",
        "        bin_gaps,\n",
        "        bottom=bar_bottoms,\n",
        "        width=(bin_uppers[0] - bin_lowers[0]) * 0.9,  # Adjust bar width slightly\n",
        "        color='lightcoral',\n",
        "        edgecolor='firebrick',\n",
        "        alpha=0.7,\n",
        "        label='Gap (Over/Under-confidence)',\n",
        "    )\n",
        "\n",
        "    # Plot points for average soft label vs average model probability\n",
        "    # Only plot for bins with samples\n",
        "    valid_bins = bin_counts > 0\n",
        "    ax1.plot(\n",
        "        binned_model_probs_mean[valid_bins],\n",
        "        binned_soft_labels_mean[valid_bins],\n",
        "        's-',\n",
        "        color='navy',\n",
        "        markersize=8,\n",
        "        label='Model calibration',\n",
        "    )\n",
        "\n",
        "    ax1.set_xlabel('Average Predicted Probability (Confidence) in bin', fontsize=12)\n",
        "    ax1.set_ylabel('Average True Probability (Soft Label) in bin', fontsize=12)\n",
        "    ax1.set_title(f'Reliability Diagram (Soft Labels)\\nECE: {ece:.4f}', fontsize=14)\n",
        "    ax1.legend(loc='upper left', fontsize=10)\n",
        "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax1.set_xlim([0, 1])\n",
        "    ax1.set_ylim([0, 1])\n",
        "\n",
        "    # Histogram of predicted probabilities\n",
        "    ax2 = fig.add_subplot(gs[3, 0])\n",
        "    ax2.hist(\n",
        "        model_probabilities,\n",
        "        range=(0, 1),\n",
        "        bins=n_bins,\n",
        "        color='cornflowerblue',\n",
        "        edgecolor='black',\n",
        "        alpha=0.8,\n",
        "    )\n",
        "    ax2.set_xlabel('Predicted Probability', fontsize=12)\n",
        "    ax2.set_ylabel('Number of Samples', fontsize=12)\n",
        "    ax2.set_title('Histogram of Predicted Probabilities', fontsize=14)\n",
        "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    ax2.set_xlim([0, 1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig, ece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXcvivY9MlFX"
      },
      "outputs": [],
      "source": [
        "plot_calibration_soft_labels(\n",
        "    soft_labels=true, model_probabilities=pred, n_bins=10, strategy='uniform'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WrVee2hUBAC"
      },
      "outputs": [],
      "source": [
        "len(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mgTrSrGnhT2"
      },
      "outputs": [],
      "source": [
        "data_test = DATA[test_indices]\n",
        "label_test = LABEL[test_indices]\n",
        "\n",
        "plot_gallery(\n",
        "    data_test,\n",
        "    label_test,\n",
        "    pred,\n",
        "    bins=10,\n",
        "    per_bin=5,\n",
        "    figsize=(30, 15),\n",
        "    title_fontsize=28,\n",
        "    label_fontsize=23,\n",
        "    wspace=0.2,\n",
        "    hspace=0.15,\n",
        "    seed=42,\n",
        "    save_path='drive/MyDrive/unions_dwarfs_results/figures/gallery_kfold.pdf',\n",
        ")\n",
        "# 'drive/MyDrive/unions_dwarfs_results/figures/gallery_kfold_5bin.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxxVqudPoVLI"
      },
      "outputs": [],
      "source": [
        "visualize_ensemble_results(stats, save_dir=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmld5cTmqBtb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
